{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Statistik mit R für Umweltwissenschaftler:innen\n",
        "subtitle: Master Modul \"Research Methods\"\n",
        "authors: Jürgen Dengler mit Beiträgen von Gian-Andrea Egeler, Daniel Hepenstrick & Stefan Widmer\n",
        "lang: de\n",
        "format: \n",
        "    html:\n",
        "        toc: true\n",
        "        toc-location: left\n",
        "---"
      ],
      "id": "c46f098b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](./myMediaFolder/media/image1.jpeg){width=\"2.9472222222222224in\"\n",
        "height=\"1.3430555555555554in\"}\n",
        "\n",
        "\n",
        "****\n",
        "\n",
        "Skript, Version 25\n",
        "\n",
        "\n",
        "\n",
        "**Empfohlenes Zitat:**\n",
        "\n",
        "Dengler, J., Egeler, G.-A., Hepenstrick, D. & Widmer, S. 2022. *Statistik mit R für Umweltwissenschaftler:innen. Skript Version 25.* Institut für Umwelt und Natürliche Resourcen (IUNR), ZHAW, Wädenswil, CH.\n",
        "\n",
        "Korrekturhinweise und Verbesserungsvorschläge an juergen.dengler@zhaw.ch sind willkommen.\n",
        "\n",
        "\n",
        "# Vorwort\n",
        "\n",
        "*Jürgen Dengler*\n",
        "\n",
        "Ich bin Ökologe, kein Statistiker. Trotzdem (oder vielleicht gerade deswegen) wurde ich vor gut drei Jahren, als ich am IUNR als Dozent und Leiter der Forschungsgruppe Vegetationsökologie gefragt, ob ich nicht den Statistikteil im „Research Methods\"-Modul des neuen Masterstudiengangs „Umwelt und Natürliche Resourcen\" übernehmen würde. Ich habe zugesagt, obwohl ich mir der doppelten Herausforderung klar war: (1) als statistische Autodidakt Statistik zu lehren und (2) dies nicht nur für ÖkologInnen, sondern für angehende UmweltingenieurInnen im Allgemeinen zu tun, deren Interessen von Umweltbildung bis zu Umwelttechnologien reichen und die gleichermassen im naturwissenschaftlichen wie im sozialwissenschaftlichen Bereichen unterwegs sind.\n",
        "\n",
        "Der Kurs hat sich über die Jahre weiterentwickelt, vor allem durch konstruktiv-kritisches Feedback der Studierenden. Während nur wenige der ehemaligen TeilnehmerInnen vermutlich von sich behaupten würden, im Modul zu begeisterten Statistikfans geworden zu sein, so konnte ich doch in nachfolgenden Mastermodulen (etwa der „Summer School Biodiversity Monitoring\" oder bei Präsentationen von Masterarbeiten) feststellen, dass viele das Handwerkszeug sehr solide gelernt haben und souverän anwenden konnten. Manche konnten am Ende des Masterstudium durch stetiges Learning by doing in der offenen Plattform R sogar statistische Fähigkeiten vorweisen, die deutlich über das im Kurs selbst vermittelte hinausgehen. Ja, acht halbe Kurstage sind extrem wenig, um auch nur die wichtigsten Grundlagen der Statistik zu lernen. Wenn ihr erfolgreich sein wollt, müsst ihr also aktiv mitmachen und mehr Quellen nutzen als nur unsere Inputs im Modul.\n",
        "\n",
        "Ich hatte eigentlich nicht vor, ein Skript zum Kurs zu erstellen, obwohl das Studierende auch in den Vorjahren immer wieder gewünscht haben. Der Aufwand dafür schien mir zu gross -- auch in Relation zu den Stunden, die mir für den Kurs zur Verfügung stehen. Ausserdem fand ich, dass das Lernsetting in den Vorjahren mit einer Vorlesung mit vielen Interaktionen mit den Studierenden, gefolgt von der Vorführung und Diskussion von Demo-R-Skripten und schliesslich betreuten Übungen angemessen und recht effizient war. Dann kam bekanntlich Covid-19 und im Herbstsemester 2020 war alles anders. Wir haben entschieden das „Methodenmodul\" aus epidemologischen Gründen ohne physischen Kontakt zu euch durchzuführen. Ich hätte wie andere Dozierende in dieser Situation mit Screencasts arbeiten können, aber ohne die Möglichkeit, dabei auf eure Fragen direkt eingehen zu können, schien mir das wenig erfolgsversprechend. Auch den ganzen Vormittag lang online-Kurs zu halten, schien mir für euch wie für uns Dozierende unzumutbar. Insofern habe ich mich nach Diskussionen mit den anderen Beteiligten entschieden, doch ein Skript zu erstellen. Die Idee ist, dass ihr es vorgängig zu den Kurstagen lest und wir dann in einem gemeinsamen Online-Raum auf Zoom, im Sinne eines „inverted classroom\" eure offenen Fragen diskutieren können und ich ggf. Punkte, die nicht alle verstanden haben noch einmal „live\" erklären kann.\n",
        "\n",
        "Das hier vorliegende Skript ist zunächst die Verschriftlichung der Vorlesungsfolien der letzten Jahre. Aber viele Aspekte, die auf den Folien nur in Stichpunkten auftauchten, da sie im Kurs live besprochen wurden, sind jetzt eben auch ausformuliert. Nebenbei wurde natürlich manch Anderes auch noch verbessert, ergänzt und aktualisiert. Nichtsdestotrotz ist es die erste Fassung dieses Skriptes und alle Unzulänglichkeiten seien mir nachgesehen. Verbesserungsvorschläge sind jederzeit willkommen.\n",
        "\n",
        "Wichtig ist, dass dieses Skript nicht als alleiniges Lehrmaterial gedacht ist. Genauso wichtig sind die gemeinsamen Präsenz-Lektionen mit Diskussion des theoretischen Stoffes und der Vorführung (Demo) exemplarischer R-Codes sowie die Übungen und deren Besprechung. Ich empfehle euch auch, begleitend auch andere Quellen zu nutzen, insbesondere wenn einige von euch meine Erklärungen schwer verständlich finden sollten. Welche Form der Informationsbereitstellung jemand eingängig findet, ist individuell sehr verschieden. Für Statistik 1--5 empfehle ich euch insbesondere das Lehrbuch von Crawley (2015), welches das offizielle Begleitlehrbuch zum Kurs ist. Ich werde auch nicht alle Details aus Crawley (2015) im Kurs wiederholen. In den ersten drei Durchführungen haben wir noch das Buch von Logan (2010) verwendet, das ausführlicher ist und „Kochrezepte\" auch für komplexere Fälle bietet, die über das hinausgehen, was wir im Kurs behandeln können. Der Vorteil von Crawley (2015) ist, dass das Buch knapper ist und nicht nur auf biologische Fälle, sondern auf beliebige Disziplinen bezogen. Trotzdem ist Logan (2010) weiterhin eine empfehlenswerte Quelle für inferenzstatistische Methoden. Leider gibt es nach meiner Sichtung von etwa zwei Dutzend Statistikbüchern mit R, keines das gleichermassen die Inferenzstatistik und die deskriptiv-multivariate Statistik in der für den Kurs angemessenen Tiefe behandelt. Man könnte das Mammutwerk von Crawley (2013) nennen, aber trotz über 1000 Seiten sind dort die multivariat-deskriptiven Methoden nur sehr kurz (aber immerhin) behandelt und es ist eher ein Kompendium als ein Lehrbuch. Insofern werde ich für Statistik 6--8 auf andere Quellen zurückgreifen, insbesondere auf das exzellente Lehrbuch von Borcard et al. (2018), das aber weitestgehend inferenzstatistischen Methoden aussen vorlässt und die multivariat-deskriptiven aus der alleinigen Sicht von ÖkologInnen beschreibt. Zu guter Letzt möchte ich noch das Buch von Quinn & Keough (2002) empfehlen, das m. E. die ganze Bandbreite statistischer Methoden für ÖkologInnen beschreibt und hervorragend mit vielen Beispielen erklärt, aber eben aus der „Vor-R-Zeit\", mithin ohne Beispiel-Code. Da nahezu alle aus meiner Sicht empfehlenswerten aktuellen Statistikbücher auf Englisch sind, dieses Skript jedoch auf Deutsch, habe ich im Skript wichtige Fachtermini in beiden Sprachen angegeben (Englisch ist dann *kursiv*), um eine leichtere Verknüpfung zu schaffen.\n",
        "\n",
        "Im Skript wird die Theorie beginnend mit den einfachsten statistischen Verfahren (die den Masterstudierenden schon geläufig sein sollten) sukzessive aufgebaut, wobei an geeigneten Stellen wichtige Grundsätze (z.B. unabhängigkeit der Messwerte, Voraussetzungen für Tests etc.) erklärt werden, die für die Statistik insgesamt relevant sind. Die Theorie ist immer mit dem entsprechenden R-Code kombiniert, einschliesslich der Interpretation der textlichen und grafischen Ausgaben von R. Das Skript enthält nur Auszüge des R-Codes, der in Gänze im Unterricht (in der jeweils zweiten Lektion) vorgestellt und besprochen wird. Da es in diesem Kursteil um das Verständnis der Statistik geht, wurde kein grosser Aufwand auf das «Optimieren» des visuellen Outputs gelegt, welches den Code of wesentlich verlängert und den Blick vom «Eigentlichen» abgelenkt hätte.\n",
        "\n",
        "## Quellen\n",
        "\n",
        "Borcard, D., Gillet, F. & Legendre, P. 2018. *Numerical ecology with R*. 2nd ed. Springer, Cham, CH: 435 pp.\n",
        "\n",
        "Crawley, M.J. 2013. *The R book*. 2nd ed. John Wiley & Sons, Chichester, UK: 1051 pp.\n",
        "\n",
        "Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed. John Wiley & Sons, Chichester, UK: 339 pp.\n",
        "\n",
        "Logan, M. 2010. *Biostatistical design and analysis using R: a practical guide*. Wiley-Blackwell, Chichester, UK: 546 pp.\n",
        "\n",
        "Quinn, G.P. & Keough, M.J. 2002. *Experimental design and data analysis for biologists*. Cambridge University Press, Cambridge, UK: 537 pp.\n",
        "\n",
        "# Statistik 1 Grundlagen der Statistik\n",
        "\n",
        "**In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der *frequentist*-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem *t*-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *versteht, was Statistik im Kern leistet und warum Statistik für     wissenschaftliche Erkenntnis (in den meisten Disziplinen)     unentbehrlich ist;*\n",
        "\n",
        "- *könnt Angaben zu p-Werten oder Signifikanzlevels kritisch     würdigen;*\n",
        "\n",
        "- *wisst, wann man einen t-Test und wann einen Chi-Quadrat-Test     verwendet und wie man das praktisch in R durchführt; und*\n",
        "\n",
        "- *habt eine grundlegende Idee, worauf es beim Berichten statistischer     Ergebnisse, insbesondere in Abbildungen ankommt.*\n",
        "\n",
        "## Warum brauchen wir Statistik?\n",
        "\n",
        "### Ein Beispiel\n",
        "\n",
        "Ich möchte die grundlegende Notwendigkeit von Statistik mit einem fiktiven Beispiel visualisieren. Gehen wir von einer einfachen Frage aus dem Zierpflanzenbau aus:\n",
        "\n",
        "***Unterscheiden sich zwei verschiedene Sorten (Cultivare) in der Blütengrösse?***\n",
        "\n",
        "![](./myMediaFolder/media/image2.emf.png){width=\"4.53125in\" height=\"3.2916666666666665in\"}\n",
        "\n",
        "Um diese Frage zu beantworten, vermessen wir die Blüten der beiden abgebildeten Individuen:\n",
        "\n",
        "- Individuum A: 20 cm²\n",
        "- Individuum B: 12 cm²\n",
        "\n",
        "Mithin wäre unsere naive Antwort auf die Eingangsfragen: **Ja, die Blüten von Sorte A sind grösser als jene von B**. Wir können sogar sagen, um wie viel grösser (8 cm² oder 67 %).\n",
        "\n",
        "Nun haben Pflanzen (wie fast alle Objekte, mit denen wir uns beschäftigen, mit Ausnahme vielleicht von Elementarteilchen) eine gewisse Variabilität:\n",
        "\n",
        "![](./myMediaFolder/media/image3.emf.png){width=\"5.53125in\" height=\"3.84375in\"}\n",
        "\n",
        "Folglich ist es sinnvoller, für die Beantwortung der Frage jeweils mehrere Individuen zu vermessen. Wir greifen nun 10 Individuen jeder Sorte heraus und erzielen folgende Messergebnisse:\n",
        "\n",
        "- Individuen A1--A10 [cm²]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\n",
        "- Individuen B1--B10 [cm²]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\n",
        "\n",
        "Wir erhalten für A einen Mittelwert von 15.3 cm² und für B einen Mittelwert von 11.4 cm² (was wir einfach in Excel ausrechnen können). **Wir schliessen daher, dass die Blüten von A im Mittel 3.9 cm² grösser sind als jene von B**.\n",
        "\n",
        "Wir könnten uns also zufrieden zurücklehnen und unserem Ergebnis, das wir mit etwas **deskriptiver Statistik** (Mittelwerte) erzielt haben, vertrauen. Wo liegt der Haken? Wir haben nicht alle existierenden Individuen der Sorten A und B vermessen (die „Grundgesamtheit\"), sondern nur eine Stichprobe von jeweils 10 Individuen. Nun könnte es sein, dass KollegInnen von uns die gleiche Untersuchung mit jeweils anderen Stichproben von je 10 Individuen durchgeführt haben, etwa folgendermassen (mit ihren jeweiligen Schlussfolgerungen):\n",
        "\n",
        "> **Mess-Serie 1:\\\n",
        "> **Individuen A1--A10 \\[cm²\\]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\\\n",
        "> Individuen B1--B10 \\[cm²\\]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\\\n",
        "> Ergebnis: A = 15.3; B = 11.4; A -- B = 3.9 cm² ***►A ist grösser als\n",
        "> B***\n",
        ">\n",
        "> **Mess-Serie 2:\\\n",
        "> **Individuen A1--A10 \\[cm²\\]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\\\n",
        "> Individuen B1--B10 \\[cm²\\]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\\\n",
        "> Ergebnis: A = 12.5; B = 11.3; A -- B = 1.2 cm² ***►A ist (wenig)\n",
        "> grösser als B***\n",
        ">\n",
        "> **Mess-Serie 3:\\\n",
        "> **Individuen A1--A10 \\[cm²\\]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\\\n",
        "> Individuen B1--B10 \\[cm²\\]: 12; 15; 16; 7; 8; 10; 12; 11; 13; 10\\\n",
        "> Ergebnis: A = 11.0; B = 11.0; A -- B = 0.0 cm² ***►A ist gleich gross\n",
        "> wie B***\n",
        ">\n",
        "> **Mess-Serie 4:\\\n",
        "> **Individuen A1--A10 \\[cm²\\]: 20; 19; 25; 10; 8; 15; 13; 18; 11; 14\\\n",
        "> Individuen B1--B10 \\[cm²\\]: 12; 15; 16; 16; 14; 10; 12; 11; 13; 10\\\n",
        "> Ergebnis: A = 11.0; B = 12.9; A -- B = -- 1.9 cm² ***►A ist kleiner\n",
        "> als B\\ > ***\n",
        "\n",
        "Wer hat nun Recht? Um das zu beantworten, benötigen wir die **schliessende Statistik (Inferenzstatistik)**.\n",
        "\n",
        "### Fazit\n",
        "\n",
        "- In der Regel wollen wir nicht wissen, ob ein einzelnes Individuum     der Sorte A sich von einem einzelnen Individuum der Sorte B     unterscheidet.\n",
        "\n",
        "- Meist interessiert uns, ob sich die Sorte A als solche von der Sorte     B unterscheidet.\n",
        "\n",
        "- Da es in der Regel nicht möglich ist, sämtliche existierenden     Individuen beider Sorten (**Grundgesamtheiten**; engl.     *populations*) zu vermessen, vermessen wir die Individuen in zwei     **Stichproben** (engl. *samples*).\n",
        "\n",
        "- Die **Inferenzstatistik** sagt uns dann, **wie wahrscheinlich** ein     festgestellter **Unterschied in den Mittelwerten der Stichproben**     einem tatsächlichen **Unterschied in den Mittelwerten der     Grundgesamtheiten** entspricht.\n",
        "\n",
        "## Warum mit R?\n",
        "\n",
        "![](./myMediaFolder/media/image4.png)\n",
        "\n",
        "Zugegeben: wir haben euch nicht gefragt...\n",
        "\n",
        "### Was spricht dagegen?\n",
        "\n",
        "Auf den ersten Blick mag aus eurer Sicht ja einiges dagegensprechen\n",
        "\n",
        "- **keine GUI** (grafische Benutzeroberfläche) zum Klicken - auf Englisch - **schwerer** zu erlernen\n",
        "\n",
        "### Was spricht dafür?\n",
        "\n",
        "- R ist **kostenlos & open source** (unabhängig von teuren Lizenzen)\n",
        "- R ist extrem **leistungsfähig** und immer **up-to-date** (da\n",
        "    Tausende „ehrenamtlich\" mitprogrammieren)\n",
        "- R ist nah an den **speziellen Bedürfnissen** der einzelnen\n",
        "    Disziplinen (durch zahlreiche spezielle *Packages*)\n",
        "- R «zwingt» die Benutzenden dazu, ihr **statistisches Vorgehen zu\n",
        "    durchdenken** (was zu besseren Ergebnissen führt)\n",
        "- R gewährleistet eine sehr **gute Dokumentation** des eigenen\n",
        "    Vorgehens („Reproduzierbarkeit\"), da der geschriebene R Code anders\n",
        "    als eine Klickabfolge in einem kommerziellen Statistikprogramm mit\n",
        "    GUI eingesehen und erneut durchgeführt werden kann\n",
        "- R ist effizient, da man Code, den man einmal entwickelt hat, **immer\n",
        "    wieder verwenden bzw. für neue Projekte anpassen** kann\n",
        "- Für R gibt es **umfangreiche Hilfe im Internet** (googlen, spezielle\n",
        "    Foren,...)\n",
        "\n",
        "### Fazit\n",
        "\n",
        "Der Kursleiter (J.D.) hat Statistik nicht in seinem Studium gelernt und es sich später im Laufe seiner Forscherlaufbahn mühsam sich selbst beigebracht. Damals gab es noch kein R. Dafür gab es teure kommerzielle Statistikprogramme wie SPSS und STATISTICA, durch die man sich mit einer grafischen Benutzeroberfläche durchklicken konnte und am Ende ein Ergebnis bekam. Nicht immer war ganz klar, was das Programm da gerechnet hatte, aber immerhin bekam man mit relativ wenigen Klicks ein numerisches Ergebnis oder eine Abbildung (oft allerdings in bescheidenem Layout) heraus. Häufig musste man aber erleben, dass das gewünschte statistische Verfahren im jeweiligen Programm in der gewünschten Version nicht implementiert war oder ein teures Zusatzpaket nötig gewesen wäre, das die eigene Universität nicht erworben hatte. Und wenn man dann an eine andere Universität wechselte, musste man oft feststellen, dass dort ein anderes Statistikprogramm erworben und genutzt wurde, für das man viele Dinge umlernen musste. Ganz zu schweigen von Zeiten ausserhalb einer Hochschule, wenn man keinen Zugriff auf ein kommerzielles Statistikprogramm hatte.\n",
        "\n",
        "Aus dieser Sicht könnt ihr euch also glücklich schätzen, dass es heute R gibt und so leistungsfähig ist wie nie zuvor und auch dass das IUNR in der Ausbildung im Bachelor- und Masterlevel konsequent auf R setzt. Während es auf den ersten Blick vielleicht schwieriger erscheinen mag als die Benutzung von SPSS oder STATISTICA, bin ich überzeugt, dass ein Statistikkurs mit R euch bei gleichem Aufwand ein anderes Verständnislevel für Statistik ermöglichen wird als es Statistikkurse zu meiner Studienzeit taten. Nebenher bekommt ihr noch ein implizites Verständnis wie Algorithmen funktionieren, auch nicht ganz unwichtig in einer zunehmend digitalen Welt.\n",
        "\n",
        "## Die Rolle von Hypothesen in der Wissenschaft\n",
        "\n",
        "### Rekapitulation\n",
        "\n",
        "Im Methodenmodul und sicher auch in euren vorausgehenden Studiengängen habt ihr euch bereits mit Hypothesen beschäftigt. Daher beginnen wir mit einem Arbeitsauftrag (allein oder im Austausch mit KommilitonInnen):\n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        "## Arbeitsauftrag\t\n",
        "\n",
        "Formuliert jeweils in einem Satz die folgenden Punkte:\n",
        "\n",
        "- Eine beispielhafte Aussage, die den Ansprüchen an eine Hypothese genügt\n",
        "- Eine beispielhafte Aussage, die keine Hypothese ist\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "### Was ist eine Hypothese?\n",
        "\n",
        "Es gibt in der Literatur wie fast immer in der Wissenschaft verschiedene Formulierungen. Ich schlage die folgende vor:\n",
        "\n",
        ":::{callout-note}\n",
        "\n",
        "Eine Hypothese ist eine aus einer allgemeinen Theorie abgeleitete Vorhersage für eine spezifische Situation.\n",
        "\n",
        ":::\n",
        "\n",
        "Leider wird der Begriff „Hypothese\" heutzutage in der Wissenschaft „inflationär\" und aus meiner Sicht sogar häufig falsch verwendet.\n",
        "\n",
        "Zweifelhaft sind „ad hoc\"-Hypothesen auf der Basis einer Vorabuntersuchung bzw. eines „Bauchgefühls\", aber ohne eine Erklärung des für das vorhergesagte Ergebnis verantwortlichen Mechanismus (also letztlich ohne Theorie dahinter). Wissenschaftstheoretisch sollte man nie dieselben Daten zum Aufstellen und zum Testen einer Hypothese verwenden!\n",
        "\n",
        "Gänzlich falsch sind angebliche „Hypothesen\", die nachträglich aus den schon erzielten Ergebnissen abgeleitet werden.\n",
        "\n",
        "Warum findet man in der wissenschaftlichen Literatur wie auch in studentischen Arbeiten so viele „Hypothesen\", die wissenschaftstheoretisch dem Konzept einer Hypothese nicht gerecht werden? Der Grund dürfte darin liegen, dass viele von der Annahme geleitet werden, dass nur eine hypthesentestende Forschung eine gute/richtige Forschung ist. Tatsächlich ist aber hypothesengenierende Forschung genauso wichtig und richtig wie hypothesentestende Forschung. Es gilt also:\n",
        "\n",
        ":::{callout-note}\n",
        "\n",
        "Wenn das Vorwissen nicht für eine **plausibel begründete Hypothese** ausreicht (**hypothesentestende Forschung**), formuliert man das Forschungsthema korrekterweise besser als **offene Frage** (**hypothesengenerierende Forschung**).\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "Dabei können offene Fragen meist mit (fast) den gleichen statistischen Verfahren addressiert werden wie Hypothesen. Allerdings sollten Hypothesen konkret sein, also nicht „A unterscheidet sich von B\", sondern entweder „A ist grösser als B\" oder „A ist kleiner als B\". Hier würde also im Fall einer Hypothese ein einseitiger Test, im Fall einer offenen Frage ein zweiseitiger Test zur Anwendung kommen. Dazu aber später mehr.\n",
        "\n",
        "### Wissenschaftliches Arbeiten (in a nutshell)\n",
        "\n",
        "Wenn wir modernes wissenschaftliches Arbeiten ganz knapp visualisieren, ergibt sich folgendes Bild:\n",
        "\n",
        "![](./myMediaFolder/media/image6.emf.png){width=\"5.574305555555555in\" height=\"3.4368055555555554in\"}\n",
        "\n",
        "Bei den ersten Schritten von den Beobachtungen bis zur Spekulation über die Musterursachen handelt es sich um hypothesengenerierende Forschung. Erst wenn man regelmässig, ähnliche Befunde hat, macht das Formulieren enier echten Hypothese Sinn, die nicht nur das gefundene Muster vorhersagt, sondern auch einen Mechanismus bereithält, der erklärt, wie es zustande gekommen ist. Eine solche Hypothese kann dann in einer neuen Untersuchung (mit neuen Daten!) getestet werden, die spezifisch darauf ausgelegt ist, alternative Erklärungsmöglichkeiten auszuschliessen („Experiment\"). Hypothesengenierende und hypothesentestende Forschung sind im modernen Forschungsablauf also beide gleichermassen nötig, aber in der Regel getrennt voneinander.\n",
        "\n",
        "In einer Forschungsarbeit, die für das Testen einer zuvor in anderen Arbeiten erarbeiteten Hypothese, entwickelt wurde („Experiment\"), kann das Ergebnis entweder eine Bestätigung oder eine Falsifizierung sein. Wichtig ist, dass eine einmalige Bestätigung keine Verifizierung einer Hypothese ist, während eine einmalige Falsifizierung zur Widerlegung genügt. **Eine Verifizierung einer Hypothese in einem absoluten Sinn ist grundsätzlich nicht möglich, absolute Wahrheit gibt es in der Wissenschaft nicht!** Wenn man jedoch eine Hypothese mit immer neuen „Experimenten\" unter immer neuen Rahmenbedingungen „herausfordert\" und sie dabei nie falsifiziert wird, dann wird aus einer **einfachen Hypothese** zunehmend **gesichertes Wissen**. Wenn man dagegen eine Hypothese widerlegt hat, muss man zurückgehen. Die vorgeschlagene Erklärung für das gefundene Muster oder sogar das Muster an sich hat sich als nicht korrekt/nicht allgemeingültig herausgestellt. Man muss sich also einen anderen Mechanismus/eine andere Hypothese ausdenken und diese erneut testen. Dies geschieht dann nicht in derselben, sondern in einer folgenden wissenschaftlichen Arbeit.\n",
        "\n",
        "Mit diesem Wissen über den Ablauf von wissenschaftlicher Erkenntnis und der Rolle des Hypothesentestens dabei habe ich noch eine Frage, zu der ihr euch bis zum Kurstag Gedanken machen solltet:\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "## Frage\n",
        "\n",
        "Profitiert Wissenschaft mehr von der Bestätigung oder von der Falsifizierung von Hypothesen?\n",
        "(Bitte begründet eure Antwort!)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Die Rolle der Statistik beim Hypothesengenerieren und -testen\n",
        "\n",
        "Wir haben gesehen, dass Hypothesen zentral für die moderne Wissenschaft sind, sowohl ihr Generieren als auch ihr Test. Doch welche Rolle spielt die Statistik dabei?\n",
        "\n",
        "Statistiche Verfahren, die implizit oder explizit Hypothesen testen, bezeichnet man als **Inferenzstatistik (schliessende Statistik)** -- im Gegensatz zur **deskriptiven Statistik**.\n",
        "\n",
        "### Von der Hypothese zur Nullhypothese...\n",
        "\n",
        "Die Herausforderung ist nun aber, wie oben gesehen, dass man eine **Hypothese (H~a~)** (auch **Forschungshypothese** oder **[a]{.underline}lternative Hypothese** genannt) nicht verifizieren kann, sondern nur falsifizieren. In der Statistik behilft man sich daher mit einem Trick, der sogenannten **Nullhypothese (H~0~)**. Die Nullhypothese ist die Negation der Hypothese, d. h. die Summe aller möglichen Beobachtungen, die mit der Hypothese nicht im Einklang sind. Wenn man nun die Nullhypothese falsifiziert, kann man indirekt die Hypothese bestätigen.\n",
        "\n",
        "In unserem Beispiel von oben:\n",
        "\n",
        "- Hypothese (H~A~): ***Sorte A und Sorte B unterscheiden sich in ihrer     Blütengrösse***\n",
        "- Nullhypothese (H~A0~): ***Sorte A und Sorte B haben die gleiche     Blütengrösse***\n",
        "\n",
        "Das ist formal korrekt, wissenschaftlich ist die Forschungshypothese aber wenig überzeugend, weilschwerlich ein Mechanismus vorstellbar ist, der in Sorte B sowohl kleinere als auch grössere, nur keine gleich grossen Blüten hervorbringt. Insofern wäre das folgende Paar sinnvoller:\n",
        "\n",
        "- Hypothese (H~B~): ***Sorte A hat grössere Blüten als Sorte B***\n",
        "- Nullhypothese (H~B0~): ***Sorte A hat kleinere oder gleich grosse     Blüten wie Sorte B***\n",
        "\n",
        "Die erste Forschungshypothese (H~A~) ist eine ungerichtete Hypothese und entspricht dem, was man in hypothesengenerierender Forschung implizit macht (wenn man also offene Fragen, aber keine konkreten Hypothesen hat). In diesem Fall wäre die zugehörige Forschungsfrage: **„Unterscheiden sich die Sorten A und B in ihren Blütengrössen?\"**. Die zweite Forschungshypothese (H~B~) ist dagegen gerichtet und wäre für hypothesentestenden Forschung adäquat. In der hypothesentestenden Forschung sollten wir auch eine Begründung/einen Mechanismus anführen, der vermutlich zu dem vorhergesagten Ergebnis führt, etwa dass die Sorte A polyploid ist. Dies gehört zur Begründung der Forschungshypothese, aber ist nicht Bestandteil der Forschungshypothese.\n",
        "\n",
        "### Einschub: Wichtige Termini in der Statistik\n",
        "\n",
        "Bis hierher sind uns schon einige wichtige statistische Begriffe (wie Stichprobe und Grundgesamtheit) begegnet, deshalb sollen sie hier samt ihren englischen Pendants noch einmal rekapituliert werden:\n",
        "\n",
        "  ------------------------------------------------------------------------------------\n",
        "  **Deutscher Begriff** **Englischer        **Definition**        **Beispiel(e)**\n",
        "                        Begriff**                                 \n",
        "  --------------------- ------------------- --------------------- --------------------\n",
        "  **Beobachtung**       *Observation*       experimentelle bzw.   Pflanzenindividuum\n",
        "                                            Beobachtungseinheit   \n",
        "\n",
        "  **Stichprobe**        *Sample*            alle beprobten        die 20 untersuchten\n",
        "                                            Einheiten             Pflanzenindividuen\n",
        "\n",
        "  **Grundgesamtheit**   *Population*        Gesamtheit aller      alle Individuen der\n",
        "                                            Einheiten, über die   beiden Sorten\n",
        "                                            eine Aussage          \n",
        "                                            getroffen werden soll \n",
        "\n",
        "  **Messung**           *Measurement*       einzelne erhobene     Blütengrösse eines\n",
        "                                            Information           Individuums\n",
        "\n",
        "  **Variable**          *Variable*          Kategorie der         Blütengrösse, Sorte\n",
        "                                            erhobenen Information \n",
        "  ------------------------------------------------------------------------------------\n",
        "\n",
        "Der englische Begriff *population* führt oft zu Verwirrung, da er in der Statistik etwas anderes meint als in der Biologie. Population ist schlicht die Grundgesamtheit, die in seltenen Fällen einer biologischen Population entspricht, in den meisten Fällen aber nicht (etwa *population of chairs*). Auch Messung/measurement wird in der Statistik weiter als in der Allgemeinsprache verwendet, d. h. auch für Zählungen oder Erhebung von kategorialen Variablen.\n",
        "\n",
        "### Einschub: Parameter vs. Prüfgrössen\n",
        "\n",
        "Wenn wir in Inferenzstatistik betreiben, also von einer Stichprobe auf die Grundgesamtheit schliessen wollen, müssen wir zudem zwischen Parametern und Prüfgrössen unterscheiden. Unter Parameter (*parameter*) wird eine Grösse der deskriptiven Statistik für eine betimmte Variable in der Grundgesamtheit verstanden, über die wir eine Aussage treffen wollen, die wir aber nicht kennen. Dagegen ist eine Prüfgrösse (*statistic*) eine aus den Messungen der Variablen in der Stichprobe berechnete Grösse, die zur Schätzung des Parameters dient. Etwas verwirrend ist, dass *stastitic* (Prüfgrösse) und *statistics* (die Statistik als Fach) fast gleich lauten. Oft wird die Konvention verwendet, dass die Prüfgrössen mit kursiven lateinischen Buchstaben (z. B. *s*²) und die korrespondierenden Parameter mit den äquivalenten griechischen Buchstaben (z. B. σ²) bezeichnet werden (siehe die folgende Tabelle):\n",
        "\n",
        "![](./myMediaFolder/media/image7.png){width=\"6.498611111111111in\" height=\"4.392746062992126in\"}\n",
        "\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "### Statistische Implementierung des Hypothesentestens (am Beispiel des t-Tests)\n",
        "\n",
        "Wie lässt sich das Hypothesentesten nun mathematisch und statistisch umsetzen. Wir bleiben bei unserer offenen Forschungsfrage „Unterscheiden sich die Sorten A und B in ihren Blütengrössen?\", woraus sich die Forschungshypothese „Sorten A und B unterscheiden sich in ihren Blütengrössen\" ergibt. Mit dem Mittelwert µ der Variablen (Blütengrösse) in den jeweiligen Grundgesamtheiten (A und B) lassen sich Forschungshypothese und Nullhypothese mathematisch wie folgt formulieren:\n",
        "\n",
        "> **H~a~:** µ~A~ ≠ µ~B~\n",
        ">\n",
        "> **H~0~:** µ~A~ = µ~B~ oder µ~A~ -- µ~B~ = 0\n",
        "\n",
        "Für die Überprüfung der H~0~ gibt es eine Teststatistik (Prüfgrösse) den\n",
        "*t*-Wert, der wie folgt definiert ist:\n",
        "\n",
        "$$t = \\frac{\\left( {\\overline{y}}_{A} - {\\overline{y}}_{B} \\right) - (\\mu_{A} - \\mu_{B})}{s_{{\\overline{y}}_{A} - {\\overline{y}}_{B}}}$$\n",
        "\n",
        "Da für die H~0~ gilt µ~A~ -- µ~B~ = 0, lässt sich das vereinfachen zu:\n",
        "\n",
        "$$t = \\frac{\\left( {\\overline{y}}_{A} - {\\overline{y}}_{B} \\right)}{s_{{\\overline{y}}_{A} - {\\overline{y}}_{B}}}$$\n",
        "\n",
        "Die Prüfgrösse *t* ist also die Differenz der beiden Mittelwerte dividiert durch den Standardfehler der Differenz der beiden Mittelwerte. Wenn also die Differenz der Mittelwerte gross und/oder der Standardfehler dieser Differenz klein ist, so ist t weit von Null entfernt.\n",
        "\n",
        "Was sagt uns der berechnete *t*-Wert nun? Um daraus etwas schlussfolgern zu können, müssen wir ihn mit der theoretischen *t*-Verteilung vergleichen. Für diese gilt:\n",
        "\n",
        "- Sie ist symmetrisch, mit einem Maximum bei 0.\n",
        "- Der genaue Kurvenverlauf variiert in Abhängigkeit von den Freiheitsgraden (*degrees of freedom* = df). Bei vielen Freiheitsgraden, d. h. einer grossen Stichprobengrösse (mehr dazu, wie sich die Stichprobenzahl in Freiheitsgrade übersetzt, folgt später), nähert sicht die *t*-Verteilung einer Normalverteilung (auch *z*-Verteilung genannt).\n",
        "\n",
        "Die allgemeine Konvention in der Statistik ist, dass die Nullhypothese dann verworfen wird, wenn die berechnete Prüfgrösse extremer ist als 95 % aller möglichen Werte bei der gegebenen Stichprobengrösse. Beim t-Test fragt man also, ob der berechnete *t*-Wert extremer ist als 95 % aller *t*-Werte der der Stichprobengrösse entsprechenden *t*-Verteilung. Da unsere Hypothese ungerichtet ist (also ist verschieden und nicht ist grösser/ist kleiner), benötigen wir einen zweiseitigen *t*-Test. Dieser bestimmt die \"kritischen\" *t*-Werte (*t~c~*), indem auf beiden Seiten quasi 2.5 % der Fläche des Integrals unter der Wahrscheinlichkeitsverteilung abgeschnitten werden, wie die folgende Abbildung veranschaulicht:\n",
        "\n",
        "![](./myMediaFolder/media/image8.png){width=\"5.96875in\" height=\"2.4982327209098862in\"}\\ (aus Quinn & Keough 2002)\n",
        "\n",
        "Wenn also der berechnete *t*-Wert \\> *t~c~* (oder < --*t~c~*) ist, dann sind wir **hinreichend sicher**, dass sich die Mittelwerte nicht nur in der Stichprobe, sondern auch in der Grundgesamtheit unterscheiden.\n",
        "\n",
        "### Fehler I. und II. Art\n",
        "\n",
        "Wichtig ist, dass es in der physischen Realität nie eine absolute Sicherheit gibt. Wenn wir also wir feststellen, dass die Wahrscheinlichkeit, dass die Nullhypothese zutrifft (oder präziser: dass das vorliegende Ergebnis oder ein extremeres bei Zutreffen der Nullhypothese aufgetreten wäre) kleiner als 5 % ist, gibt es eben doch Fälle gibt, in denen wir fälschlich die Nullhypothese verwerfen, d. h. das Vorliegend eines Effektes bejahen, obwohl er in der Realität (d. h. der Grundgesamtheit) nicht auftritt. Das bezeichnet man als **Typ I-Fehler**. Umgekehrt kann es aber auch passieren, dass man die Nullhypothese aufgrund des statistischen Tests beibehält, also einen Effekt nicht nachweist, obwohl er in der Realität existiert (**Typ II-Fehler**). Diese beiden Phänomene sind in der folgenden beiden Abbildungen visualisiert:\n",
        "\n",
        "![](./myMediaFolder/media/image9.emf.png){width=\"5.038331146106737in\" height=\"2.8962259405074366in\"}\n",
        "\n",
        "![](./myMediaFolder/media/image10.png){width=\"5.4375in\" height=\"2.4479166666666665in\"}\\ (aus Quinn & Keough 2002)\n",
        "\n",
        "Wie man der zweiten Visualisierung entnehmen kann, steigt die Wahrscheinlichkeit eines Typ II-Fehlers, je weiter man die akzeptierte Wahrscheinlichkeit eines Typ I-Fehlers reduziert. In der Statistik wir im Allgemeinen sehr viel stärker auf die Minimierung von Typ I-Fehlern fokusiert, d. h. man will vermeiden, dass man fälschlich einen Effekt behauptet, der in Realität nicht existiert, während es als weniger problematisch angesehen wird, einen vorhandenen, aber dann sehr schwachen Effekt, nicht nachgewiesen zu haben.\n",
        "\n",
        "### p-Werte und Signifikanzniveaus\n",
        "\n",
        "Signifikanzniveaus und p-Werte sind zentrale Termini in der am weitesten verbreiteten inferenz-statistischen Schule, der ***frequentist statistics*** („Frequentistische Statistik\", aber ich habe den Begriff noch nie im Deutschen gehört). Deren Grundideen sind:\n",
        "\n",
        "- Die beobachteten Werte werden als eine Beobachtung unter vielen     möglichen Beobachtungen interpretiert, die zusammen eine     **Häufigkeitsverteilung** ergeben.\n",
        "\n",
        "- Es wird eine **einzige wahre Beschreibung der Realität** angenommen,     der man sich mit bestimmten Irrtumswahrscheinlichkeiten annähern     kann\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "In der *frequentist statistics*, sind die *p*-Werte das zentrale „Gütemass\". Als ***p*-Wert** bezeichnet man dabei die berechnete **Wahrscheinlichkeit eines Typ I-Fehlers**. Der *p*-Wert bezeichnet also die Wahrscheinlichkeit, dass man aufgrund des statistischen Tests einen Zusammenhang feststellt, ohne dass dieser in Realität existiert.\n",
        "\n",
        ":::\n",
        "\n",
        "Als **statistisch signifikant** bezeichnet man Ergebnisse, die unter einem bestimmten *p*-Wert liegen. Diese Schwellenwerte sind Konventionen und nicht „gottgegeben\". Traditionell werden drei Signifikanzniveaus verwendet (wozu R noch ein viertes hinzugefügt hat, das man mit „marginal signifikant\" bezeichnen könnte), die wie folgt notiert werden:\n",
        "\n",
        "| Notation | Bedeutung   |                                                      |\n",
        "|:---------|-------------|:-----------------------------------------------------|\n",
        "| \\*\\*\\*   | *p* < 0.001 | höchst signifikant; *highly significant*             |            \n",
        "| \\*\\*     | *p* < 0.01  | hoch signifikant; *very significant*                 |     \n",
        "| \\*       | *p* < 0.05  | signifikant; *significant*                           |\n",
        "| .        | *p* < 0.1   | marginal signifikant; *marginally significant*       |          \n",
        "\n",
        "Die Schwellenwerte der Signifikanzniveaus (d. h. Schwellenwerte für akzeptierte Typ I-Fehler) werden auch mit α bezeichnet. Was man in einer Arbeit als signifikant betrachtet, sollte man vor Beginn der Untersuchung festlegen und im Methodenteil schreiben („als Signifikanzschwelle verwenden wir α **=** 0.05\" oder „als signifikant sehen wir Ergebnisse mit p **<** 0.05 an\"). Es bietet sich normalerweise an, bei der allgemeinen Konvention von α **=** 0.05 zu bleiben, es sei denn es sprechen spezifische Gründe dagegen. Ein Grund könnte sein, dass die Verwerfung der Nullhypothese/Annahme der Forschunshypothese schwerwiegende Folgen hätte und man sich daher besonders sicher sein will.\n",
        "\n",
        "Da sich ober-und unterhalb der genannten Schwellen nichts Fundamentales ändert, sollte man grundsätzlich die exakten *p*-Werte mit drei Nachkommastellen (z.B. „*p* = 0.038\"bzw. wenn noch niedriger als „*p* < 0.001\") angeben. Zur besseren Lesbarkeit können zusätzlich die korrespondierenden Signifikanzniveaus angegeben werden.\n",
        "\n",
        "Es ist wichtig, sich bewusst zu sein, dass **statistisch signifikant nicht gleichbedeutend ist mit biologisch bzw. sozialwissenschaftlich bedeutsam**. Ein Effekt kann statistisch hochsignikant sein (wg. grosser Stichprobengrösse) und trotzdem inhaltlich bedeutungslos (da die Effektgrösse minimal ist). Umgekehrt kann ein inhaltlich bedeutsamer Effekt evtl. nicht statistisch signifikant nachgewiesen werden, wenn man extrem wenige Replikate hatte.\n",
        "\n",
        "Mit dem Kriterium „statistische Signifikanz\"/*p*-Wert trennen wir unsere Ergebnisse in einem ersten Schritt in jene, die wir für **belastbar** halten und jene, die mit grosser Wahrscheinlichkeit „zufällig\" („Rauschen in den Daten\", Messungenauigkeit, etc.) zustande gekommen sind. Bei den belastbaren müssen wir dann immer noch ihre **Relevanz** (also die Effektstärke) beurteilen.\n",
        "\n",
        "## t-Test (für eine metrische Variable im Vergleich von zwei Gruppen)\n",
        "\n",
        "Bei den beiden vorausgehenden einfachen Tests haben wir jeweils binäre Daten bezüglich ihrer Häufigkeitsverteilung analysiert. Oft haben wir aber metrische Variablen als abhängige Grösse, etwa in unserem Blumenbeispiel:\n",
        "\n",
        "![](./myMediaFolder/media/image11.emf.png){width=\"1.968503937007874in\" height=\"1.9526837270341206in\"}\n",
        "\n",
        "H~0~: Die beiden Sorten unterscheiden sich nicht in der Blütengrösse.\n",
        "\n",
        "### Students und Welch t-Test\n",
        "\n",
        "Als statistisches Verfahren kommt **Students *t*-Test für zwei unabhängige Stichproben** zum Einsatz („Student\" ist das Pseudonym für William Sealy Gosset, dem Erfinder des Tests, dessen Arbeitsvertrag in der Privatwirtschaft das Publizieren von Ergebnissen verbot).\n",
        "\n",
        "$$\n",
        "t = \\frac{\\bar{X}_1-\\bar{X}_2}{s_p\\times \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n",
        "$$\n",
        "\n",
        "Wobei $s_p$ der gepoolten Varianz entspricht:\n",
        "\n",
        "$$\n",
        "s_p = \\sqrt{\\frac{(n_1-1)s^{2}_{X_{1}}+(n_2-1)s^{2}_{X_{2}}))}{n_1+n_2-2}}\n",
        "$$\n",
        "\n",
        "Der berechnete t-Wert wird mit der t-Verteilung für (*n*~1~ -- 1) + (*n*~2~ -- 1) Freiheitsgraden verglichen. Der klassische t-Test setzt Normalverteilung und gleiche Varianzen voraus:\n",
        "\n",
        "```{.r}\n",
        "t.test(blume$a,blume$b, var.equal=T)\n",
        "```\n",
        "\n",
        "Wenn Varianzgleichheit nicht gegeben ist, verwendet man Welch' *t*-Test. Dieser approximiert die Freiheitsgrade mit der Welch-Satterthwaite-Gleichung. Er setzt weiterhin Normalverteilung voraus, benötigt aber keine gleichen Varianzen. Welch' *t*-Test kann/sollte also immer verwendet werden, wenn keine vorherigen Tests auf Varianzgleichheit durchgeführt werden und ist daher Standard (default) in R:\n",
        "\n",
        "\n",
        "$$\n",
        "t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s\\frac{}{\\Delta}}\n",
        "$$\n",
        "\n",
        "Wobei\n",
        "\n",
        "$$\n",
        "s\\frac{}{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}\n",
        "$$\n",
        "\n",
        "\n",
        "```{.r} \n",
        "t.test(blume$a,blume$b, var.equal=F)\n",
        "t.test(blume$a,blume$b) \n",
        "```\n",
        "\n",
        "### Ein- und zweiseitiger t-Test\n",
        "\n",
        "Bislang war unsere Hypothese, dass irgendein Unterschied vorliegt (was, wie oben dargelegt, keine adäquate Forschungshypothese ist, sondern die implizite Hypothese, wenn man eine offene Frage formuliert, aber keine klare Theorie hat). Wenn es eine Theorie gibt, aus der sich eine klare Vorhersage treffen lässt, so enthält diese normalerweise auch eine Aussage über die Richtung des Effekts, also ob die Blüten von A grösser als jene von B sind oder umgekehrt. Dann verwendet man einen einseitigen *t*-Test, denn man je nach Richtung der Hypothese mit greater oder less spezifizieren muss. Bildlich gesprochen werden beim gängigen Signifikanzniveau von α = 0.05 beim beidseitigen *t*-Test je 2.5 % der Integralfläche links und rechts „abgeschnitten\", beim einseitigen *t*-Test dagegen 5 % auf einer Seite. Wenn der berechnete *t*-Wert in einem der abgeschnittenen „Dreiecke\" liegt, ist das Ergebnis signifikant.\n",
        "\n",
        "![](./myMediaFolder/media/image16.png){width=\"3.1496062992125986in\" height=\"3.080637576552931in\"}\\ (aus Quinn & Keough 2002)\n",
        "\n",
        "```{.r}\n",
        "t.test(blume$a,blume$b) #zweiseitig\n",
        "t.test(blume$a,blume$b, alternative=\"greater\") #einseitig\n",
        "t.test(blume$a,blume$b, alternative=\"less\") #einseitig\n",
        "```\n",
        "\n",
        "### Gepaarter und ungepaarter t-Test\n",
        "\n",
        "Bislang haben wir angenommen, dass die Individuen der beiden Sorten unabhängig voneinander jeweils zufällig ausgewählt wurden. Dann ist ein ungepaarter *t*-Test (*default*-Einstellung in R) richtig. Wenn jedoch je zwei Messwerte zusammengehören, etwa wenn je eine Pflanze der Sorten A und B gemeinsam in einem Topf wuchsen , so kommt ein gepaarter *t*-Test zur Anwendung. Da dieser mehr „Informationen\" zur Verfügung hat, hat er mehr statistische „*power*\", wird i. d. R. also zu stärker signifikanten Ergebnissen führen:\n",
        "\n",
        "```{.r}\n",
        "t.test(blume$a,blume$b, paired=T) #gepaarter t-Test\n",
        "```\n",
        "\n",
        "## Binomial-Test (für die Häufigkeitsverteilung einer binomialen Variablen)\n",
        "\n",
        "Der Binomial-Test ist eines der einfachsten statistischen Verfahren überhaupt. Er testet, ob die Verteilung einer binären Variable von einer Zufallsverteilung abweicht. Eine binomiale (binäre) Variable ist eine, die zwei mögliche Zustände hat, etwa lebend/tot, männlich/weiblich oder besser/schlechter. Wenn das Ergebnis zufällig wäre, müssten in der Stichprobe beide Ausprägungen ungefähr gleich häufig vertreten sein. Folglich testet der Binomialtest, wie wahrscheinlich es ist, dass die vorgefundene Häufigkeitsverteilung in der Stichprobe zustande gekommen wäre, wenn beide Zustände gleich häufig sind. Wenn diese Wahrscheinlichkeit < 0.05 ist, nimmt man in der Statistik gewöhnlich an, dass der Unterschied in der Stichprobe einem realen Unterschied in der Grundgesamtheit ist.\n",
        "\n",
        "Betrachten wir den Frauenanteil im schweizerischen Nationalrat als Beispiel. Im Jahr 2019 waren 84 von 200 Mitgliedern weiblich (42%). Nehmen wir in guter Näherung an, dass im Stimmvolk das Geschlechterverhältnis 1:1 ist: Kann die Abweichung von 50 % unter den Mitgliedern noch durch Zufall erklärt werden oder deutet das auf eine «Bevorzugung» von Männern bei der Kandidat:innenaufstellung und im Wahlvorgang hin. Die Antwort liefert der Binomialtest, dem man die Zahl der „Erfolge\" (weiblich: 82) und die Stichprobengrösse (200) übergeben muss:\n",
        "\n",
        "```{.r}\n",
        "binom.test(82,200)\n",
        "```\n",
        "\n",
        "\n",
        "```{default}\n",
        "     Exact binomial test\n",
        "    \n",
        "data: 84 and 200\n",
        "number of successes = 84, number of trials = 200, p-value = 0.02813\n",
        "alternative hypothesis: true probability of success is not equal to 0.5\n",
        "95 percent confidence interval:\n",
        " 0.3507439 0.4916638\n",
        "sample estimates:\n",
        "probability of success\n",
        "                  0.42\n",
        "```\n",
        "\n",
        "\n",
        "Der Unterschied ist also signifikant (*p* < 0.05), wir können die Nullhypothese («keine Bevorzugung von Männern») also verwerfen. Der Output sagt uns auch noch, dass ohne Bevorzugung/Benachteiligung eines Geschlechts der gegenwärtige Frauenanteil im Nationalrat nur zustande hätte kommen können, wenn der Frauenanteil im Stimmvolk zwischen 35 % und 49 % läge. Da dieser Bereich 50 % (also den der Nullhypothese ensprechenden Wert) nicht einschliesst, ist es logisch, dass diese verworfen wird. Der Test ist „symmetrisch\": Wir können also statt der Anzahl der weiblichen Nationalratsmitglieder auch jene der männlichen eingeben und bekommen den gleichen *p*-Wert\n",
        "\n",
        "```{.r}\n",
        "binom.test(116,200)\n",
        "\n",
        " Exact binomial test\n",
        "\n",
        " data: 116 and 200\n",
        "\n",
        " number of successes = 116, number of trials = 200, p-value = 0.02813\n",
        "```\n",
        "\n",
        "**alternative hypothesis: true probability of success is not equal to\n",
        "0.5**\n",
        "\n",
        "```{.r}\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "0.5083362 0.6492561\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "probability of success\n",
        "\n",
        "\n",
        "0.58\n",
        "```\n",
        "\n",
        "## Chi-Quadrat- bzw. Fishers Test (für die Assoziation zweier binomialer Variablen)\n",
        "\n",
        "Die Frage beim Assoziationstest ist eine ähnliche wie beim Binomialtest. Wiederum geht es um binomiale Variablen, dieses Mal aber nicht um eine einzige, sondern um zwei an denselben Objekten erhobene Variablen, deren Zusammenhang man wissen will. \n",
        "\n",
        "Im folgenden Beispiel wollen wir wissen, ob die Augenfarbe und die Haarfarbe von Personen miteinander zusammenhängen. Die einfachste Form des Assoziationstests setzt zwei binomiale/binäre Variablen voraus, wir müssen also z. B. grüne Augen ausschliessen oder mit einer der beiden anderen Augenfarben zusammenfassen. Unsere Beobachtungsergebnisse von 114 Personen könnten wie folgt aussehen: \n",
        "\n",
        "![](./myMediaFolder/media/image17.emf.png){width=\"3.543307086614173in\" height=\"0.6770122484689414in\"}\n",
        "\n",
        "Sind diese Werte so erwartbar unter der Nullhypothese, dass Augenfarbe und Haarfarbe unabhängig voneinander sind? Anders als beim Binomialtest oben ist die Nullhypothese jedoch nicht die Gleichverteilung aller Merkmale bzw. Merkmalskombinationen. Vielmehr gehen wir von der gegebenen Häufigkeit der vier Einzelmerkmale aus. Wir müssen also berechnen, mit welcher Wahrscheinlichkeit die Kombination blaue Augen -- helle Haare unter den 114 ProbantInnen auftreten sollte, wenn beide Merkmale unabhängig voneinander sind. Das geht folgendermassen:\n",
        "\n",
        "![](./myMediaFolder/media/image18.emf.png){width=\"4.724409448818897in\"\n",
        "height=\"0.9215758967629046in\"}\n",
        "\n",
        "![](./myMediaFolder/media/image19.emf.png){width=\"3.937007874015748in\"\n",
        "height=\"1.0698589238845144in\"}\n",
        "\n",
        "Die beobachteten Werte (z. B. 38 Personen mit blauen Augen/hellen Haare) unterscheiden sich deutlich von den erwarteten Werten unter der Nullhypothese (22.35 Personen). Aber ist das auch statistisch signifikant?\n",
        "\n",
        "### Chi-Quadrat-Test\n",
        "\n",
        "Der traditionelle statistische Test für diese Frage ist Pearsons Chi-Quadrat-Test (auch *Χ*^2^-Test geschrieben). Wie *t* ist *Χ*^2^ eine Teststatistik, die abhängig von den Freiheitsgraden (df) einer ganz bestimmten Kurve folgt.\n",
        "\n",
        "> $Χ^{2}$=$\\sum_{}^{}{\\frac{{(O\\  - E)}^{2}}{E},\\ mit\\ O = observed;E = expected}$\n",
        "\n",
        "![](./myMediaFolder/media/image20.png){width=\"2.5519477252843394in\"\n",
        "height=\"2.538961067366579in\"}\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Wir können den *Χ*^2^-Wert in unserem Fall einfach händisch berechnen:\n",
        "\n",
        "![](./myMediaFolder/media/image21.emf.png){width=\"4.724409448818897in\"\n",
        "height=\"1.5789916885389326in\"}\n",
        "\n",
        "Ist *Χ*² = 35.33 nun signifikant oder nicht? Dazu müssen wir noch die\n",
        "Freiheitsgrade berechnen und das Signifikanzniveau festlegen:\n",
        "\n",
        "- **Freiheitsgrade:** (Spalten -- 1) x (Zeilen -- 1) = (2 -- 1) x (2\n",
        "    -- 1) = 1\n",
        "\n",
        "- **Signifikanzlevel:** z.B. α = 0.05\n",
        "\n",
        "Traditionell hätte man den kritischen Wert für diese Kombination in\n",
        "einer gedruckten Tabelle nachgeschlagen. Wir fragen einfach R, wobei wir\n",
        "1 -- α (in unserem Fall 1-0.05) eingeben müssen, da wir wissen wollen,\n",
        "ob wir im äussersten rechten Teil der Verteilungskurve liegen, also\n",
        "extremer als 95 % der Werte unter der Nullhypothese keiner Assoziation.\n",
        "\n",
        "```{.r}\n",
        "qchisq(0.95,1)\n",
        "\n",
        "\n",
        "\\[1\\] 3.841495\n",
        "```\n",
        "\n",
        "Unser berechneter *Χ*²-Wert (35.33) ist viel grösser als der kritische\n",
        "Wert (3.84), also gibt es eine Assoziation zwischen den Variablen (d. h.\n",
        "die Kombinationen blau/hell und braun/dunkel sind überproportional\n",
        "häufig). Wenn wir, wie oben empfohlen, einen präzisen p-Wert für die\n",
        "Assoziation wollen, erhalten wir ihn folgendermassen (beachte, dass\n",
        "chisq.test eine Matrix als Argument benötigt):\n",
        "\n",
        "```{.r}\n",
        "count <- matrix(c(38,14,11,51), nrow = 2)\n",
        "\n",
        "chisq.test(count)\n",
        "\n",
        "Pearson's Chi-squared test with Yates' continuity correction\n",
        "data: count\n",
        "X-squared = 33.112, df = 1, p-value = 8.7e-09\n",
        "```\n",
        "\n",
        "Die Assoziation ist also höchst signifikant (*p* < 0.001).\n",
        "\n",
        "### Fishers exakter Test\n",
        "\n",
        "Für kleine Erwartungswerte in den Zellen (< 5) ist der Chi-Quadrat-Test\n",
        "nicht zuverlässig. Dafür gibt es Fishers exakten Test.\n",
        "\n",
        "```{.r}\n",
        "count2 <- matrix(c(3,5,9,1),nrow=2)\n",
        "\n",
        "\n",
        "fisher.test(count2)\n",
        "\n",
        "\n",
        "Fisher's Exact Test for Count Data\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "data: count\n",
        "\n",
        "\n",
        "p-value = 0.04299\n",
        "\n",
        "\n",
        "alternative hypothesis: true odds ratio is not equal to 1\n",
        "\n",
        "\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "0.001280876 1.102291244\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "odds ratio\n",
        "\n",
        "\n",
        "0.08026151\n",
        "```\n",
        "\n",
        "Man kann/sollte Fishers exakten Test jedoch grundsätzlich verwenden, da\n",
        "er mit der heutigen Rechenleistung von Computern kein Problem mehr\n",
        "darstellt. Angewandt auf unseren Haarfarben/Augenfarben-Datensatz ergibt\n",
        "sich:\n",
        "\n",
        "```{.r}\n",
        "count\n",
        "\n",
        "\n",
        "\\[,1\\] \\[,2\\]\n",
        "\n",
        "\n",
        "\\[1,\\] 38 11\n",
        "\n",
        "\n",
        "\\[2,\\] 14 51\n",
        "\n",
        "\n",
        "fisher.test(count)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "Fisher's Exact Test for Count Data\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "data: count\n",
        "\n",
        "\n",
        "p-value = 2.099e-09\n",
        "\n",
        "\n",
        "alternative hypothesis: true odds ratio is not equal to 1\n",
        "\n",
        "\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "4.746351 34.118920\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "odds ratio\n",
        "\n",
        "\n",
        "12.22697\n",
        "```\n",
        "\n",
        "Wie man der Ausgabe entnehmen kann ist die Teststatistik hier die\n",
        "sogenannte ***odds ratio***, ein Term für den es keine gute deutsche\n",
        "Übersetzung gibt. Sie bezeichnet die **Wahrscheinlichkeit des Eintretens\n",
        "geteilt durch die Wahrscheinlichkeit des Nichteintretens**. Aus der\n",
        "Umgangssprache und Wettspielen sind wir bereits vertraut mit *odds\n",
        "ratios*: «50:50-Chancen» bezeichnen nichts anderes als eine *odds ratio*\n",
        "von 1 (50 / 50 = 1). Bei einem Assoziationstest ist entspricht der *odds\n",
        "rati*o die Multiplikation der Wahrscheinlichkeiten auf der einen\n",
        "Diagonalen geteilt durch jene der anderen Diagonalen, also (38 x 51) /\n",
        "(14 x 11).\n",
        "\n",
        "## Wie berichte ich statistische Ergebnisse?\n",
        "\n",
        "### Welche relevanten Informationen benötige ich und wo finde ich sie?\n",
        "\n",
        "Die Ergebnisausgaben in R sind mitunter umfangreich. Da kommt es darauf\n",
        "an, effizient herausfiltern zu können, was welche Information darin\n",
        "bedeutet und welche davon man in einer wissenschaftlichen arbeit\n",
        "braucht. Hier ist die Ausgabe des vorhergehenden gepaarten *t*-Tests:\n",
        "\n",
        "```{.r}\n",
        "Paired t-test\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "data: blume$a and blume$b\n",
        "\n",
        "\n",
        "t = 3.4821, df = 9, p-value = 0.006916\n",
        "\n",
        "\n",
        "alternative hypothesis: true difference in means is not equal to 0\n",
        "\n",
        "\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "1.366339 6.433661\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "mean of the differences\n",
        "\n",
        "\n",
        "3.9\n",
        "```\n",
        "\n",
        "Welche Informationen davon werden benötigt:\n",
        "\n",
        "1.  Name des Tests (**Methode**)\n",
        "\n",
        "2.  Signifikanz/*p*-Wert (**Verlässlichkeit des Ergebnisses**)\n",
        "\n",
        "3.  Effektgrösse und -richtung (**unser eigentliches Ergebnis!**)\n",
        "\n",
        "4.  ggf. Wert der Teststatistik und Freiheitsgrade\n",
        "    („Zwischenergebnisse\")\n",
        "\n",
        "Werfen wir noch einmal einen Blick auf den Output von R:\n",
        "\n",
        "```{.r}\n",
        "Paired t-test\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "data: blume$a and blume$b\n",
        "\n",
        "\n",
        "t = 3.4821, df = 9, p-value = 0.006916\n",
        "\n",
        "\n",
        "alternative hypothesis: true difference in means is not equal to 0\n",
        "\n",
        "\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "1.366339 6.433661\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "mean of the differences\n",
        "\n",
        "\n",
        "3.9\n",
        "```\n",
        "\n",
        "Wichtig ist es, bei aller „Begeisterung\" für die *p*-Werte nicht unser\n",
        "eigentliches Ergebnis zu vergessen, d. h. die Antwort auf die Frage ob\n",
        "die Blüten von A oder von B grösser sind und wenn ja wie stark (blau).\n",
        "Ob Freiheitsgrade und der Wert der Teststatistik angegeben werden\n",
        "müssen, darüber gehen die Geschmäcker auseinander. Wenn man die Daten\n",
        "korrekt in R eingegeben hat, spezifiziert R die Freiheitsgrade\n",
        "automatisch und bei gegebenen Freiheitsgraden ist die Beziehung von *t*\n",
        "zu *p* eindeutig. Deshalb genügt es m. E. *p* anzugeben. (Aber wenn der\n",
        "Betreuer oder die Editorin auch noch *t* und *df* haben wollen, dann\n",
        "sollte man sie parat haben). Ein adäquater Satz im Ergebnisteil, der den\n",
        "obigen R *output* zusammenfasst, lautet daher:\n",
        "\n",
        "Die Blütengrösse unterschied sich hochsignifikant zwischen den beiden\n",
        "Sorten mit einem Mittelwert von 15.3 cm² für Sorte A und 11.4 cm² für\n",
        "Sorte B (gepaarter *t*-Test, *p* = 0.007, *t* = 3.482, FG = 9).\n",
        "\n",
        "Oder auf Englisch:\n",
        "\n",
        "Flower sizes differed very significantly between the two cultivars with\n",
        "a mean size of 15.3 cm² in cultivar A and 11.4 cm² in the cultivar B\n",
        "(paired *t*-test, *p* = 0.007, *t* = 3.482, df = 9).\n",
        "\n",
        "### Text, Tabelle oder Abbildung?\n",
        "\n",
        "Hier kommen ein paar wichtige Vorgaben und Empfehlungen:\n",
        "\n",
        "- **Jedes Ergebnis nur 1x ausführlich darstellen**, entweder als\n",
        "    Abbildung, in einer Tabelle oder als Text\n",
        "\n",
        "- Wenn als Abbildung oder Tabelle, dann **im Text mit einem\n",
        "    zusammenfassenden Statement darauf verweisen**, das nicht alle\n",
        "    Details wiederholt\n",
        "\n",
        "- **Signifikante und nicht signifikante Ergebnisse** berichten\n",
        "\n",
        "- **Gängige Strategie:**\\\n",
        "    - **Abbildungen:** für die wichtigsten signifikanten Ergebnisse\\\n",
        "    - **Tabellen:** für die weiteren signifikanten Ergebnisse\\\n",
        "    - **Nur Text:** für die nicht signifikanten Ergebnisse\n",
        "\n",
        "### Abbildungen in wissenschaftlichen Arbeiten\n",
        "\n",
        "Zumindest für die wichtigsten signifikanten Ergebnisse produzieren wir\n",
        "normalerweise Abbildungen. Dabei ist es wichtig, die folgenden\n",
        "Prinzipien zu beherzigen:\n",
        "\n",
        "- Abbildungen (und Tabellen) sollten **ohne den zugehörigen Text\n",
        "    informativ** sein, d. h. normalerweise *p*-Werte in der\n",
        "    Abbildung/Tabelle bzw. Unter-/Überschrift angeben\n",
        "\n",
        "- **Achsen sind verständlich beschriftet** (ausgeschriebene\n",
        "    Variablennamen mit Einheit)\n",
        "\n",
        "- **Keine Abbildungsüberschrift** (es gibt die Legende in der\n",
        "    Abbildungsunterschrift)\n",
        "\n",
        "- Keine überflüssigen Elemente (z. B. Rahmen, farbiger Hintergrund,\n",
        "    horizontale und vertikale Linien)\n",
        "\n",
        "- Klarer **Kontrast**, ausreichende **Linienstärke** und\n",
        "    **Schriftgrösse.**\n",
        "\n",
        "### Abbildungen mit „base R\" oder mit ggplot2?\n",
        "\n",
        "Im Folgenden visualisiert mit den Boxplots, die zum *t*-Test gehören.\n",
        "\n",
        "In „base R\" geht das folgendermassen:\n",
        "\n",
        "```{.r}\n",
        "boxplot(size\\~cultivar,data=blume.long)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image22.png){width=\"3.694805336832896in\"\n",
        "height=\"1.7484011373578303in\"}\n",
        "\n",
        "In ggplot2 geht es folgendermassen (mit *default*-Einstellungen):\n",
        "\n",
        "```{.r}\n",
        "library(ggplot2)\n",
        "\n",
        "\n",
        "ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image23.png){width=\"3.543307086614173in\"\n",
        "height=\"2.488674540682415in\"}\n",
        "\n",
        "Gut ist, dass die Achsen automatisch beschriftet wurden. Störend ist der\n",
        "graue Hintergrund (reduziert Kontrast) und die weissen Gitternetzlinien\n",
        "(übeflüssig und dank des zu geringen Kontrasts eh kaum zu sehen).\n",
        "\n",
        "Man kann das in ggplot2 durch Wahl des vordefinierten theme_classic\n",
        "optimieren:\n",
        "\n",
        "**ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+\n",
        "theme_classic()**\n",
        "\n",
        "![](./myMediaFolder/media/image24.png){width=\"3.543307086614173in\"\n",
        "height=\"2.5234076990376204in\"}\n",
        "\n",
        "Das Ergebnis ist insgesamt OK, allerdings sind die Linien zu fein und\n",
        "die Schrift zu klein -- jeweils relativ zur Gesamtgrösse der Abbildung.\n",
        "\n",
        "Man kann weiter optimieren durch Hinzufügen weiterer Steuerelemente:\n",
        "\n",
        "**ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) +\n",
        "theme_classic()+ theme(axis.line = element_line(size=1), axis.ticks =\n",
        "element_line(size=1),axis.text = element_text(size = 20), axis.title =\n",
        "element_text(size = 20))**\n",
        "\n",
        "![](./myMediaFolder/media/image25.png){width=\"3.543307086614173in\"\n",
        "height=\"2.5715080927384077in\"}\n",
        "\n",
        "Jetzt passt es... Einzig könnte man noch den *p*-Wert einblenden und die\n",
        "Achsenbeschriftungen jeweils mit einem Grossbuchstaben beginnen.\n",
        "\n",
        "Ob man die Grafiken mit ggplot2 oder base R gestaltet, sei jedem selbst\n",
        "überlassen. Beides hat Vor- und Nachteile. Was man aber vermeiden\n",
        "sollte, sind die Ausgaben von ggplot2 mit default-Einstellungen, da\n",
        "diese gängigen Standards für gute Grafiken widerspechen. Hier noch\n",
        "einmal zusammengefasst die Vor- und Nachteile beider Systeme:\n",
        "\n",
        "```{.r}\n",
        "Base R:\n",
        "```\n",
        "\n",
        "- Einfache Syntax, daher geeignet für schnelles Plotten\n",
        "- ABER: Syntax variiert zwischen verschiedenen Plottbefehlen\n",
        "- ABER: «Finetunen» von Grafiken oftmals umständlich oder gar nicht\n",
        "    möglich\n",
        "- Geeignet für Vektoren (ggplot2 braucht dataframes o.ä)\n",
        "- Geeignet für das Plotten von Modellen (plot(lm())\n",
        "- Einfaches Plotten der Modelldiagnostik (plot(summary())\n",
        "\n",
        "\n",
        "**Vorteile ggplot2:**\n",
        "\n",
        "- Leistungsfähige, universelle Syntax, daher leicht anpassbar an den Bedarf, wenn man das Prinzip erst einmal verstanden hat\n",
        "- Viele Funktionen \"out of the box\"\n",
        "- Einfachere Gestaltungsmöglichkeit (Farbskalen usw.)\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- Wissenschaftliche Forschung zielt in der Regel entweder auf das\n",
        "    **Generieren oder das Testen von Hypothesen**.\n",
        "\n",
        "- **Inferenzstatistik** ist das Set statistischer Verfahren (Tests),\n",
        "    das sowohl für das Testen als auch das Generieren von Hypothesen)\n",
        "    verwendet wird.\n",
        "\n",
        "- Inferenzstatistik ist notwendig, um zu bestimmen, **wie\n",
        "    wahrscheinlich ein beobachtetes Muster durch angenommenen\n",
        "    EInflussgrössen (Variablen)** und nicht durch (a) Messfehler\n",
        "    oder (b) andere «Störgrössen» **hervorgerufen wurde**.\n",
        "\n",
        "- Der ***p*-Wert ist die Wahrscheinlichkeit eines Typ\n",
        "    I-Fehlers**, d. h. einen Effekt zu berichten, wo keiner ist; nach\n",
        "    üblicher Konvention wird ein Effekt dann als hinreichend sicher\n",
        "    (signifikant) angesehen, wenn *p* < 0.05.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "<!-- -->\n",
        "```\n",
        "\n",
        "- Mit einem **Chi-Quadrat-Test** (oder besser mit Fishers exaktem\n",
        "    Test) kann man auf eine **Assoziation zwischen zwei kategorialen\n",
        "    Variablen** testen.\n",
        "\n",
        "- Mit einem ***t*-Test kann man** auf **Unterschiede in den\n",
        "    Mittelwerten einer metrischen Variablen** zwischen zwei Gruppen\n",
        "    testen.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed.\n",
        "John Wiley & Sons, Chichester, UK: 339 pp.**\n",
        "\n",
        "- Chapter 1 -- Fundamentals\n",
        "- Chapter 6 -- Two Samples\n",
        "\n",
        "Quinn, G.P. & Keough, M.J. 2002. *Experimental design and data analysis\n",
        "for biologists*. Cambridge University Press, Cambridge, UK: 537 pp.\n",
        "\n",
        "# Statistik 2: Einführung in lineare Modelle\n",
        "\n",
        "**In Statistik 2 lernen die Studierenden die Voraussetzungen und die\n",
        "praktische Anwendung „einfacher\" linearer Modelle in R (sowie teilweise\n",
        "ihrer „nicht-parametrischen\" bzw. „robusten\" Äquivalente). Am Anfang\n",
        "steht die Varianzanalyse (ANOVA) als Verallgemeinerung des *t*-Tests,\n",
        "einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es\n",
        "um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests\n",
        "und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit\n",
        "Korrelationen, die auf einen linearen Zusammenhang zwischen zwei\n",
        "metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen\n",
        "einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer\n",
        "Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe\n",
        "linearer Modelle (Befehl lm in R) auszeichnet.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *wisst, welche Voraussetzungen parametrische (und\n",
        "    nicht-parametrische) Tests haben und welche Alternativen euch bei\n",
        "    wesentlichen Verletzungen zur Verfügung stehen;*\n",
        "\n",
        "- *könnt eine ANOVA in R durchführen, versteht ihre Ergebnisse und\n",
        "    könnt diese adäquat in Text und Abbildungen dokumentieren;*\n",
        "\n",
        "- *habt den Unterschied zwischen Korrelationen und Regressionen\n",
        "    verstanden und könnt sie in R implementieren;*\n",
        "\n",
        "- *kennt die Voraussetzungen und Gemeinsamkeiten aller linearen\n",
        "    Modelle; und*\n",
        "\n",
        "- *wisst, warum es nach der Berechnung eines linearen Modelles\n",
        "    essenziell ist, die Residuen zu checken, und könnt die\n",
        "    diagnostischen Grafiken von R dazu interpretieren.*\n",
        "\n",
        "## Varianzanalyse (ANOVA): Einstieg\n",
        "\n",
        "### Einfaktorielle Varianzanalyse (One-Way ANOVA)\n",
        "\n",
        "Eine ANOVA (*Analysis of variance*) ist die Verallgemeinerung des\n",
        "*t*-Tests für mehr als zwei Gruppen (*Factor levels*). Auch hier wollen\n",
        "wir wissen, **ob/wie sich die Mittelwerte der abhängigen Variablen\n",
        "zwischen den Gruppen unterscheiden**. Varianzanalyse heisst das\n",
        "Verfahren, weil der statistische Test zur Beantwortung der Frage das\n",
        "**Verhältnis zweier Varianzen** testet. Was es mit den zwei Varianzen\n",
        "auf sich hat, ist im Folgenden erklärt.\n",
        "\n",
        "Gehen wir zurück zu unserem Blumenbeispiel. Die Idee der ANOVA ist, dass\n",
        "die Mittelwerte der Blütengrössen der beiden Sorten dann verschieden\n",
        "sind, wenn die Summe der Abweichungen (Residuen) vom Gesamtmittelwert\n",
        "„signifikant\" grösser ist als die Summe der Abweichungen von den\n",
        "Sortenmittelwerten. Das ist in der folgenden Abbildung veranschaulicht.\n",
        "Die Punkte stellen die 20 Messwerte der Blütengrössen dar, wobei sie in\n",
        "der rechten Teilabbildung nach Sorten gruppiert sind. Der\n",
        "Gesamtmittelwert links und die beiden Sortenmittelwerte rechts sind als\n",
        "horizontale Linien dargestellt. Die vertikalen Linien sind die Residuen,\n",
        "als der Anteil der Varianz, welcher durch das jeweilige statistische\n",
        "Modell nicht erklärt wird. Das Modell links ist, dass die Blüten\n",
        "einheitlich gross sind, unabhängig von der Sorte, während das komplexere\n",
        "Modell rechts unterschiedliche Mittelwerte abhängig von der Sorte\n",
        "annimmt.\n",
        "\n",
        "![](./myMediaFolder/media/image26.png){width=\"3.1496062992125986in\"\n",
        "height=\"3.5514074803149605in\"}![](./myMediaFolder/media/image27.png){width=\"3.1496062992125986in\"\n",
        "height=\"3.5514074803149605in\"}\n",
        "\n",
        "Varianz ist ein Mass für die Streuung von Werten um ihren Mittelwert.\n",
        "Mathematisch wird die Varianz wie folgt berechnet :\n",
        "\n",
        "**Varianz = Summe der Abweichungsquadrate / Freiheitsgrade\\\n",
        "***(Summe der Abweichungsquadrate = Sum of squares = SS)*\n",
        "\n",
        "Abweichungsquadrate sind dabei die quadrierten Werte der grünen (bzw.\n",
        "schwarzen und roten) vertikalen Linien in der obigen Abbildung. Die\n",
        "Distanzen werden quadriert, so dass negative Abweichungen gleichermassen\n",
        "zählen. Würde man nur die unquadrierten Werte aufsummieren, wäre das\n",
        "Ergebnis immer 0, da die horizontale Linie (der Mittelwert) ja genaus\n",
        "gelegt wurde, dass die positiven und negativen Abweichungen\n",
        "betragsmässig gleich sind. Ein zentraler Punkt der Varianzanalyse ist,\n",
        "dass sich die Gesamtsumme der Abweichungsquadrate (*Total* s*um of\n",
        "squares*) als die Summe zweier Teile (SSE und SSA) darstellen lässt:\n",
        "\n",
        "**\\\n",
        "SSY = SSE + SSA\\\n",
        "\\\n",
        "**SSY = *Total sum of squares\\\n",
        "*SSE = *Error sum of squares* (entsprechend der unerklärte Varianz =\n",
        "Residuen)\\\n",
        "SSA = *Sum of squares attributable to treatment* (hier: Sorte)\n",
        "\n",
        "Schauen wir das zunächst beim Blumen-Datensatz an. Dazu müssen wir die\n",
        "Daten, die wir bislang im sogenannten *wide format* hatten (eine Spalte\n",
        "für Blütengrösse A und eine zweite für Blütengrösse B) im *long format*\n",
        "bereitstellen (eine Spalte für die Sorte und eine für die Blütengrösse).\n",
        "Generell ist das *long format* empfehlenswert, da viel universeller und\n",
        "von den meisten statistischen Verfahren verlangt.\n",
        "\n",
        "```{.r}\n",
        "head(blume.long)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "cultivar size\n",
        "\n",
        "\n",
        "1 a 20\n",
        "\n",
        "\n",
        "2 a 19\n",
        "\n",
        "\n",
        "3 a 25\n",
        "\n",
        "\n",
        "4 a 10\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "11 b 8\n",
        "\n",
        "\n",
        "12 b 12\n",
        "\n",
        "\n",
        "13 b 9\n",
        "```\n",
        "\n",
        "Schauen wir uns zunächst noch einmal das Ergebnis als „normalen\" t-Test\n",
        "an:\n",
        "\n",
        "```{.r}\n",
        "t.test(size\\~cultivar, blume.long, var.equal=T) \n",
        "\n",
        "\n",
        "Two Sample t-test\n",
        "\n",
        "\n",
        "data: size by cultivar\n",
        "\n",
        "\n",
        "t = 2.0797, df = 18, p-value = 0.05212\n",
        "```\n",
        "\n",
        "**alternative hypothesis: true difference in means between group a and\n",
        "group b is not equal to 0**\n",
        "\n",
        "```{.r}\n",
        "95 percent confidence interval:\n",
        "\n",
        "\n",
        "-0.03981237 7.83981237\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "mean in group a mean in group b\n",
        "\n",
        "\n",
        "15.3 11.4\n",
        "```\n",
        "\n",
        "Nun nehmen wir dieselben Daten und analysieren sie mit einer\n",
        "Varianzanalyse. Der Befehl dazu ist aov (was für [a]{.underline}nalysis\n",
        "[o]{.underline}f [v]{.underline}ariance steht). Man kann sich die\n",
        "Ergebnisse der ANOVA mit summary und summary.lm anzeigen lassen und\n",
        "bekommt jeweils unterschiedliche Informationen (die wir beide\n",
        "benötigen):\n",
        "\n",
        "```{.r}\n",
        "summary(aov(size\\~cultivar))\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "cultivar 1 76.0 76.05 4.325 0.0521 .\n",
        "\n",
        "\n",
        "Residuals 18 316.5 17.58  \n",
        "\n",
        "\n",
        "summary.lm(aov(size\\~cultivar))\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) 15.300 1.326 11.54 9.47e-10 \\*\\*\\*\n",
        "\n",
        "\n",
        "cultivarb -3.900 1.875 -2.08 0.0521 .\n",
        "```\n",
        "\n",
        "Beim ersten Output (summary) sehen wir eine typische „ANOVA-Tabelle\" wie\n",
        "man sie als Ergebnis linearer Modelle erhält. Die Bedeutung der\n",
        "Abkürzungen ist wie folgt:\n",
        "\n",
        "- Df = *Degrees of freedom* (Freiheitsgrade)\n",
        "\n",
        "- Sum Sq = *Sum of squares* (Summe der Abweichungsquadrate)\n",
        "\n",
        "- Mean Sq = *Sum of squares / degrees of freedom* (Quotient der beiden\n",
        "    Werte)\n",
        "\n",
        "- F value = *Mean Sq (Treatment) / Mean Sq (Residuals)* (Quotient der\n",
        "    beiden mittleren Abweichungsquadrate)\n",
        "\n",
        "- Pr(\\>F) = *Probability to obtaine a more extreme F value under the\n",
        "    null hypothesis*\\\n",
        "    (*p*-Wert)\n",
        "\n",
        "Der F-Wert ist das Verhältinis der durch die Variable und die Residuen\n",
        "erklärten Varianzen (*Mean squares*), also 76.05 / 17.58 = 4.33. Der\n",
        "*F*-Wert (4.33) entsprichtdem quadrierte *t*-Wert (--2.08) aus der\n",
        "unteren Tabelle. Der *p*-Wert (0.052) in der obigen Tabelle ist also\n",
        "genau der gleiche wie im *t*-Test, was die Äquivalenz von ANOVA und\n",
        "*t*-Test zeigt. Dieser *p*-Wert steht für die Nullhypothese, dass sich\n",
        "die beiden Sorten nicht in ihrer Blütengrösse unterscheiden.\n",
        "\n",
        "Derselbe *p*-Wert taucht im summary.lm-Output unten in der zweiten Zeile\n",
        "auf. Aber für was steht der extrem kleine p-Wert in der ersten Zeile des\n",
        "summary.lm-Outputs (9.47 x 10^--10^)? In der Zeile steht *(Intercept)*,\n",
        "also Achsenabschnitt. Hier ist der vorhergesagte Mittelwert für die\n",
        "erste Sorte (Cultivar a) gemeint. Die Nullhypothese zu dieser Zeile ist,\n",
        "dass die Blütengrösse dieser Sorte = 0 ist. Da Blütengrössen immer\n",
        "positive Werte haben (nie negativ und für eine existierende Blüte auch\n",
        "nie 0), ist das keine sinnvolle/relevante Nullhypothese. In den\n",
        "allermeisten Fällen bezieht sich der *p*-Wert in der ersten Zeile eines\n",
        "summary.lm-Outputs auf eine unsinnige/irrelevante Nullhypothese und wir\n",
        "können/müssen ihn ignorieren. Eine weitere wichtige Information liefert\n",
        "uns die zweite Tabelle aber noch: die Effektgrösse und -richtung. Dazu\n",
        "müssen wir in die Spalte *Estimates* schauen, welche die sogenannten\n",
        "Parameterschätzungen enthält. Im Falle einer ANOVA enthält die\n",
        "*(Intercept)*-Zeile den geschätzten Mittelwert für die alphabetisch\n",
        "erste Kategorie (bei uns also Cultivar a), währen das *Estimate* in der\n",
        "Zeile cultivarb für den Unterschied im Mittelwert von Cultivar b vs.\n",
        "Cultivar a steht, hier steht also die biologisch relevante Information,\n",
        "sprich: die Blüten von Cultivar b sind im Mittel 3.9 cm² kleiner als\n",
        "jene von Cultivar a. Allerdings sind wir uns dieser Aussage nicht\n",
        "besonders sicher, da sie statistisch nur marginal signifikant ist (*p* =\n",
        "0.052).\n",
        "\n",
        "Wenn wir eine «echte» ANOVA mit drei oder mehr Kategorien durchführen,\n",
        "die also nicht mehr mit dem t-Test analysiert werden kann, sieht der\n",
        "Output vergleichbar aus, nur hat sich die Zahl der Freiheitsgrade in der\n",
        "ersten Zeile erhöht (immer Zahl der Kategorien -- 1, bei 3 Kategorien\n",
        "also 2).\n",
        "\n",
        "```{.r}\n",
        "summary(aov(size\\~cultivar))\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "cultivar 2 736.1 368.0 18.8 7.68e-06 \\*\\*\\*\n",
        "\n",
        "\n",
        "Residuals 27 528.6 19.6\n",
        "```\n",
        "\n",
        "In diesem Fall gibt es also höchstsignifikante Unterschiede in der\n",
        "Blütengrösse zwischen den drei Sorten. Wir könnten das Ergebnis kurz und\n",
        "prägnant wie folgt wiedergeben:\n",
        "\n",
        "Die Blütengrösse unterschied sich höchstsignifikant zwischen den drei\n",
        "Sorten (ANOVA, *p* < 0.001, *F*~2;27~ = 18.8; Abb. 1).\n",
        "\n",
        "![](./myMediaFolder/media/image28.png){width=\"2.9166765091863516in\"\n",
        "height=\"3.8155271216097986in\"}\n",
        "\n",
        "**Abb. 1. Boxplots der Blütengrössen der drei verglichenen Cultivare a,\n",
        "b und c (jeweils *n* = 10).**\n",
        "\n",
        "Zwei Anmerkungen: (1) Bei drei und mehr Kategorien kann man im Text\n",
        "nicht mehr effizient schreiben, welche Sorte sich wie von welcher\n",
        "anderen unterscheidet, deshalb bietet sich hier eher eine Visualisierung\n",
        "an (sofern die ANOVA signifikant ist). (2) Wenn man den *F*-Wert angeben\n",
        "möchte, so muss man im Subskript nachgestellt die Freiheitsgrade im\n",
        "Zähler (2) und im Nenner (27) angeben, die man der ANOVA-Tabelle\n",
        "entnehmen kann.\n",
        "\n",
        "### Post-hoc-Test (Tukey)\n",
        "\n",
        "In der vorhergehenden ANOVA wissen wir nun, dass es insgesamt ein\n",
        "signifikantes Muster gibt, dass also nicht alle drei Sorten der gleichen\n",
        "Grundgesamtheit angehören. Was wir nicht wissen, ist, welche Sorte sich\n",
        "von welcher anderen unterscheidet, und ggf. wie stark. Wenn die ANOVA\n",
        "insgesamt signifikant ist, muss das längst nicht heissen, dass jede\n",
        "Sorte sich von jeder anderen unterscheidet. Nun könnte man auf die Idee\n",
        "kommen, einfach für jedes Sortenpaar einen *t*-Test durchzuführen. Das\n",
        "Problem ist, dass man dann u. U. ziemlich viele Tests mit denselben\n",
        "Daten macht, und da summieren sich die Typ I-Fehlerraten schnell auf,\n",
        "sprich: bei vielen Tests werden rein zufällig manche ein signifikantes\n",
        "Ergebnis ergeben (mit α = 0.05 wird 5 % Irrtum zugelassen, d. h. im\n",
        "Durchschnitt liefert jeder zwanzigste Test ein falsch-positives\n",
        "Ergebnis). Um diesem Problem Rechnung zu tragen, gibt es sogenannte\n",
        "posthoc-Tests, die nach einer signifikanten ANOVA angewandt werden. Wenn\n",
        "die ANOVA nicht signifkant war, darf dagegen kein posthoc-Test angewandt\n",
        "werden! Der gängigste posthoc-Test ist jener von Tukey und findet sich\n",
        "u. a. im agricolae-Paket:\n",
        "\n",
        "```{.r}\n",
        "library(agricolae)\n",
        "\n",
        "\n",
        "aov.1 <- aov(size\\~cultivar, data=blume2)\n",
        "\n",
        "\n",
        "HSD.test(aov.1, \"cultivar\", group=FALSE, console=T)\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "Comparison between treatments means\n",
        "\n",
        "\n",
        "difference pvalue signif. LCL UCL\n",
        "\n",
        "\n",
        "a - b 3.9 0.1388 -1.006213 8.806213\n",
        "\n",
        "\n",
        "a - c -8.0 0.0011 \\*\\* -12.906213 -3.093787\n",
        "\n",
        "\n",
        "b - c -11.9 0.0000 \\*\\*\\* -16.806213 -6.993787\n",
        "```\n",
        "\n",
        "Das Ergebnis sagt uns, dass sich c von a und c von b, nicht aber b von a\n",
        "signifikant unterscheiden. Bei nur drei Kategorien kann man das noch so\n",
        "formulieren, bei vier, fünf oder mehr wird es aber schnell langatmig und\n",
        "komplex. Das lässt sich mit sogenannten homogenen Gruppen lösen. Hier\n",
        "versieht man die Kategorien mit gleichen Buchstaben, die sich nicht\n",
        "signifikant voneinander unterscheiden, ggf. kann dann eine Kategorie\n",
        "auch mehrere Buchstaben tragen. In unserem Fall wäre die Lösung also:\n",
        "\n",
        "- Cultivar a: A\n",
        "\n",
        "- Cultivar b: A\n",
        "\n",
        "- Cultivar c: B\n",
        "\n",
        "Diese Buchstaben kann man in die Ergebnisabbildung plotten oder als\n",
        "Superskript in einer Ergebnistabelle der Mittelwerte. Die folgende\n",
        "Abbildung zeigt ein Beispiel. Hier unterscheiden sich nur *High* und\n",
        "*Low* signifikant voneinander, da dies das einzige Paar ist, das keine\n",
        "gemeinsamen Buchstaben hat:\n",
        "\n",
        "![](./myMediaFolder/media/image29.png){width=\"3.627785433070866in\"\n",
        "height=\"3.45in\"}\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Hier ist noch gezeigt, wie man die Beschriftung in die Boxplots bekommt:\n",
        "\n",
        "```{.r}\n",
        "aov.2 <- aov(Sepal.Width \\~ Species, data=iris)\n",
        "\n",
        "\n",
        "HSD.test(aov.2, \"Species\", console=T)\n",
        "\n",
        "\n",
        "Treatments with the same letter are not significantly different.\n",
        "\n",
        "\n",
        "Sepal.Width groups\n",
        "\n",
        "\n",
        "setosa 3.428 a\n",
        "\n",
        "\n",
        "virginica 2.974 b\n",
        "\n",
        "\n",
        "versicolor 2.770 c\n",
        "```\n",
        "\n",
        "Die Buchstaben aus dem Output muss man dann manuell zur jeweiligen Art\n",
        "plotten (Reihenfolge der Arten beachten!)\n",
        "\n",
        "```{.r}\n",
        "boxplot(Sepal.Width \\~ Species, ylim=c(2,5), data=iris)\n",
        "\n",
        "\n",
        "text(1, 4.8, \"a\")\n",
        "\n",
        "\n",
        "text(2, 4.8, \"c\")\n",
        "\n",
        "\n",
        "text(3, 4.8, \"b\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image30.png){width=\"3.256872265966754in\"\n",
        "height=\"3.2152777777777777in\"}\n",
        "\n",
        "## Voraussetzung statistischer Verfahren\n",
        "\n",
        "In Statistik 1 wurde kurz erwähnt, dass jeder statistische Test auf\n",
        "bestimmten Annahmen bezüglich der Werteverteilung in der Grundgesamtheit\n",
        "beruht. Beim klassischen *t*-Test nach Student sind das die\n",
        "Normalverteilung und die Varianzhomogenität.\n",
        "\n",
        "### Parametrische vs. nicht-parametrische Verfahren\n",
        "\n",
        "Verfahren, die auf dem folgenden gängigen Set von Voraussetzungen\n",
        "beruhen, werden als **parametrische Verfahren** bezeichnet. Es sind dies\n",
        "zugleich die **„linearen Modelle\"** (doch zu diesem Begriff später\n",
        "mehr):\n",
        "\n",
        "1.  **Normalverteilung der [Residuen]{.underline}**\n",
        "\n",
        "2.  **Varianzhomogenität**\n",
        "\n",
        "3.  **Feste *x*-Werte**\n",
        "\n",
        "4.  **Unabhängigkeit der Beobachtungen / Zufällige Beprobung**\n",
        "\n",
        "Dem gegenüber gestellt werden so-genannte „nicht-parametrische\"\n",
        "Verfahren. Der Begriff ist allerdings sehr irreführend, da\n",
        "nicht-parametrische Verfahren nicht etwa keine Voraussetzungen haben,\n",
        "sondern meist nur geringfügig schwächere als parametrische Verfahren.\n",
        "Die **Voraussetzungen für die Anwendung gängiger nicht-parametrischer\n",
        "Verfahren** sind:\n",
        "\n",
        "1.  **Die Verteilung der Residuen kann einer beliebigen Funktion folgen,\n",
        "    muss aber für die verschiedenen Faktorlevels (Kategorien) gleich\n",
        "    sein**\n",
        "\n",
        "2.  **Feste *x*-Werte**\n",
        "\n",
        "3.  **Unabhängigkeit der Beobachtungen / Zufällige Beprobung**\n",
        "\n",
        "Diese beiden Listen, weisen auf zwei weitverbreitete Irrtümer in der\n",
        "Statistik hin, die in älteren Statistikbüchern regelmässig falsch\n",
        "dargestellt wurden und die auch heute noch in Statistikursen an\n",
        "Hochschulen oft falsch gelehrt werden:\n",
        "\n",
        "- Nur die Residuen des statistischen Models sollten normalverteilt\n",
        "    sein. Dagegen ist es gleichgültig, ob die Werte der abhängigen\n",
        "    Variablen normalverteilt sind und erst recht gilt das für die\n",
        "    unabhängigen Variablen.\n",
        "\n",
        "- Die Varianzhomogenität ist wichtiger als Normalverteilung der\n",
        "    Residuen.\n",
        "\n",
        "- Die naive Empfehlung, bei kleinsten Abweichungen von der\n",
        "    Varianzhomogenität oder Normalverteilung auf ein\n",
        "    nicht-parametrisches Äquivalent auszuweichen, ist im besten Fall\n",
        "    unvorteilhaft (da nicht-parametrische Verfahren meist eine geringere\n",
        "    Teststärke haben), im schlimmsten Fall falsch (wie die\n",
        "    Voraussetzungen des nicht-parametrischen Verfahrens gleichermassen\n",
        "    verletzt sind).\n",
        "\n",
        "In der Folge ist zu beobachten, dass vielfach vorschnell und unnötig auf\n",
        "„nicht-parametrische\" Verfahren ausgewichen wird. **Dagegen sprechen\n",
        "viele Gründe dafür, in fast allen Fällen mit parametrischen Verfahren zu\n",
        "arbeiten**:\n",
        "\n",
        "- Parametrische Verfahren sind recht robust gegen die Verletzung der\n",
        "    Voraussetzung, d. h. sie liefern selbst recht starken Abweichungen\n",
        "    noch (fast) korrekte *p*-Werte:\n",
        "\n",
        "Laut Quinn & Keough (2002) haben Simulationen Folgendes gezeigt:\\\n",
        "- *n*~1~ = *n*~2~ = 6: selbst bei bis zu vierfacher SD noch korrekte\n",
        "*p*-Werte\n",
        "\n",
        "\\- *n*~1~ = 11, *n*~2~ = 21: Wenn SD~1~ = 4 SD~2~, dann entspricht ein\n",
        "berechneter *p* = 0.05\\\n",
        "in Wirklichkeit *p* = 0.16\n",
        "\n",
        "mit n1 und n2 = Stichprobengrösse für Faktorlevels 1 und 2 und SD =\n",
        "Standardabweichung\n",
        "\n",
        "- Die meisten komplexeren statistischen Verfahren existieren ohnehin\n",
        "    nur in einer parametrischen Variante.\n",
        "\n",
        "- Dank Datentransformationen und Generalisierungen linearer Modelle\n",
        "    kann man auch mit Nicht-Normalität der Residuen und\n",
        "    Varianzinhomogenität = Heteroskedasitzität umgehen.\n",
        "\n",
        "### Wie testet man die Voraussetzungen? (klassischer Weg)\n",
        "\n",
        "Der **„klassische\" (aber nicht zielführende!!!)** Rat in vielen\n",
        "Statistikbüchern/-kursen ist die Anwendung statistischer Tests für\n",
        "Normalität und Varianzhomognität. Für die Normalität (beachten, dass die\n",
        "Residuen, nicht dir Rohdaten getestet werden müssen, also im Fall einer\n",
        "ANOVA die Werte jeder Kategorie für sich). Es gibt u.a. den\n",
        "Kolmogorov-Smirnov-Test (mit Lillefors-Korrektur) und den\n",
        "Sharpiro-Wilks-Test:\n",
        "\n",
        "```{.r}\n",
        "shapiro.test(blume$b)\n",
        "```\n",
        "\n",
        "Für das Testen der Varianzhomogenität gibt es u.a. den *F*-Test zur\n",
        "Varianzhomogenität und den Levene-Test (im Paket car):\n",
        "\n",
        "```{.r}\n",
        "var.test(blume$a, blume$b)\n",
        "\n",
        "\n",
        "library(car)\n",
        "\n",
        "\n",
        "leveneTest(blume$a, blume$b,center = mean)\n",
        "```\n",
        "\n",
        "Wenn die *p*-Werte dieser Tests < 0.05 sind, dann liegt eine\n",
        "statistisch signifikante Abweichung von der jeweiligen Voraussetzung\n",
        "vor. Die klassische Konsequenz war, dann auf ein nicht-parametrisches\n",
        "Verfahren auszuweichen. Studierende und viele PraktikerInnen lieben\n",
        "diese scheinbar simple Schwarz-weiss-Sicht, die ein klares Prozedere\n",
        "vorzugeben scheint. Leider bringen diese Tests für die Entscheidung\n",
        "zwischen parametrischen und nicht-parametrischen Verfahren NICHTS. Die\n",
        "Gründe sind eigentlich einfach:\n",
        "\n",
        "- Die genannten Tests testen allesamt die Wahrscheinlichkeit der\n",
        "    Abweichung, nicht den Grad der Abweichung (wobei Letzteres der\n",
        "    relevante Punkt ist).\n",
        "\n",
        "- Damit werden einerseits bei kleinen Stichproben auch problematische\n",
        "    Abweichungen nicht erkannt, bei grossen Stichproben harmlose\n",
        "    Abweichungen dagegen „moniert\" (man sollte sich bewusst sein, dass\n",
        "    Variablen in der realen Welt niemals perfekt normalverteilt oder\n",
        "    perfekt varianzhomogen sind)\n",
        "\n",
        "Deshalb wird in modernen Lehrbüchern ausdrücklich davon abgeraten, die\n",
        "genannten Tests für diesen Zweck zu verwenden (z. B. Quinn & Keough\n",
        "2002).\n",
        "\n",
        "### Wie testet man die Voraussetzungen? (empfohlener Weg)\n",
        "\n",
        "Da die „klassischen\" numerischen Tests nichts helfen, bleibt nur ein\n",
        "Weg, selbst wenn er zunächst unbefriedigend und subjektiv erscheinen\n",
        "mag. Moderne statistische Lehrbücher empfehlen heute, Normalverteilung\n",
        "der Residuen und Varianzhomogenität visuell zu prüfen und nur bei groben\n",
        "Verletzungen über Gegenmassnahmen nachzudenken.\n",
        "\n",
        "Im Fall von *t*-Tests bzw. ANOVAs ist die einfachste Möglichkeit, nach\n",
        "Faktorlevels gruppierte Boxplots zu betrachten. Alternativ gingen auch\n",
        "Histogramme, allerdings sind diese nur bei grossen *n* aussagekräftig:\n",
        "\n",
        "![](./myMediaFolder/media/image31.png){width=\"1.968503937007874in\"\n",
        "height=\"2.724958442694663in\"}![](./myMediaFolder/media/image32.png){width=\"1.968503937007874in\"\n",
        "height=\"2.9772944006999125in\"}![](./myMediaFolder/media/image33.png){width=\"1.968503937007874in\"\n",
        "height=\"2.977295494313211in\"}\n",
        "\n",
        "Für die **Beurteilung der Varianzhomogenität** betrachtet man am besten\n",
        "die Höhe der Boxen im Boxplot. Wenn sie ähnlich hoch sind, ist alles OK,\n",
        "wenn sie sehr stark abweichen, hat man evtl. ein Problem. Sehr stark\n",
        "meint aber, siehe oben, wirklich sehr stark, d. h. wenn die Box in einer\n",
        "Kategorie mehr als 4-mal so hoch ist wie in einer anderen (bei\n",
        "gleichen/ähnlichen Replikatzahen), und ab mehr als doppelt so hoch bei\n",
        "erheblich verschiedenen Replikatzahlen. Im vorliegenden Fall ist die\n",
        "Varianz in Gruppe 1 etwa 2.5-mal so hoch wie in Gruppe 2, da die Zahl\n",
        "der Replikate aber identisch war, wäre das noch OK.\n",
        "\n",
        "Zur **Beurteilung der Normalverteilung** bzw. des entscheidenden Aspekts\n",
        "der Normalverteilung, der Symmetrie, sind ebenfalls die Boxplots\n",
        "aufschlussreich. Eine starke Verletzung liegt vor, wenn der Median weit\n",
        "ausserhalb der Mitte der Box liegt oder wenn der obere «whisker» viel\n",
        "länger als der untere ist.\n",
        "\n",
        "Ausserdem gibt es noch das ***Central Limit Theorem* (CLT)** in der\n",
        "Statistik. Dieses Theorem besagt, dass wenn eine betrachtete Variable\n",
        "selbst schon ein Mittelwert ist, sie zwingend einer Normalverteilung\n",
        "folgt. In diesem Fall ist also gar kein Test nötig/sinnvoll. Wenn man\n",
        "sich auf das CLT berufen will, kann man z. B. Quinn & Keough (2002)\n",
        "zitieren.\n",
        "\n",
        "### Was tun, wenn die Voraussetzungen verletzt sind? (nicht-parametrische Verfahren)\n",
        "\n",
        "Bei Verletzung der Voraussetzungen, kann man auf nicht-parametrische\n",
        "Verfahren ausweichen, was OK ist, wenn man sich völlig klar darüber ist,\n",
        "welche Voraussetzungen diese ihrerseits haben:\n",
        "\n",
        "Das nicht-parametrische Äquivalent zum *t*-Test ist der\n",
        "**Wilcoxon-Rangsummen-Test**. Er funktioniert, indem Werte in Ränge\n",
        "transformiert und summiert werden (W-statistic). Nachteile sind, dass er\n",
        "sehr konservativ ist (d. h. tendenziell zu hohe *p*-Werte schätzt) und\n",
        "zudem keine exakten *p*-Werte berechnen kann, wenn „Bindungen\" (*ties*)\n",
        "vorliegen (d. h. mehrere Beobachtungen identische Werte aufweisen).\n",
        "Ausserdem sei noch einmal betont, dass der Wilcoxon-Test zwar keine\n",
        "Annahme über die Verteilung der Werte pro Gruppe macht, jedoch\n",
        "voraussetzt, dass diese in jeder Gruppe gleich ist.\n",
        "\n",
        "```{.r}\n",
        "wilcox.test(blume$a, blume$b)\n",
        "```\n",
        "\n",
        "Ferner gibt es **Randomisierungs-*t*-Tests**. Diese haben den Vorteil,\n",
        "dass keine Annahme über die Verteilung getroffen werden muss (die\n",
        "Verteilung wird aus den Daten generiert). Zugleich müssen die\n",
        "Beobachtungen noch nicht einmal unabhängig sein. Allerdings testet man\n",
        "hier strenggenommen auch nicht auf Unterschiede in den\n",
        "Grundgesamtheiten, sondern ermittelt die Wahrscheinlichkeit, die\n",
        "beobachteten Unterschiede zufällig erzielt zu haben. Wer mehr über\n",
        "Randomisierungs-Tests wissen will, findet in Logan (2010: 148--150)\n",
        "weitergehende Infos.\n",
        "\n",
        "**Im Fall der ANOVA gibt es zwei Situationen:**\n",
        "\n",
        "\\(1\\) Wir haben starke **Abweichungen von der Normalverteilung** der\n",
        "Residuen, aber **ähnliche Varianzen**. Dann kann der Kruskal-Wallis-Test\n",
        "zum Einsatz kommen (ebenfalls ein Rangsummen-Test). Der zugehörige\n",
        "posthoc-Test ist der Dunn-Test mit Benjamin-Hochberg-Korrektur der\n",
        "*p*-Werte (wegen multiplem Testen):\n",
        "\n",
        "```{.r}\n",
        "kruskal.test(data = blume2, size\\~cultivar)\n",
        "\n",
        "\n",
        "library(FSA)\n",
        "```\n",
        "\n",
        "**dunnTest(data = blume2, size\\~cultivar,\\\n",
        "method = \"bh\")**\n",
        "\n",
        "\\(2\\) Wenn dagegen die **Varianzen sehr heterogen** sind, die **Residuen\n",
        "aber relativ normal/symmetrisch**, wie in der folgenden Abbildung, kann\n",
        "der **Welch-Test** eingesetzt werden:\n",
        "\n",
        "![](./myMediaFolder/media/image28.png){width=\"2.9166765091863516in\"\n",
        "height=\"3.8155271216097986in\"}\n",
        "\n",
        "```{.r}\n",
        "oneway.test(data=blume2, size\\~cultivar, var.equal=F)\n",
        "```\n",
        "\n",
        "### Was tun, wenn die Voraussetzungen verletzt sind? (Transformationen)\n",
        "\n",
        "Statt auf nicht-parametrische Verfahren auszuweichen, kann man auch\n",
        "Transformationen anwenden. Da es um die Verteilung der Residuen geht,\n",
        "muss primär die abhängige Variable für Transformationen in Betracht\n",
        "gezogen werden, manchmal hilft aber auch die Transformation einer\n",
        "unabhängigen Variablen (weitergehende Infos siehe Fox & Weisberg 2019:\n",
        "161--169).\n",
        "\n",
        "Wenn man über die Anwendung von Transformationen nachdenkt, sind zwei\n",
        "Aspekte relevant: (1) Entgegen manchen Behauptungen sind\n",
        "untransformierte Daten (linear Skala) nicht *per se*\n",
        "natürlicher/richtiger. Auch die lineare Skala ist eine Konvention. Viele\n",
        "Naturgesetze (z. B. unsere Sinneswahrnehmung) funktionieren dagegen auf\n",
        "einer Logarithmusskala. (2) Wenn man die abhängige Variable\n",
        "transformiert, muss man sich aber klar darüber sein, dass man dann\n",
        "strenggenommen Hypothesen über die transformierten Daten, nicht über die\n",
        "ursprünglichen Werte testet. Achtung: Wenn man die Analysen mit\n",
        "tranformierten Daten durchführt, darf man **für die Ergebnisdarstellung\n",
        "die Rücktransformation mittels der jeweiligen Umkehrfunktion** nicht\n",
        "vergessen!\n",
        "\n",
        "Gängige Transformation für die abhängige Variable sind die folgenden:\n",
        "\n",
        "**Logarithmus-Transformation:**\n",
        "\n",
        "- Gut bei rechtsschiefen Daten/wenn die Varianz mit dem Mittelwert\n",
        "    zunimmt.\n",
        "\n",
        "- Die „natürlichste\" Transformation.\n",
        "\n",
        "- Natürlicher Logarithmus (log) oder Zehnerlogarithmus (log10)\n",
        "    möglich.\n",
        "\n",
        "- Werte müssen \\> 0 sein.\n",
        "\n",
        "```{.r}\n",
        "log (*x* + Konstante)-Transformation:\n",
        "```\n",
        "\n",
        "- Findet man häufig in der Literatur, wenn abhängige Variablen\n",
        "    transformiert werden sollen, die auch Nullwerte enthalten\n",
        "\n",
        "- Es werden unterschiedliche Konstanten (*x*) addiert, mal 1, mal\n",
        "    0.01. Es ist aber völlig willkürlich, ob man 1000000 oder 0.00000001\n",
        "    oder 3.24567 addiert, hat aber starken Einfluss auf die Ergebnisse\n",
        "\n",
        "- Auch lassen sich die Ergebnisse nach so einer komplexen\n",
        "    Transformation schlecht interpretieren (da man dann ja eine\n",
        "    Hypothese über die transformierten Daten testet, s. o.)\n",
        "\n",
        "- In Übereinstimmung mit Wilson (2007) rate ich daher dringend von\n",
        "    derlei Transformationen ab!\n",
        "\n",
        "**Wurzeltransformation:**\n",
        "\n",
        "- Hat einen ähnlichen Effekt wie die Logarithmus-Transformation, lässt\n",
        "    sich im Gegensatz zu dieser auch beim Vorliegen von Nullwerten\n",
        "    anwenden (Werte müssen nur positiv sein).\n",
        "\n",
        "- Die „Stärke\" der Transformation kann man durch die Art der Wurzel\n",
        "    kontinuierlich einstellen: Quadratwurzel, Kubikwurzel, 4. Wurzel,...\n",
        "\n",
        "**„arcsine\"-Transformation:**\n",
        "\n",
        "asin(sqrt(x))\\*180/pi\n",
        "\n",
        "- Wurde traditionell für Prozentwerte (Proportionen) und andere\n",
        "    abhängige Variablen empfohlen, die zwischen 0 und 1 bzw. 0 und 100%\n",
        "    begrenzt sind (z. B. Quinn & Keough 2002).\n",
        "\n",
        "- Nach neueren Untersuchungen (Warton & Hui 2011) wird eher davon\n",
        "    abgeraten.\n",
        "\n",
        "**Rangtransformation:**\n",
        "\n",
        "- Im Prinzip das, was „nicht-parametrische\" Verfahren machen.\n",
        "\n",
        "- Grösster Informationsverlust von allen genannten Verfahren (noch\n",
        "    grösser wäre der Informationsverlust nur bei Überführung der\n",
        "    metrischen abhängigen Variablen in Kategorien oder gar in eine\n",
        "    Binärvariable).\n",
        "\n",
        "Die folgenden Abbildungen visualisieren exemplarisch die Effekte\n",
        "unterschiedlicher Transformationen auf die Werteverteilung (ganz links\n",
        "sind jeweils die untransformierten Daten, die Transformation rechts hat\n",
        "jeweils eine deutlich bessere Annäherung an die Normalverteilung\n",
        "erzielt).\n",
        "\n",
        "![](./myMediaFolder/media/image34.png){width=\"5.5625in\"\n",
        "height=\"2.5833333333333335in\"}![](./myMediaFolder/media/image35.png){width=\"4.868055555555555in\"\n",
        "height=\"2.729488188976378in\"}![](./myMediaFolder/media/image36.png){width=\"3.937007874015748in\"\n",
        "height=\"2.208363954505687in\"}\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Meist muss man nur die abhängige Variable transformieren. Es gibt aber\n",
        "Spezialfälle, wo man erst nach Transformation der abhängigen und der\n",
        "unabhängigen Variable eine adäquate Residuenverteilung erzielt. Dies ist\n",
        "insbesondere dann der Fall, wenn wir eine in Wirklichkeit nicht-lineare\n",
        "Beziehung mit einem linearen Modell abbilden. Wenn etwa im Falle einer\n",
        "einfachen linearen Regression (s. u.) in Wirklichkeit ein Potenzgesetz\n",
        "(*y* = *a x^b^*) vorliegt, erzielt man näherungsweise Varianzhomogenität\n",
        "und Normalverteilung der Residuen nur, wenn man a und b\n",
        "logarithmustransformiert.\n",
        "\n",
        "## Mehrfaktorielle ANOVA\n",
        "\n",
        "Bislang haben wir uns eine ANOVA mit nur einem Prädiktor, d. h. einer\n",
        "kategorialen Variablen mit zwei bis vielen Ausprägungen, angeschaut. Das\n",
        "Prinzip lässt sich aber auch auf zwei und mehr kategoriale Prädiktoren\n",
        "ausweiten. Man spricht dann von einer **mehrfaktoriellen ANOVA**. Im\n",
        "Optimalfall sollten alle Kombinationen Faktorlevels aller\n",
        "Prädiktorvariablen auftreten (dann spricht man von einem\n",
        "**vollfaktoriellen Design**), am besten sogar in gleicher/ähnlicher\n",
        "Häufigkeit.\n",
        "\n",
        "Betrachten wir exemplarisch die Situation mit zwei Prädiktoren\n",
        "(zweifaktorielle Varianzanalyse, *two-way ANOVA*). Hierzu haben wir in\n",
        "unserem Blumenbeispiel neben den drei Sorten noch ein weiteres\n",
        "„Treatment\" hinzugefügt, nämlich, ob die Pflanzen im Gewächshaus (house\n",
        "= yes) oder im Freiland (house = no) aufgezogen wurden. Der Boxplot in\n",
        "der explorativen Datenanalyse sieht wie folgt aus:\n",
        "\n",
        "![](./myMediaFolder/media/image37.png){width=\"2.831603237095363in\"\n",
        "height=\"2.989070428696413in\"}\n",
        "\n",
        "Wir haben nun zwei Möglichkeiten, die zweifaktorielle Varianzanalyse\n",
        "durchzuführen, **mit oder ohne Berücksichtigung von Interaktionen**:\n",
        "\n",
        "```{.r}\n",
        "summary(aov(size\\~cultivar+house))\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "cultivar 2 417.1 208.5 5.005 0.01 \\*\n",
        "\n",
        "\n",
        "house 1 992.3 992.3 23.815 9.19e-06 \\*\\*\\*\n",
        "\n",
        "\n",
        "Residuals 56 2333.2 41.7  \n",
        "\n",
        "\n",
        "summary(aov(size\\~cultivar\\*house))\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "cultivar 2 417.1 208.5 5.364 0.0075 \\*\\*\n",
        "\n",
        "\n",
        "house 1 992.3 992.3 25.520 5.33e-06 \\*\\*\\*\n",
        "\n",
        "\n",
        "cultivar:house 2 233.6 116.8 3.004 0.0579 .\n",
        "\n",
        "\n",
        "Residuals 54 2099.6 38.9\n",
        "```\n",
        "\n",
        "Ohne Interaktion (oben) verknüpfen wir die beiden Prädiktoren einfach\n",
        "mit „+\"; wenn wir die Interaktion auch analysieren wollen (unten), dann\n",
        "verwenden wir „\\*\" zur Verknüpfung. Ein Interaktion läge dann vor, wenn\n",
        "sich die Auswirkung von Gewächshaus vs. Freiland zwischen den Sorten\n",
        "unterschiede, etwa in einem Fall positiv, im anderen neutral oder\n",
        "negativ. Wir sehen, dass die untere ANOVA mit dem Interaktionsterm im\n",
        "Output eine dritte Zeile cultivar:house enhält, welcher die Signifikanz\n",
        "der Interaktion angibt (in unserem Fall also marginal signifikant).\n",
        "\n",
        "Liegt eine signifikante Interaktion vor, dann nimmt man zur\n",
        "Ergebnisdarstellung am besten eine Grafik, einen sogenannten\n",
        "Interaktionsplot, da sich die Interaktion schon bei zweifaktoriellen\n",
        "ANOVAs schwer in Worte fassen lässt und noch schwerer bei\n",
        "dreifaktoriellen ANOVAs mit potenziell einer Dreifachinteraktion und\n",
        "drei Zweifachinteraktionen:\n",
        "\n",
        "```{.r}\n",
        "interaction.plot(cultivar,house,size)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image38.png){width=\"3.1770833333333335in\"\n",
        "height=\"2.988860454943132in\"}\n",
        "\n",
        "Die Interaktion war nicht signifikant, was sich darin zeigt, dass die\n",
        "Linienzüge für yes und no einigermassen parallel sind, d. h. im\n",
        "Gewächshaus alle drei Kultivare grösser waren. Allerdings haben sich die\n",
        "drei Kultivare nicht völlig konsistent verhalten: der positive Einfluss\n",
        "von Gewächshaus war bei Sorte c viel grösser als bei den anderen beiden\n",
        "(was zu einem *p*-Wert der Interaktion nahe an der Signifikanzschwelle\n",
        "geführt hat).\n",
        "\n",
        "```{.r}\n",
        "#Visualisierung 2-fach-Interaktion etwas elaborierter mit ggplot\n",
        "\n",
        "\n",
        "library(sjPlot)\n",
        "\n",
        "\n",
        "library(ggplot2)\n",
        "\n",
        "\n",
        "theme_set(theme_classic())\n",
        "\n",
        "\n",
        "aov <- aov(size \\~ cultivar \\* house, data = blume3)\n",
        "```\n",
        "\n",
        "**plot_model(aov, type = \"pred\", terms = c(\"cultivar\", \"house\")\n",
        ")**\n",
        "\n",
        "![](./myMediaFolder/media/image39.png){width=\"3.441965223097113in\"\n",
        "height=\"2.7085684601924758in\"}\n",
        "\n",
        "Mit sjPlot kann man auch gut 3-fach-Interaktionen visualisieren, wie das\n",
        "folgende Beispiel zur Auswirkung von Managment und Hirschbeweidung\n",
        "(fenced = keine Hirsche) über zwei Versuchsjahre auf den\n",
        "Pflanzenartenreichtum zeigt:\n",
        "\n",
        "![](./myMediaFolder/media/image40.png){width=\"3.441965223097113in\"\n",
        "height=\"2.7085684601924758in\"}\n",
        "\n",
        "**aov.deer <- aov(Species.richness \\~ Year \\* Treatment \\* Plot.type,\n",
        "data = Riesch)**\n",
        "\n",
        "**plot_model(aov.deer, type = \"pred\", terms = c(\"Year\",\n",
        "\"Treatment\", \"Plot.type\"))**\n",
        "\n",
        "## Korrelationen\n",
        "\n",
        "```{.r}\n",
        "Pearson-Korrelationen** analysieren den \n",
        "```Zusammenhang zwischen zwei\n",
        "metrischen Variablen** und beantworten dabei die folgenden Fragen:\n",
        "\n",
        "- Gibt es einen **linearen** Zusammenhang?\n",
        "\n",
        "- In welche Richtung läuft er?\n",
        "\n",
        "- Wie stark ist er?\n",
        "\n",
        "Wichtig dabei ist, dass Korrelationen keine Kausalität voraussetzen oder\n",
        "annehmen. Es gibt also keine abhängige und unabhängige Variable, keine\n",
        "Unterscheidung in Prädiktor- und Antwortvariable. Logischerweise liefern\n",
        "Korrelationen dann auch identische Ergebnisse, wenn *x*- und *y-*Achse\n",
        "vertauscht werden.\n",
        "\n",
        "Die folgenden fünf Abbildungen zeigen verschiedene Situationen. Bei (a)\n",
        "liegt eine positive Korrelation vor, bei (b) eine negative und bei\n",
        "(c)--(e) keine Korrelation. Bei (e) erkennt man zwar visuell eine\n",
        "Beziehung (ein «Peak» in der Mittel, also eine unimodale Beziehung),\n",
        "aber das ist eben kein linearer Zusammenhang.\n",
        "\n",
        "![](./myMediaFolder/media/image41.png){width=\"5.277048337707787in\"\n",
        "height=\"3.1029833770778654in\"}\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Bei der Pearson-Korrelation betrachtet man die beiden Parameter\n",
        "Kovarianz (reicht von −∞ bis +∞) und die Korrelation, welche die\n",
        "Covarianz auf den Bereich von --1 bis +1 standardisiert. Pearsons\n",
        "Korrelationskoeffizient r ist der Schätzer für die Korrelation basierend\n",
        "auf der Stichprobe:\n",
        "\n",
        "![](./myMediaFolder/media/image42.png){width=\"4.46875in\"\n",
        "height=\"1.9023589238845144in\"}\\\n",
        "\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Die implizite Nullhypothese (H~0~) ist nun ρ = 0. Die Teststatistik ist\n",
        "das uns schon bekannte *t* mit $t = \\ \\frac{r}{s_{r}}$ , wobei *s~r~*\n",
        "für den Standardfehler von *r* steht und bei *n* -- 2 Freiheitsgraden\n",
        "gestet wird.\n",
        "\n",
        "Die Pearson-Korrelation ist die „parametrische\" Variante der\n",
        "Korrelationen. Ihre Anwendung hat zwei Voraussetzungen (in Klammern ist\n",
        "angegeben, wie man ihr Vorliegen visuell überprüfen kann):\n",
        "\n",
        "- Linearität (Überprüfung mit einem *xy*-Scatterplot)\n",
        "\n",
        "- Bivariate Normalverteilung (Überprüfung mit Boxplots beider\n",
        "    Variablen)\n",
        "\n",
        "Wenn diese Voraussetzungen ungenügend erfüllt sind, kann man auf\n",
        "nicht-parametrische Äquivalente ausweichen. Diese testen auf monotone,\n",
        "nicht auf lineare Beziehungen, liefern allerdings keine exakten\n",
        "Ergebnisse bei Bindungen (d.h. wenn der gleiche Wert mehrfach vorkommt):\n",
        "\n",
        "- Für 7 ≤ *n* ≤ 30: **Spearman-Rang-Korrrelation (*r~s~*)**\\\n",
        "    (im Prinzip Pearsons *r* für rangtransformierte Daten)\n",
        "\n",
        "- Für *n* \\> 30: **Kendall's tao (τ)**\n",
        "\n",
        "Hier noch der R Code für alle drei Möglichkeiten:\n",
        "\n",
        "**cor.test(df$Species.richness, df$N.deposition, method =\n",
        "\"pearson\")**\n",
        "\n",
        "**cor.test(df$Species.richness, df$N.deposition, method =\n",
        "\"spearman\")**\n",
        "\n",
        "**cor.test(df$Species.richness, df$N.deposition, method =\n",
        "\"kendall\")**\n",
        "\n",
        "##  Einfache lineare Regressionen\n",
        "\n",
        "### Idee\n",
        "\n",
        "Einfache lineare Regressionen sind konzeptionell und mathematisch\n",
        "ähnlich zu Pearson-Korrelationen. Oft werden beide Verfahren daher\n",
        "fälsch auch begrifflich durcheinandergeworfen. Der **entscheidende\n",
        "Unterschied** ist, dass wir für eine Regression eine **theoretisch\n",
        "vermutete Kausalität** haben müssen. Damit haben wir, anders als bei\n",
        "einer Korrelation, eine fundamentalte Unterscheidung in:\n",
        "\n",
        "- ***X*: unabhängige Variable** (*independent variable*),\n",
        "    Prädiktorvariable (*predictor*)\n",
        "\n",
        "- ***Y*: abhängige Variable** (*dependent variable*), Antwortvariable\n",
        "    (*response*)\n",
        "\n",
        "Bei Visualisierungen ist zu beachten, dass die unabhängige Variable\n",
        "immer auf der *x*-Achse dargestellt wird, die abhängige dagegen auf der\n",
        "nach oben gerichteten *y*-Achse.\n",
        "\n",
        "Mathematisch wird eine lineare Regression analysiert, indem die\n",
        "bestangepasste Gerade durch die Punktwolke des *xy*-Scatterplots gelegt\n",
        "wird. Dabei sieht das lineare Modell folgendermassen aus:\n",
        "\n",
        "- **Geradengleichung:** *y* = *b*~0~ + *b*~1~ *x*\n",
        "\n",
        "- **Statistisches Modell:** *y~i~* = β~0~ + β~1~ *x*~i~ + ε~i~, wobei\n",
        "    ε~i~ das Residuum des *i*-ten Daten­punktes ist, d. h. seine\n",
        "    vertikale Abweichung vom vorhergesagten Wert\n",
        "\n",
        "Mit einer einfachen linearen Regression testet man die folgenden beiden\n",
        "Nullhypothesen:\n",
        "\n",
        "- H~0~: **β~0~ = 0 (Achsenabschnitt \\[*intercept*\\] der\n",
        "    Grundgesamtheit ist Null)** (diese erste Nullhypothese ist, ähnlich\n",
        "    wie bei Varianzanalysen, in den meisten Fällen wissenschaftlich\n",
        "    nicht relevant)\n",
        "\n",
        "- H~0~: **β~1~ = 0 (Steigung \\[*slope*\\] der Grundgesamtheit ist\n",
        "    Null)**\n",
        "\n",
        "Die folgende Abbildung veranschaulicht die verschiedenen Möglichkeiten:\n",
        "\n",
        "![](./myMediaFolder/media/image43.png){width=\"6.299127296587926in\"\n",
        "height=\"2.0104166666666665in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "### Statistische Umsetzung\n",
        "\n",
        "Es mag vielleicht zunächst überraschen, aber ähnlich wie beim Vergleich\n",
        "von Mittelwerten zwischen kategorischen Ausprägungen kategorischer\n",
        "Variablen, liegt auch der linearen Regression eine **Varianzanalyse**\n",
        "zugrunde:\n",
        "\n",
        "![](./myMediaFolder/media/image44.png){width=\"6.498611111111111in\"\n",
        "height=\"2.4637510936132982in\"}![](./myMediaFolder/media/image45.png){width=\"3.2294531933508313in\"\n",
        "height=\"2.3463320209973753in\"}\\\n",
        "\\\n",
        "(aus Quinn & Keough 2002)\n",
        "\n",
        "Wiederum ist die Teststatistik ein *F*-ratio, nämlich *F* =\n",
        "MS~Regression~ / MS~Residual~, wobei MS für die mittleren Quadratsummen\n",
        "steht, also die Quadratsummen (SS) geteilt durch die Freiheitsgrade\n",
        "(df). Wie oben unter der Varianzanalyse schon erwähnt, folgt *F* einer\n",
        "*t*²-Verteilung.\n",
        "\n",
        "### Implementierung in R\n",
        "\n",
        "Das Kommando zum Berechnen einfacher linearer Regressionen lautet lm.\n",
        "Wie bei einem Mittelwertvergleich mittels Varianzanalyse gibt es dann\n",
        "zwei verschiedene Ansichten des Ergebnis-Outputs, die jeweils\n",
        "verschiedene Teilaspekte zeigen (Hier am Beispiel der Beziehung von\n",
        "Pflanzenartenreichtum zur Stickstoffdeposition):\n",
        "\n",
        "```{.r}\n",
        "lm <- lm(Species.richness\\~N.deposition, data = df)\n",
        "\n",
        "\n",
        "anova(lm) #ANOVA-Tabelle, 1. Möglichkeit\n",
        "\n",
        "\n",
        "summary.aov(lm) #ANOVA-Tabelle, 2. Möglichkeit\n",
        "\n",
        "\n",
        "Response: Species.richness\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "N.deposition 1 233.91 233.908 28.028 0.0001453 \\*\\*\\*\n",
        "\n",
        "\n",
        "Residuals 13 108.49 8.346\n",
        "```\n",
        "\n",
        "Die anova-Ansicht liefert uns die oben besprochene ANOVA-Tabelle,\n",
        "einschliesslich der Signifikanz der Steigung (hier *p* = 0.0001).\n",
        "Weitere erforderliche Aspekte des Ergebnisses sehen wir in der\n",
        "summary-Ansicht:\n",
        "\n",
        "```{.r}\n",
        "summary(lm) #Regressionskoeffizienten\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) 25.60502 1.26440 20.251 3.25e-11 \\*\\*\\*\n",
        "\n",
        "\n",
        "N.deposition -0.26323 0.04972 -5.294 0.000145 \\*\\*\\*\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "Residual standard error: 2.889 on 13 degrees of freedom\n",
        "\n",
        "\n",
        "Multiple R-squared: 0.6831, Adjusted R-squared: 0.6588\n",
        "\n",
        "\n",
        "F-statistic: 28.03 on 1 and 13 DF, p-value: 0.0001453\n",
        "```\n",
        "\n",
        "Wie wir sehen, tauchen wiederum der *F*-Wert (28.03) und sogar zweimal\n",
        "der *p*-Wert der Steigung (0.0001) auf, daneben auch der i. d. R.\n",
        "bedeutungslose *p*-Wert des Achsenabschnitts (*intercept*) (3.25 x\n",
        "10^-11^).\n",
        "\n",
        "Werfen wir noch einmal einen Blick auf den Output von R:\n",
        "\n",
        "```{.r}\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) 25.60502 1.26440 20.251 3.25e-11 \\*\\*\\*\n",
        "\n",
        "\n",
        "N.deposition -0.26323 0.04972 -5.294 0.000145 \\*\\*\\*\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "Residual standard error: 2.889 on 13 degrees of freedom\n",
        "\n",
        "\n",
        "Multiple R-squared: 0.6831, Adjusted R-squared: 0.6588\n",
        "\n",
        "\n",
        "F-statistic: 28.03 on 1 and 13 DF, p-value: 0.0001453\n",
        "```\n",
        "\n",
        "Wir benötigen\n",
        "\n",
        "> 1\\. **Name des Verfahrens (Methode)**: Einfache lineare Regression\n",
        "> (mit der Methode der kleinsten Quadrate).\n",
        ">\n",
        "> 2\\. **Signifikanz (Verlässlichkeit des Ergebnisses)**: *p*-Wert der\n",
        "> Steigung, nicht der *p*-Wert des Achsenabschnittes (wird nach üblicher\n",
        "> Konvention auf drei Nachkommastellen gerundet oder, wenn unter 0.001,\n",
        "> dann als *p* < 0.001 angegeben).\n",
        ">\n",
        "> 3\\. **Effektgrösse und -richtung (unser eigentliches Ergebnis!)**: Im\n",
        "> Falle einer linearen Regression ist das die Funktionsgleichung, die\n",
        "> sich aus den Schätzungen der Koeffizienten ergibt.\n",
        ">\n",
        "> 4\\. **Erklärte Varianz (Relevanz des Ergebnisses)**: Wie viel der\n",
        "> Gesamtvariabilität der Daten wird durch das Modell erklärt? Ob *R*²\n",
        "> oder *R*²~adj.~ angegeben werden sollte, wird unterschiedlich gesehen,\n",
        "> jedenfalls sollte man explizit sagen, was gemeint ist. *R*² ist\n",
        "> übrigens der quadrierte Wert von Pearsons Korrelationskoeffizienten\n",
        "> *r*.\n",
        ">\n",
        "> 5\\. **ggf. Wert der Teststatistik mit denFreiheitsgraden\n",
        "> („Zwischenergebnisse\")**: *F*~1,8~ = 11.34.\n",
        "\n",
        "Ein adäquater Ergebnistext könnte daher wie folgt lauten:\n",
        "\n",
        "Die Variable *b* nahm hochsignifikant mit der Variablen *a* zu\n",
        "(Funktionsgleichung: *b* = 5.02 + 0.42 \\**a*, *F*~1,8~ = 11.34, *p*\n",
        "=0.010, *R*² = 0.586).\n",
        "\n",
        "Bei einem signifkanten Ergebnis bietet sich auch noch eine\n",
        "Visualisierung mittels Scatterplot an, in den die Regressionsgerade\n",
        "geplottet ist:\n",
        "\n",
        "```{.r}\n",
        "plot(b\\~a,xlim=c(0,25),ylim=c(0,20))\n",
        "\n",
        "\n",
        "abline(lm(b\\~a))\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image46.png){width=\"3.5993372703412074in\"\n",
        "height=\"3.4270833333333335in\"}\n",
        "\n",
        "### Voraussetzungen\n",
        "\n",
        "Einfache lineare Regressionen basieren auf drei Vorausetzungen:\n",
        "\n",
        "> 1\\. **Linearität**\n",
        ">\n",
        "> 2\\. **Normalverteilung** (der Residuen!)\n",
        ">\n",
        "> 3\\. **Varianzhomogenität**\n",
        "\n",
        "Für das meistverwendete **Verfahren der kleinsten Abweichungsgquadrate**\n",
        "(wie bislang besprochen; ***ordinary least squares* = OLS**), auch als\n",
        "**Modell I-Regressionen** bezeichnet, muss zudem gelten:\n",
        "\n",
        "> 4\\. **Feste *x*-Werte**, d. h.\\\n",
        "> - *x*-Werte vom Experimentator gesetzt ODER\\\n",
        "> - Fehler in den *x*-Werten viel kleiner als in den *y*-Werten\\\n",
        "> \\\n",
        "> **Sowie auch für folgende Fälle:\\\n",
        "> **- Hypothesentest H~0~: β~1~ = 0 im Fokus, nicht der exakte Wert von\n",
        "> β~1\\\n",
        "> ~- Für prädiktive Modelle\\\n",
        "> - Wenn keine bivariate Normalverteilung vorliegt\n",
        "\n",
        "### Alternativen zur Methode der kleinsten Quadrate (OLS)\n",
        "\n",
        "Wenn keine der oben unter Punkt 4 genannten Voraussetzungen erfüllt ist,\n",
        "dann sollte eine sogenannte **Modell-II-Regression\n",
        "(Nicht-OLS-Regression)** durchgeführt werden. Hier stehen als\n",
        "Möglichkeiten die *Major axis regression*, die *Ranged major axis\n",
        "regression* und die *Reduced major axis regression* zur Verfügung.\n",
        "Details finden sich in Logan (2010: 173--175), woraus aus die folgende\n",
        "Visualisierung stammt:\n",
        "\n",
        "![](./myMediaFolder/media/image47.jpeg){width=\"6.433109142607174in\"\n",
        "height=\"5.0625in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "In R stehen solche Methoden u. a. im Paket lmodel2 zur Verfügung:\n",
        "\n",
        "```{.r}\n",
        "library(lmodel2)\n",
        "\n",
        "\n",
        "lmodel2(b\\~a)\n",
        "\n",
        "\n",
        "Regression results\n",
        "\n",
        "\n",
        "Method Intercept Slope Angle (degrees) P-perm (1-tailed)\n",
        "\n",
        "\n",
        "1 OLS 5.019254 0.4170422 22.63820 NA\n",
        "\n",
        "\n",
        "2 MA 4.288499 0.4648040 24.92919 NA\n",
        "\n",
        "\n",
        "3 SMA 3.067471 0.5446097 28.57314 NA\n",
        "```\n",
        "\n",
        "Wie man sieht, unterscheiden sich die beiden Modell-II-Ergebnisse\n",
        "deutlich von Modell I (OLS).\n",
        "\n",
        "## Lineare Modelle allgemein\n",
        "\n",
        "### Was macht ein lineares Modell aus?\n",
        "\n",
        "Die meisten statistischen Verfahren, die wir bis zu diesem Punkt\n",
        "angeschaut haben, gehören zu den **linearen Modellen**. Dieser Begriff\n",
        "wird häufig weitgehend synonym mit „parametrischen Verfahren\" verwendet,\n",
        "ist aber treffender. Von den bisherigen Verfahren gehören die folgenden\n",
        "zu den linearen Modellen:\n",
        "\n",
        "- Pearson-Korrelation\n",
        "\n",
        "- *t*-Test\n",
        "\n",
        "- Varianzanalyse\n",
        "\n",
        "- Einfache lineare Regression\n",
        "\n",
        "Was macht nun lineare Modelle aus:\n",
        "\n",
        "- Voraussetzungen: **Normalverteilung der Residuen und\n",
        "    Varianzhomogenität**\n",
        "\n",
        "- In R kann man sie (mit Ausnahme der Pearson-Korrelation) mit dem\n",
        "    **Befehl lm** abbilden (ja, auch die Varianzanalyse!)\n",
        "\n",
        "- Varianzanalysen und lineare Regressionen nutzen beide\n",
        "    **ANOVA-Tabellen mit *F*-ratios** als Testverfahren\n",
        "\n",
        "- Lineare Modelle lassen sich als **Linearkombination der\n",
        "    Prädiktoren** schreiben, d. h.:\\\n",
        "    - Prädiktoren werden [nicht]{.underline} als Multiplikator, Divisor\n",
        "    oder Exponent anderer\\\n",
        "    Prädiktoren verwendet\\\n",
        "    - die Beziehung muss aber [nicht zwingend linear]{.underline} sein.\n",
        "\n",
        "### Welche Verfahren gehören zu den linearen Modellen?\n",
        "\n",
        "Neben den schon besprochenen einfachen Verfahren gehören auch eine ganze\n",
        "Reihe komplexerer Vefahren zu den linearen Modellen, die aber alle den\n",
        "vorstehenden Bedingungen entsprechen. Die meisten werden wir in\n",
        "Statistik 3 besprechen. Logan (2010: 165) hat eine recht umfassende\n",
        "folgende Übersicht erstellt. Darin sind metrische Prädiktoren als x, x1\n",
        "und x2 bezeichnet, kategoriale als A bzw. B. Was unter *R Model formula*\n",
        "steht, würde im jeweiligen Fall in die Klammern des lm-Befehls gesetzt:\n",
        "\n",
        "![](./myMediaFolder/media/image48.jpeg){width=\"5.288186789151356in\"\n",
        "height=\"7.665556649168854in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "### Testen der Voraussetzungen von linearen Modellen (Modelldiagnostik)\n",
        "\n",
        "Wie geschrieben, haben lineare Modelle bestimmte Voraussetzungen. Selbst\n",
        "wenn lineare Modelle recht robust gegen Verletzungen der Vorassetzungen\n",
        "sind, so muss man doch jedes Mal, nachdem man ein lineares Modell\n",
        "gerechnet hat, prüfen, ob die Voraussetzungen erfüllt waren. Es geht\n",
        "hier primär um die Voraussetzungen Varianzhomogenität, Normalverteilung\n",
        "der Residuen und Linearität.\n",
        "\n",
        "Wichtig ist, zu verstehen, dass man zunächst das lineare Modell rechnen\n",
        "muss und erst nachträglich prüfen kann, ob die Voraussetzungen erfüllt\n",
        "waren. Das liegt daran, dass die Kernannahmen Varianzhomogenität und\n",
        "Normalverteilung der Residuen sich auf das Modell, nicht auf die\n",
        "Originaldaten beziehen. Einzig für *t-*Tests und ANOVAs kann man diese\n",
        "beiden Punkte auch in der explorativen Datenanalyse vor dem Berechnen\n",
        "des Modells erkunden, für lineare Regressionen und komplexere Modelle\n",
        "geht das nicht. Wenn der nachträgliche Test zeigt, dass eine der\n",
        "Voraussetzungen schwerwiegend verletzt war, bedeutet das, dass man das\n",
        "Modell neu spezifizieren muss, etwa durch eine geeignete Transformation\n",
        "der abhängigen Variablen.\n",
        "\n",
        "Das **Überprüfen der Voraussetzungen (= Modelldiagnostik)** erfolgt\n",
        "visuell mittels der sogenannten Residualplots, die man mit dem\n",
        "generische plot-Befehl bekommt, wenn man als Argument das Ergebnis eines\n",
        "linearen Modells hat. Man bekommt dann vier Plots, die man am besten in\n",
        "einem 2 x 2-Arrangement ausgibt (das macht der erste Befehl):\n",
        "\n",
        "```{.r}\n",
        "par(mfrow = c(2, 2)) #4 Plots in einem Fenster\n",
        "\n",
        "\n",
        "plot(lm)\n",
        "```\n",
        "\n",
        "Betrachten wir zwei Fälle, zunächst das Beispiel von eben:\n",
        "\n",
        "![](./myMediaFolder/media/image49.png){width=\"2.7674464129483813in\"\n",
        "height=\"3.0488812335958007in\"}\n",
        "\n",
        "und die zugehörigen Residualplots:\n",
        "\n",
        "![](./myMediaFolder/media/image50.png){width=\"4.362068022747157in\"\n",
        "height=\"4.805667104111986in\"}\n",
        "\n",
        "In diesem Fall ist **alles OK**. Man muss vor allem die oberen beiden\n",
        "Teilabbildungen betrachten. Links oben kann man gut erkennen, wenn\n",
        "Linearität oder Varianzhomogenität verletzt wären, rechts oben dagegen,\n",
        "wenn die Normalverteilung der Residuen verletzt wäre. Zu berücksichtigen\n",
        "ist, dass reale Daten nie perfekt linear, varianzhomogen und\n",
        "normalverteilt sind.\n",
        "\n",
        "Uns interessieren nur **massive Abweichungen**. Wir würden sie wie folgt\n",
        "erkennen:\n",
        "\n",
        "- **Linearität:** Eine Verletzung erkennen wir in der linken oberen\n",
        "    Abbildung, wenn wir eine **„Wurst\" bzw. „Banane\"** sehen, also wenn\n",
        "    die linken Punkte alle unter der gepunktelten Linie, die mittleren\n",
        "    alle darüber und die rechten wieder alle darunter lägen (oder\n",
        "    umgekehrt).\n",
        "\n",
        "- **Varianzhomogenität:** Eine Verletzung erkennen wir in der linken\n",
        "    oberen Abbildung, wenn die Punktwolke einen starken **Keil** (meist\n",
        "    nach rechts offen) beschreibt.\n",
        "\n",
        "- **Normalverteilung der Residuen:** Eine Verletzung erkennen wir in\n",
        "    der rechten oberen Abbildung, wenn die Punkte sehr stark von der\n",
        "    gestrichelten Linie abweichen, insbesondere wenn sie eine\n",
        "    ausgeprägte **Treppenkurve** bilden.\n",
        "\n",
        "Die beiden unteren Abbildungen sind für die Diagnostik weniger wichtig.\n",
        "Links unten haben wir eine skalierte Version der Abbildung links oben.\n",
        "Die Abbildung rechts unten zeigt uns, ob bestimmte Datenpunkte\n",
        "übermässigen Einfluss auf das Gesamtergebnis haben. Das wären Punkte mit\n",
        "einer *Cook's distance* über 0.5 und insbesondere über 1. In solchen\n",
        "Fällen sollten wir noch einmal kritisch prüfen, ob (a) evtl. ein\n",
        "Eingabefehler vorliegt und (b) der bezeichnete Punkt wirklich zur\n",
        "Grundgesamtheit gerechnet werden sollte. Wenn aber beide Aspekte nicht\n",
        "zu beanstanden sind, dann gibt es auch keinen Grund, den entsprechenden\n",
        "Datenpunkt auszuschliessen; wir müssen uns nur bewusst sein, dass er das\n",
        "Gesamtergebniss übermässig stark beeinflusst.\n",
        "\n",
        "Zum Schluss kommt noch ein Beispiel, bei dem die Modellvoraussetzungen\n",
        "einer linearen Regression klar nicht erfüllt sind.\n",
        "\n",
        "![](./myMediaFolder/media/image51.png){width=\"2.5722769028871393in\"\n",
        "height=\"2.5104166666666665in\"}\n",
        "\n",
        "![](./myMediaFolder/media/image52.emf.png){width=\"4.968503937007874in\"\n",
        "height=\"4.604400699912511in\"}\n",
        "\n",
        "Hier sind die **Voraussetzungen klar nicht erfüllt**: (a) es liegt\n",
        "starke **Varianzinhomogenität** vor (links oben als nach rechts offener\n",
        "Keil erkennbar, links unten als klar ansteigende Kurve); (b) die\n",
        "**Normalverteilung der Residuen ist auch nicht gegeben** (im Q-Q-Plot\n",
        "rechts oben weichen die Punkte stark von der theoretischen Kurve ab und\n",
        "bilden stattdessen eine Treppenkurve). Schliesslich sehen wir rechte\n",
        "unten auch noch, dass es einen extrem **einflussreichen Datenpunkt** mit\n",
        "*Cook's distance* \\> 1 und einen weiteren mit *Cook's distance* \\> 0.5\n",
        "gibt.\n",
        "\n",
        "In diesem Fall schlussfolgern wir, dass das **Modell fehlspezifiziert**\n",
        "war. Da die Varianz mit dem Mittelwert zunimmt, während zugleich keine\n",
        "Null-Werte unter der abhängigen Variablen auftreten, wäre eine\n",
        "Logarithmus-Transformation der abhängigen Variablen hier vermutlich ein\n",
        "zielführendes Vorgehen. Dieses sollten wir ausprobieren und\n",
        "anschliessend wiederum die Residualplots betrachten.\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- ***t*-Tests und ANOVAs** sind parametrische Verfahren, um auf\n",
        "    **Unterschiede in den Mittelwerten einer metrischen Variablen**\n",
        "    zwischen zwei bzw. beliebig vielen Gruppen zu testen.\n",
        "\n",
        "- **Korrelationen** testen auf einen linearen Zusammenhang zwischen\n",
        "    zwei metrischen Variablen, **ohne Kausalität anzunehmen.**\n",
        "\n",
        "- Einfache **lineare Regressionen** machen das Gleiche unter Annahme\n",
        "    eines **gerichteten Zusammenhangs** (d. h. wenn es eine unabhängige\n",
        "    und eine abhängige Variable gibt).\n",
        "\n",
        "- **Parametrische Verfahren** basieren auf **bestimmten Annahmen** zur\n",
        "    Streuung der Daten, sind aber **robust** gegenüber deren Verletzung.\n",
        "\n",
        "- Die **Voraussetzungen parametrischer Verfahren** beziehen sich auf\n",
        "    die **Residuen**, nicht auf die unabhängigen, noch auf die\n",
        "    abhängigen Variablen *per se*.\n",
        "\n",
        "- Sowohl lineare Regressionen als auch ANOVAs gehören zu den\n",
        "    **linearen Modellen** und können in R mit dem **Befehl lm**\n",
        "    spezifiziert werden.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed.\n",
        "John Wiley & Sons, Chichester, UK: 339 pp.**\n",
        "\n",
        "```{.r}\n",
        "- Chapter 7 -- Regression: pp. 114--139\n",
        "\n",
        "\n",
        "- Chapter 8 -- Analysis of Variance: pp. 150--167\n",
        "```\n",
        "\n",
        "Fox, J. & Weisberg, S. 2019. *An R companion to applied regression*. 3rd\n",
        "ed. SAGE Publications, Thousand Oaks, CA, US: 577 pp.\n",
        "\n",
        "Logan, M. 2010. *Biostatistical design and analysis using R. A practical\n",
        "guide*. Wiley-Blackwell, Oxford, UK: 546 pp.\n",
        "\n",
        "\\- pp. 151-166 (lineare Modelle)\n",
        "\n",
        "\\- pp. 167-207 (Korrelation und einfache lineare Regression)\n",
        "\n",
        "\\- pp. 254-282 (Einfaktorielle ANOVA)\n",
        "\n",
        "\\- pp. 311-359 (Mehrfaktorielle ANOVA)\n",
        "\n",
        "Quinn, G.P. & Keough, M.J. 2002. *Experimental design and data analysis\n",
        "for biologists*. Cambridge University Press, Cambridge, UK: 537 pp.\n",
        "\n",
        "Warton, D.I. & Hui, F.K.C. 2011. The arcsine is asinine: the analysis of\n",
        "proportions in ecology. *Ecology* 92: 3--10.\n",
        "\n",
        "Wilson, J.B. 2007. Priorities in statistics, the sensitive feet of\n",
        "elephants, and don't transform data. *Folia Geobotanica* 42: 161--167.\n",
        "\n",
        "# Statistik 3: Lineare Modelle II\n",
        "\n",
        "**Statistik 3 fassen wir zu Beginn den generellen Ablauf\n",
        "inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann\n",
        "wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer\n",
        "linearen Regression verbindet. Danach geht es um komplexere Versionen\n",
        "linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die\n",
        "z. B. einen Test auf unimodale Beziehungen erlauben, indem man dieselbe\n",
        "Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen\n",
        "versuchen dagegen, eine abhängige Variable durch zwei oder mehr\n",
        "verschieden Prädiktorvariablen zu erklären. Wir thematisieren\n",
        "verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere\n",
        "den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten\n",
        "unter mehreren möglichen statistischen Modellen. Hieran wird auch der\n",
        "*informatian theoretician*-Ansatz der Statistik und die *multimodel\n",
        "inference* eingeführt.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *wisst, wofür **ANCOVA** steht, wann dieses statistische Verfahren\n",
        "    zum Einsatz kommt und wie das praktisch geht.*\n",
        "\n",
        "- *versteht, wann es Sinn macht, **quadratische Terme in eine\n",
        "    Regression** einfliessen zu lassen und warum das dann trotzdem noch\n",
        "    ein lineares Modell ist;*\n",
        "\n",
        "- *könnt **lineare Regressionen mit mehreren Prädiktoren** in R\n",
        "    implementieren und wisst, welche Aspekte ihr bei der\n",
        "    Modellspezifikation und bei der Auswahl des «besten» Modells\n",
        "    beachten müsst; und*\n",
        "\n",
        "- *kennt die Gütemasse des **information theoretician approach** und\n",
        "    könnt sie interpretieren.*\n",
        "\n",
        "## Genereller Ablauf einer statistischen Analyse\n",
        "\n",
        "Das folgende Schema zeigt den generellen Ablauf einer statistischen\n",
        "Analyse, wie er für alle schon besprochenen und auch alle noch kommenden\n",
        "Verfahren gilt:\n",
        "\n",
        "![](./myMediaFolder/media/image53.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"4.083767497812773in\"}\n",
        "\n",
        "Ein zentrales Element ist die Modelldiagnostik, die wir in Statistik 2\n",
        "am Ende behandelt haben. Leider wird sie oft vergessen! Basierend auf\n",
        "den Ergebnissen der Modelldiagnostik kann man entweder die Ergebnisse\n",
        "fertigstellen oder aber man muss zu den initialen Schritten zurückgehen.\n",
        "Möglicherweise war das gewählte statistische Verfahren schon nicht\n",
        "adäquat oder das Verfahren war in Ordnung, nur die Details der\n",
        "Spezifizierung (etwa Transformationen von Daten) müssen nachgebessert\n",
        "werden.\n",
        "\n",
        "## Covarianzanalyse (ANCOVA)\n",
        "\n",
        "Wie wir schon bei „Lineare Modelle allgemein\" in Statitik 2 gesehen\n",
        "haben, lassen sich metrische und kategoriale Variablen in einem einzigen\n",
        "linearen Modell kombinieren. Eine ANCOVA macht genau dieses, ist also im\n",
        "Prinzip eine Kombination aus ANOVA und linearer Regression. Stellen wir\n",
        "uns vor, wir hätten einen Datensatz von Körpergewichten von Kindern\n",
        "unterschiedlichen Alters (age: metrisch) und Geschlechts (sex:\n",
        "kategorial/binär, dargestellt als blau und rot). Eine ANCOVA testet nun,\n",
        "ob und wie sich das Gewicht in Abhängigkeit von beiden Faktoren verhält.\n",
        "Dabei gibt es im Prinzip sechs verschiedene Möglichkeiten/Ergebnisse:\n",
        "\n",
        "![](./myMediaFolder/media/image54.jpeg){width=\"3.953333333333333in\"\n",
        "height=\"3.1766666666666667in\"}\\\n",
        "(aus Crawley 2015)\n",
        "\n",
        "Wie andere lineare Modelle auch, kann man eine ANCOVA mittels aov oder\n",
        "mittels lm spezifizieren. Es ist zu beachten, dass hier die Reihenfolge\n",
        "der Variablen wichtig ist:\n",
        "\n",
        "```{.r}\n",
        "summary(aov(weight\\~age\\*sex))\n",
        "```\n",
        "\n",
        "Im vollen Modell (*full model,* *global model*) wurden vier Parameter\n",
        "gefittet (2 Steigungen und 2 Achsenabschnitte). Das haben wir durch das\n",
        "„\\*\"-Zeichen spezifiziert. Dieses sagt, dass nicht nur Alter und\n",
        "Geschlecht unabhängig voneinander einen (additiven) Effekt haben,\n",
        "sondern dass der Effekt des Alters je nach Geschlecht unterschiedlich\n",
        "sein könnte, also die Gewichtszunahme mit. Jedoch sind oft nicht alle\n",
        "bedeutsam. Es ist daher wichtig, das Modell so lange zu vereinfachen,\n",
        "bis nur noch bedeutsame Parameter übrig sind. Dann hat man das minimal\n",
        "adäquate Modell.\n",
        "\n",
        "Für die **Modellvereinfachung** gibt es unterschiedliche Strategien\n",
        "(mehr dazu später bei den „Multiplen linearen Regressionen\"). Man muss\n",
        "jedenfalls schrittweise vorgehen, d. h. immer nur einen Parameter\n",
        "löschen und dann das neue Modell anschauen. Wenn von den Parametern\n",
        "welche nicht signifikant sind, könnte man z. B. zunächst den am\n",
        "wenigsten signifikanten löschen und dann das neue Model betrachten, usw.\n",
        "\n",
        "Alternativ kann man auch ANOVAs zum Vergleich zweier unterschiedlich\n",
        "komplexer Modelle verwenden. Das klingt zunächst schräg, da wir bislang\n",
        "ANOVAs verwendet haben, um innerhalb eines Modelles zu sehen, ob etwa\n",
        "die durch die Steigung erklärte Varianz signifikant ist. Den gleichen\n",
        "Ansatz kann man aber auch verwenden, um zwei unterschiedlich komplexe\n",
        "Modelle miteinander zu vergleichen. Wichtig ist nur, dass das eine\n",
        "Modell im anderen geschachtelt ist:\n",
        "\n",
        "```{.r}\n",
        "anova(lm(weight\\~age\\*sex), lm(weight\\~age+sex))\n",
        "```\n",
        "\n",
        "Das komplexere Modell ist jenes mit „\\*\", das einfachere jenes mit „+\",\n",
        "da dort eine einheitliche Gewichtszunahme mit dem Alter angeommen wird.\n",
        "Wenn die ANOVA nun ein signifikantes Ergebnis liefert, heisst das, dass\n",
        "der zusätzliche Parameter des komplexeren Modells (die Interaktion Alter\n",
        "x Geschlecht) mehr erklärt als zufällig zu erwarten und daher\n",
        "beibehalten werden sollte. Wenn die ANOVA ein nicht-signifkantes\n",
        "Ergebnis liefert, sollten wir uns für das einfachere Modell (jenes mit\n",
        "„+\") entscheiden.\n",
        "\n",
        "## Polynomische Regressionen\n",
        "\n",
        "Eine quadratische Regression (Polynom 2. Ordnung) ist die einfachste\n",
        "Möglichkeit, eine sogenannte unimodale (*humpshaped*) Beziehung von\n",
        "abhängiger zur unabhängigen Variablen mathematisch abzubilden.\n",
        "Unimodal/*humpshaped* meint, dass die Kurve ein Maximum hat, d. h. die\n",
        "abhängige Variable für mittlere Werte der Prädiktorvariablen den\n",
        "höchsten Wert aufweist. Für viele Beziehungen sind solche unimodalen\n",
        "Kurvenverläufe theoretische vorhergesagt und/oder theoretisch\n",
        "nachgewiesen. In der Ökologie gilt das z. B. für die Beziehung des\n",
        "Artenreichtums zu so unterschiedlichen Faktoren wie Störungshäufigkeit\n",
        "(*intermediate disturbance hypothesis*, IDH), Boden-pH-Wert und\n",
        "Produktivität/Biomasse.\n",
        "\n",
        "Das statistische Modell für eine quadratische Beziehung ist:\n",
        "\n",
        "*y*i = β0 + β1*x*i + β2*x*i²\n",
        "\n",
        "In R wird eine quadratische Regression folgendermassen codiert:\n",
        "\n",
        "```{.r}\n",
        "summary(lm(f\\~e+I(e\\^2)))\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) -2.239308 3.811746 -0.587 0.56777\n",
        "\n",
        "\n",
        "e 1.330933 0.360105 3.696 0.00306 \\*\\*\n",
        "\n",
        "\n",
        "I(e\\^2) -0.031587 0.007504 -4.209 0.00121 \\*\\*\n",
        "```\n",
        "\n",
        "Wichtig ist, dass man den quadratischen Term im lm-Befehl nicht einfach\n",
        "als e\\^2 eingeben kann, sondern I(e\\^2) schreiben muss. Eine\n",
        "signifikante unimodale Beziehung ist dann gegeben, wenn die\n",
        "Parameterschätzung für den Quadratischen Term (also e\\^2) negativ ist --\n",
        "man hat eine nach unten offene Parabel. Ist der quadratische Term\n",
        "dagegen signifikant positiv, hat man eine nach oben offene Parabel, also\n",
        "eine u-förmige Beziehung (Minimum für die abhängige Variable bei\n",
        "intermediären Werten der Prädiktorvariablen).\n",
        "\n",
        "Wichtig ist, dass man wie bei allen statistischen Modellen nachträglich\n",
        "die Modellvoraussetzungen prüft.\n",
        "\n",
        "Im vorhergehenden Beispiel sah es mit einer einfachen linearen\n",
        "Regression so aus (Code, Ergebnisplot und Residualplots):\n",
        "\n",
        "```{.r}\n",
        "plot(f\\~e,xlim=c(0,40),ylim=c(0,20))\n",
        "\n",
        "\n",
        "abline(lm(f\\~e),col=\"blue\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image55.png){width=\"4.724409448818897in\"\n",
        "height=\"2.6384569116360455in\"}\n",
        "\n",
        "![](./myMediaFolder/media/image56.png){width=\"6.299212598425197in\"\n",
        "height=\"4.31496062992126in\"}\n",
        "\n",
        "Man ahnt schon im Scatterplot mit der gefitteten einfachen linearen\n",
        "Regression, dass etwas mit dem Modell nicht stimmt, was durch die\n",
        "Bananenform im Residualplot links oben unterstrichen wird: die Beziehung\n",
        "ist evident nicht linear.\n",
        "\n",
        "Nach Hinzufügen des quadratischen Terms sieht man schon im Scatterplot\n",
        "mit der gefitteten Funktion, dass es viel besser passt, aber erst recht\n",
        "in den Residualplots. Mit predict kann man jede Funktion plotten, die\n",
        "als Ergebnis einer Regressionsanalyse herauskommt. Im Prinzip zerlegt\n",
        "man die *x*-Achse in viele kleine Segmente und plottet dann jeweils\n",
        "Geraden zwischen zwei aufeinander folgenden vorhergesagten Punkten.\n",
        "\n",
        "```{.r}\n",
        "xv <- seq(0,40,0.1)\n",
        "\n",
        "\n",
        "plot(f\\~e,xlim=c(0,40),ylim=c(0,20))\n",
        "\n",
        "\n",
        "yv2 <- predict(lm.quad,list(e=xv))\n",
        "\n",
        "\n",
        "lines(xv,yv2,col=\"red\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image57.png){width=\"4.724409448818897in\"\n",
        "height=\"2.6384569116360455in\"}\n",
        "\n",
        "![](./myMediaFolder/media/image58.png){width=\"6.299212598425197in\"\n",
        "height=\"4.31496062992126in\"}\n",
        "\n",
        "Bezüglich des statistische Vorgehens ist zu beachten, dass man den\n",
        "quadratischen Term nur im Modell behalten sollte, wenn er signifikant\n",
        "ist (bei nur einem quadratischen Term der p-Wert aus summary, sonst ggf.\n",
        "mit anova testen oder AICc-Werte (siehe später) vergleichen). Dagegen\n",
        "muss der lineare Term (hier: e) dann beibehalten werden, wenn der\n",
        "quadratische Term signifikant ist, selbst wenn der lineare Term nicht\n",
        "signifkant ist. (Wenn beide nicht signifikant sind, fallen dagegen beide\n",
        "raus).\n",
        "\n",
        "Wenn es theoretische Gründe gibt, kann man in gleicher Weise auch\n",
        "Polynome höherer Ordnung implementieren. Wichtig ist, im Hinterkopf zu\n",
        "behalten, dass eine polynomische Regression fast immer eine deutliche\n",
        "Simplifizierung der Realität darstellt. Sie ist ein probates und\n",
        "einfaches Mittel, um zu testen, ob die Beziehung signifikant unimodal\n",
        "ist. Dagegen ist sie problematisch als prädiktives Modell, da sie oft\n",
        "negative Werte für die abhängige Variable voraussagt, zumindest\n",
        "ausserhalb des gefitteten Bereichs. Negative Werte sind aber vielfach\n",
        "theoretisch unmöglich (z. B. Artenzahlen, Stoffkonzentrationen,...).\n",
        "\n",
        "## Multiple lineare Regressionen\n",
        "\n",
        "### Vorgehen \n",
        "\n",
        "Analog zur mehrfaktoriellen ANOVA, sind multiple lineare Regressionen\n",
        "einfach lineare Regressionen mit mehreren Prädiktoren. Das statistische\n",
        "Modell lautet also folgendermassen (wobei *x*~1~ ... *x*~i~ metrische\n",
        "Variablen sind):\n",
        "\n",
        "> *y*i = β0 + β1*x*1,i + β2*x*2,i + (...) + βj*x*j,i\n",
        "\n",
        "In R wird das wie folgt codiert:\n",
        "\n",
        "```{.r}\n",
        "model1 <- lm (y \\~ x1 + x2 + x3, data = mydata)\n",
        "```\n",
        "\n",
        "Möglich sind aber auch folgende Modelle:\n",
        "\n",
        "```{.r}\n",
        "model2 <- lm (y \\~ x1 + x2 + I(x2\\^2), data = mydata)\n",
        "\n",
        "\n",
        "model3 <- lm (y \\~ x1 + x2 + log10(x3), data = mydata)\n",
        "\n",
        "\n",
        "model4 <- lm (y \\~ x1 + x2 + x1:x2, data = mydata)\n",
        "```\n",
        "\n",
        "Und für ein konkretes Beispiel (Abhängigkeit der Vogelabundanz in\n",
        "isolierten Waldinseln von verschiedenen Umweltvariablen (YR.ISOL = year\n",
        "since isolation, ALT = altitude, GRAZE = grazing):\n",
        "\n",
        "```{.r}\n",
        "model <- lm (ABUND \\~ YR.ISOL + ALT + GRAZE, data=loyn)\n",
        "\n",
        "\n",
        "summary(model)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) -73.58185 107.24995 -0.686 0.495712\n",
        "\n",
        "\n",
        "YR.ISOL 0.05143 0.05393 0.954 0.344719\n",
        "\n",
        "\n",
        "ALT 0.03285 0.02679 1.226 0.225618\n",
        "\n",
        "\n",
        "GRAZE -4.01692 0.99881 -4.022 0.000188 \\*\\*\\*\n",
        "```\n",
        "\n",
        "Und wie immer schauen wir die Residualplots an, die eigentlich ziemlich\n",
        "gut aussehen:\n",
        "\n",
        "```{.r}\n",
        "par(mfrow=c(2,2))\n",
        "\n",
        "\n",
        "plot(model)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image59.png){width=\"4.916666666666667in\"\n",
        "height=\"5.0625in\"}\n",
        "\n",
        "Allerdings dürfen wir uns hier im Falle einer multiplen Regression noch\n",
        "nicht zufrieden zurücklehnen, sondern müssen uns zunächst noch zwei\n",
        "potenziellen Problemen annehmen: (1) Korrelation zwischen den\n",
        "Prädiktoren und (2) Overfitting.\n",
        "\n",
        "### Problem 1: Korrelation zwischen den Prädiktoren\n",
        "\n",
        "Damit lm verlässliche Parameterschätzungen liefern kann, müssen die\n",
        "Prädiktoren (hinreichend) **unabhängig** (man spricht auch von:\n",
        "orthogonal) sein. Das muss man vor dem Fitten des Models testen und dann\n",
        "von Paaren hochkorrelierter Variablen jeweils eine ausschliessen.\n",
        "\n",
        "Es gibt zwei gängige Testmöglichkeiten:\n",
        "\n",
        "\\(1\\) **Korrelationmatrix:** nur Parameter mit \\|r\\| < 0.7 werden\n",
        "beibehalten (manchmal findet man auch andere Schwellenwerte, etwa 0.6\n",
        "oder 0.75: wie eigentlich alles in der Statistik, ist es keine\n",
        "Schwarz-weiss-Welt).\n",
        "\n",
        "\\(2\\) ***Variance inflation factor* (VIF):**\n",
        "\n",
        "> ${VIF}_{i}\\  = \\frac{1}{1 - R_{i}²}$ , mit *R~i~*² aus dem Modell\n",
        "> Prädiktor *i* gegen alle übrigen Prädiktoren\n",
        "\n",
        "Der VIF sagt uns, dass der Standardfehler (SE) des Prädiktors um\n",
        "$\\sqrt{VIF}$ grösser ist als im orthogonalen Fall. Meist werden\n",
        "Variablen bis VIF = 5, manchmal bis VIF = 10 akzeptiert.\n",
        "\n",
        "Die Berechnung der Korrelationsmatrix geht in R sehr einfach:\n",
        "\n",
        "```{.r}\n",
        "cor <- cor(loyn\\[,2:7\\])\n",
        "\n",
        "\n",
        "cor\n",
        "```\n",
        "\n",
        "Das Ergebnis ist allerdings unübersichtlich. Man kann es vereinfachen,\n",
        "indem man nur jene Werte darstellt, die über dem selbstgewählten\n",
        "Schwellenwert (hier 0.6) liegen.\n",
        "\n",
        "```{.r}\n",
        "cor\\[abs(cor)<0.6\\] <- 0\n",
        "\n",
        "\n",
        "cor\n",
        "\n",
        "\n",
        "AREA YR.ISOL DIST LDIST GRAZE ALT\n",
        "\n",
        "\n",
        "AREA 1 0.0000000 0 0 0.0000000 0\n",
        "\n",
        "\n",
        "YR.ISOL 0 1.0000000 0 0 -0.6355671 0\n",
        "\n",
        "\n",
        "DIST 0 0.0000000 1 0 0.0000000 0\n",
        "\n",
        "\n",
        "LDIST 0 0.0000000 0 1 0.0000000 0\n",
        "\n",
        "\n",
        "GRAZE 0 -0.6355671 0 0 1.0000000 0\n",
        "\n",
        "\n",
        "ALT 0 0.0000000 0 0 0.0000000 1\n",
        "```\n",
        "\n",
        "Wenn man die Schwelle bei 0.6 ansetzt, müsste man also von den beiden\n",
        "Variablen GRAZE und YR.ISOL eine aus dem Modell entfernen, da sie zu\n",
        "stark negativ korreliert sind. Dabei sind drei Dinge wichtig:\n",
        "\n",
        "- Statistisch gibt es kein klares Argument, welche von mehreren\n",
        "    hoch-korrelierten Variablen man im vollen Modell streichen sollte\n",
        "    (man könnte höchstens zusätzlich den VIF heranziehen). Inhaltlich\n",
        "    macht es Sinn, diejenige Variable beizubehalten, die (a) besser\n",
        "    interpretierbar ist oder (b) häufiger in vergleichbaren Studien\n",
        "    gebraucht wurde.\n",
        "\n",
        "- Man sollte im Methodenteil dokumentieren welche Variable(n) wegen\n",
        "    positiver/negativer Korrelation mit welcher anderen aus dem vollen\n",
        "    Modell gestrichen wurden.\n",
        "\n",
        "- Bei der Interpretation der Ergebnisse stehen die beibehaltenen\n",
        "    Variablen auch für die jeweils gestrichenen hochkorrelierten\n",
        "    Variablen (zumindest zu einem erheblichen Teil).\n",
        "\n",
        "Die Berechnung der VIF's geht wie folgt:\n",
        "\n",
        "```{.r}\n",
        "library(car)\n",
        "\n",
        "\n",
        "vif(model)\n",
        "\n",
        "\n",
        "YR.ISOL ALT GRAZE\n",
        "\n",
        "\n",
        "1.679995 1.200372 1.904799\n",
        "```\n",
        "\n",
        "Hier sieht man nicht, welche Variable mit welcher anderen korrliert ist,\n",
        "man bekommt nur ein Gesamtranking. Da die VIF-Werte aller drei Variablen\n",
        "unter 5 sind, können alle beibehalten werden. Wenn mehrere Variablen\n",
        "einen VIF \\> 5 haben, muss man schrittweise immer die Variable mit dem\n",
        "höchsten VIF-Wert entfernen und die VIF-Werte dann neuberechnen. Sie\n",
        "ändern sich, wenn eine Variable wegfällt, da sie die\n",
        "Gesamt-Korrelationsstruktur des Datensatzes widerspiegeln.\n",
        "\n",
        "### Problem 2: Overfitting\n",
        "\n",
        "Das Problem des Overfitting soll mit der folgenden Simulation\n",
        "veranschaulicht werden: zu einer Stichprobe von sechs Beobachtungen mit\n",
        "zwei numerischen Variablen werden schrittweise polynomische Modelle\n",
        "höher Ordnung gefittet.\n",
        "\n",
        "Der Code dafür ist:\n",
        "\n",
        "```{.r}\n",
        "lm=lm(y\\~x)\n",
        "\n",
        "\n",
        "xy <- seq(from=0,to=10,by=0.1)\n",
        "\n",
        "\n",
        "yv <- predict(lm,list(x=xv))\n",
        "\n",
        "\n",
        "lines(xv,yv)\n",
        "\n",
        "\n",
        "lm2=lm(y\\~x+I(x\\^2))\n",
        "\n",
        "\n",
        "xy <- seq(from=0,to=10,by=0.1)\n",
        "\n",
        "\n",
        "yv <- predict(lm2,list(x=xv))\n",
        "\n",
        "\n",
        "lines(xv,yv)\n",
        "\n",
        "\n",
        "\\[usw.\\]\n",
        "```\n",
        "\n",
        "Das Ergebnis sieht folgendermassen aus:\n",
        "\n",
        "![](./myMediaFolder/media/image60.png){width=\"3.28009186351706in\"\n",
        "height=\"2.952755905511811in\"}***R*² = 0.012**\n",
        "\n",
        "![](./myMediaFolder/media/image61.png){width=\"3.28009186351706in\"\n",
        "height=\"2.952755905511811in\"}***R*² = 0.111**\n",
        "\n",
        "![](./myMediaFolder/media/image62.png){width=\"3.28009186351706in\"\n",
        "height=\"2.952755905511811in\"}***R*² = 0.170**\n",
        "\n",
        "![](./myMediaFolder/media/image63.png){width=\"3.28009186351706in\"\n",
        "height=\"2.952755905511811in\"}***R*² = 0.875**\n",
        "\n",
        "![](./myMediaFolder/media/image64.png){width=\"3.28009186351706in\"\n",
        "height=\"2.952755905511811in\"}***R*² = 1.000**\n",
        "\n",
        "Wir sehen, dass die erklärte Varianz kontinuierlich vom\n",
        "2-Parameter-Modell (Achsenabschnitt und Steigung) zum 6-Parameter-Modell\n",
        "(Achsenabschnitt, Parameter für *x* bis *x*^5^) zunimmt. Ein\n",
        "polynomische Modell (*n* -- 1). Ordnung erzielt immer 100% Anpassung and\n",
        "die Daten (*R*^2^ = 1), wenn man *n* Beobachtungen hat. Aber ist das\n",
        "Modell deswegen auch besonders korrekt oder aussagekräftig? Das darf\n",
        "bezweifelt werden. Ein gutes Modell wäre ja eines, das die zugrunde\n",
        "liegende Gesetzmässigkeit erkennt und daher auch für die Interpolation\n",
        "und Extrapolation geeignet ist.\n",
        "\n",
        "Es zeigt sich, dass die gute Anpassung an die Daten (good fit, hier\n",
        "gemessen als R2) nur der eine Aspekt eines guten Modells ist. Zugleich\n",
        "sollte es möglichst einfach (*parsimonous*) sein, d. h. das Beobachtete\n",
        "mit möglichst wenigen Annahmen erklären. Es gilt das folgende Prinzip,\n",
        "das auf den mittelalterlichen Philosophen Willliam of Ockham (ca.\n",
        "1288--1347 zurückgeht).\n",
        "\n",
        "![https://upload.wikimedia.org/wikipedia/commons/a/ab/William_of_Ockham\\_-\\_Logica_1341.jpg](./myMediaFolder/media/image65.jpeg){width=\"2.7592760279965005in\"\n",
        "height=\"2.573024934383202in\"}\\\n",
        "(Skizze aus einer Handschrift von Ockhams *Summa logicae*)\n",
        "\n",
        "```{.r}\n",
        "Ockham's razor = Law of parsimony (Sparsamkeitsprinzip)\n",
        "```\n",
        "\n",
        "***Wesenheiten dürfen nicht über das Notwendige hinaus vermehrt\n",
        "werden***\n",
        "\n",
        "Formulierung von Johannes Clauberg (1622--1665)\n",
        "\n",
        "### Modellvereinfachung \n",
        "\n",
        "Nun stellt sich die Frage, wie wir vom **vollen Modell (*full model,\n",
        "global model*)** also jenem nach Entfernung hochkorrelierter Variablen\n",
        "zum „besten\" Modell gelangt, das also eine bestmögliche Kombination von\n",
        "guter Anpassung an die Daten (Fit) und Parsimonie aufweist. Dieses\n",
        "anzustrebende statistische Modell wird auch **minimal adäquates Modell\n",
        "(*mininum adequate model*)** genannt.\n",
        "\n",
        "Ganz generell gilt: Man sollte **maximal *p* = *n* / 3 Parameter\n",
        "fitten** (wobei *n* = Zahl der Datenpunkte/Beobachtungen und bei *p*\n",
        "auch der Achsenabschnitt \\[*b*~0~\\] mitgezählt wird).\n",
        "\n",
        "Mögliche **Kriterien für das „beste\" Modell** (*minimum adequate\n",
        "model*):\n",
        "\n",
        "1.  **Höchster *R*²~adj.~ =** $\\mathbf{1}\\mathbf{-}$\n",
        "    $\\frac{\\mathbf{SS}_{\\mathbf{Residual}}\\mathbf{/\\lbrack n -}\\left( \\mathbf{p + 1} \\right)\\mathbf{\\rbrack}}{\\mathbf{SS}_{\\mathbf{Total}}\\mathbf{/(n - 1)}}$**\\\n",
        "    **(vgl. *R*² =\n",
        "    $\\frac{{SS}_{Regression}}{{SS}_{Total}} = 1 - \\frac{{SS}_{Residual}}{{SS}_{Total}}$)\\\n",
        "    Ist nicht wirklich zielführend, da der „Strafterm\" (um den *R*²\n",
        "    reduziert wird) zu gering ist, um wirklich für Parsimonität zu\n",
        "    sorgen.\n",
        "\n",
        "2.  **Schrittweise Modellvereinfachung ausgehend vom „maximalen Modell\"\\\n",
        "    **Durch: Entfernen von (a) nicht-signifikanten Interaktionen, (b)\n",
        "    nicht-signifikanten quadratischen Termen und schliesslich (c)\n",
        "    nicht-signifkanten linearen Variablen.\n",
        "\n",
        "Die schrittweise Modellvereinfachung kann wiederum auf drei verschiedene\n",
        "Weisen geschehen (die meist, aber nicht immer, die gleichen Ergebnisse\n",
        "liefern):\n",
        "\n",
        "a.  **Schrittweise die am wenigsten signifkanten Terme entfernen**, bis\n",
        "    alle signifikant sind:\\\n",
        "    \\\n",
        "    **model1 <- lm (ABUND \\~ YR.ISOL + ALT + GRAZE, data=loyn)\\\n",
        "    summary(model1)\\\n",
        "    model2 <- update(model1,\\~.-YR.ISOL)\\\n",
        "    summary(model2)**\n",
        "\n",
        "b.  **Mittels ANOVA schrittweise Modelle vergleichen** und Terme\n",
        "    hinzufügen, wenn signifikent, bzw. entfernen, wenn nicht\\\n",
        "    \\\n",
        "    **anova(model1,model2)**\n",
        "\n",
        "c.  Eine **automatische Funktion** zum schrittweisen Hinzufügen\n",
        "    (*forward selection*) oder Löschen (*backward selection*) oder\n",
        "    beidem verwenden (es gibt verschiedene Packages, bei Interesse bitte\n",
        "    googlen).\n",
        "\n",
        "Varianten a bis c sind im Prinzip OK, man muss sich aber bewusst sein,\n",
        "dass gerade bei vielen Variablen dieses schrittweise Vorgehen nicht\n",
        "zwingend das wirklich beste Modell findet, sondern man in einem \"lokalen\n",
        "Optimum\" landen kann (als Alternative siehe die dredge-Funktion unter\n",
        "„*Information theoretician approach* und *multimodel inference*\".\n",
        "\n",
        "### Varianzpartitionierung \n",
        "\n",
        "Wenn man das minimal adäquate Modell gefunden hat, will man oft noch\n",
        "wissen, wie bedeutsam die einzelnen enthaltenen Variablen sind.\n",
        "Bedeutsamkeit/Relevanz haben wir weiter oben als *R*² (erklärte Varianz)\n",
        "ausgedrückt. Wir können uns also anschauen, **welche Anteile der\n",
        "erklärten Varianz auf welche Variablen zurückgehen**. Da unsere\n",
        "Variablen (auch nach einem Korrelationstest und Ausschluss der besonders\n",
        "hoch korrelierten) nicht völlig orthogonal = unabhängig voneinander\n",
        "sind, verhalten sich die Varianzen nicht additiv. Vielmehr ist die\n",
        "erklärte Varianz in einem Modell mit zwei Variablen meist niedriger als\n",
        "die Summe der Varianzen der beiden Einzelmodelle. In einer\n",
        "Varianzpartitionierung wird die Varianz jeder Variablen daher in eine\n",
        "unabhängige (*independent*, I) und eine gemeinsame (*joint*, J)\n",
        "Komponente zerlegt:\n",
        "\n",
        "```{.r}\n",
        "library(hier.part)\n",
        "\n",
        "\n",
        "loyn.preds <- with(loyn, data.frame(YR.ISOL,ALT,GRAZE))\n",
        "\n",
        "\n",
        "hier.part(loyn$ABUND,loyn.preds,gof=\"Rsqu\")\n",
        "\n",
        "\n",
        "$IJ\n",
        "\n",
        "\n",
        "I J Total\n",
        "\n",
        "\n",
        "YR.ISOL 0.11892853 0.13444049 0.2533690\n",
        "\n",
        "\n",
        "ALT 0.06960132 0.07926823 0.1488696\n",
        "\n",
        "\n",
        "GRAZE 0.30019854 0.16562324 0.4658218 \n",
        "\n",
        "\n",
        "$I.perc\n",
        "\n",
        "\n",
        "I\n",
        "\n",
        "\n",
        "YR.ISOL 24.33428\n",
        "\n",
        "\n",
        "ALT 14.24131\n",
        "\n",
        "\n",
        "GRAZE 61.42441 \n",
        "```\n",
        "\n",
        "Der grösste Teil (61%) der insgesamt erklärten Varianz dieses\n",
        "Drei-Parameter-Models wird hier also durch den Faktor Grazing erklärt.\n",
        "\n",
        "### Ergebnisdarstellung: partielle Regressionen und 3-D-Grafiken\n",
        "\n",
        "Während sich die ermittelte Beziehung zwischen Antwort- und\n",
        "Prädiktor-Variable auch bei nichtlinearen Verläufen einfach mit predict\n",
        "visualisieren lässt, solange man nur eine Prädiktorvariable hat (selbst\n",
        "wenn sie in transformierter Weise im lm eingespeist wird), ist das bei\n",
        "mehreren Prädiktoren eine Herausforderung. Hier seien zwei Möglichkeiten\n",
        "kurz erwähnt:\n",
        "\n",
        "1.  **Partielle Regressionen\\\n",
        "    **(sie zeigen wie die Beziehung aussähe, wenn all übrigen Faktoren\n",
        "    konstant wären)\\\n",
        "    \\\n",
        "    **library(car)\\\n",
        "    avPlots(model, ask=F)\\\n",
        "    **![](./myMediaFolder/media/image66.png){width=\"4.008681102362205in\"\n",
        "    height=\"3.608636264216973in\"}\n",
        "\n",
        "2.  **3D Response surfaces\\\n",
        "    **(es gibt Packages, um dasselbe auch für zwei Prädiktoren\n",
        "    gleichzeitig zu machen; dies mach insbesondere Sinn, wenn auch\n",
        "    quadratische Terme dabei sind; bei Interesse bitte googlen)\n",
        "\n",
        "## Information theoretician approach und multimodel inference\n",
        "\n",
        "### Vergleich mit frequentist statistics \n",
        "\n",
        "Es gibt zwei grundlegende statistische Philosophien:\n",
        "\n",
        "```{.r}\n",
        "*Frequentist statistics* („klassisiche\" Statistik)\n",
        "```\n",
        "\n",
        "- Alles, was wir bislang gemacht haben\n",
        "\n",
        "- [Grundannahme:]{.underline} Es gibt ein einziges richtiges Modell\n",
        "    der Wirklichkeit, dem man sich mit Irrtumswahrscheinlichkeiten\n",
        "    annähern kann\n",
        "\n",
        "- Nutzt ***p*-Werte**\n",
        "\n",
        "```{.r}\n",
        "*Information theoretician approach*\n",
        "```\n",
        "\n",
        "- Das, was wir in diesem Unterkapitel besprechen\n",
        "\n",
        "- [Grundannahme:]{.underline} Es kann ähnlich gute Modelle der\n",
        "    Wirklichkeit geben, es gibt nicht das eine wahre Modell\n",
        "\n",
        "- Nutzt **keine *p*-Werte**\n",
        "\n",
        "- Dafür **AIC** (*Akaike information criterion*) oder **BIC**\n",
        "    (*Bayesian information criterion*)\n",
        "\n",
        "- **Modellmittelung** (*model averaging*) möglich\n",
        "\n",
        "### Masse der Modellgüte: AIC, BIC, AICc, Δ~i~, Evidence ratios, Akaike weights \n",
        "\n",
        "Die folgende Übersicht zeigt die wichtigsten Gütemasse im Vergleich. Wie\n",
        "schon besprochen, berücksichtigt R²adj. (nahezu) ausschliesslich den Fit\n",
        "(also die Anpassung der Kurve an die Daten). Dagegen berücksichtigen die\n",
        "Informationskriterien Fit und Komplexität (Komplexität meint das\n",
        "Gegenteil von Parsimonität). Bei AICc und BIC = SC fliesst schliesslich\n",
        "auch noch die Zahl der Datenpunkte ein:\n",
        "\n",
        "![](./myMediaFolder/media/image67.png){width=\"6.40625in\"\n",
        "height=\"1.9315463692038495in\"}\\\n",
        "(aus Johnson & Omland 2004)\n",
        "\n",
        "Dabei gilt für AIC:\n",
        "\n",
        "```{.r}\n",
        "AIC = *n* (ln(RSS)) -- *n* ln (*n*) + 2 (*k* + 1)\n",
        "``` mit\\\n",
        "RSS = Residual sum of squares\\\n",
        "*k* = Parameter des Models, inkl. Achsenabschnitt\\\n",
        "*n* = Anzahl der Beobachtungen/Replikate\n",
        "\n",
        "```{.r}\n",
        "AICc ist der AIC für „kleine\" Stichprobengrössen\n",
        "``` (wobei „klein\" bis\n",
        "zu 40 *k* reicht, also bei 2 Parametern wie in einer einfachen linearen\n",
        "Regression „gross\" erst bei 81 Datenpunkten begänne). Deshalb und da\n",
        "sich für grosses *n* AICc asymptotisch AIC nähert, sollte man einfach\n",
        "immer AICc verwenden.\n",
        "\n",
        "AIC und BIC entstammen wiederum etwas unterschiedlichen Philosophien.\n",
        "Auf die Unterschiede gehen wir nicht im Detail ein. Die Ergebnisse\n",
        "basierend auf BIC und AICc sind in dem Kontext wie wir sie hier\n",
        "vorstellen (BIC mit nicht-informativen *priors*) nahezu gleich. BIC wird\n",
        "relevant, wenn man informative *priors* verwenden kann (aber das sprengt\n",
        "den Kurs).\n",
        "\n",
        "Es gilt folgendes für AIC, AICc und BIC analog:\n",
        "\n",
        "- Der **absolute Wert eines Informationskriteriums ist belanglos** (ob\n",
        "    also -1000, 0.1 oder +1000000). Informationskriterien können nur im\n",
        "    Vergleich zweier Modelle für die gleichen Daten sinnvoll angewandt\n",
        "    werden. Dann ist das **Modell mit dem niedrigeren Wert das bessere**\n",
        "    (bei gemeinsamer Betrachtung von Fit und Komplexität).\n",
        "\n",
        "- **∆*~i~* = AIC*~i~* -- AIC~min~\\\n",
        "    ∆*~i~*** ist die Differenz im AIC (oder eines anderen\n",
        "    Informationskriteriums) zwischen einem bestimmten Modell i und dem\n",
        "    jeweils besten Modell im Vergleich. Dabei wird meist die folgende\n",
        "    Konvention verfolgt:\\\n",
        "    - wenn **∆*~i~*** ≤ 2: Modelle sind statistisch „gleichwertig\"\\\n",
        "    - wenn **∆*~i~*** \\> 4: Modell nicht relevant\n",
        "\n",
        "```{=html}\n",
        "<!-- -->\n",
        "```\n",
        "- ***Likelihood*** von Modell *g~i~* für die Daten:\\\n",
        "    *L* = exp (-1/2 ∆*~i~*)\n",
        "\n",
        "\n",
        "```{=html}\n",
        "<!-- -->\n",
        "```\n",
        "\n",
        "- ***Evidence ratio*:**\\\n",
        "    (etwa: wie vielfach besser ist das beste Modell verglichen mit\n",
        "    Modell *i*?)\\\n",
        "    ER = *L*~best~ / *L~i~*\n",
        "\n",
        "- ***Akaike weights*:\\\n",
        "    **Normalisierte *Likelihoods* über alle verglichenen Modelle:\\\n",
        "    *W~i~* = exp (-1/2 ∆*~i~*) / ∑ \\[exp (-1/2 ∆*~j~*)\\]\n",
        "\n",
        "∆*~i~*, Likelihood, ER und Akaike weights stehen alle für die gleiche\n",
        "Information in verschiedenen Darstellungen/Transformationen. Als\n",
        "besonders praktisch erweisen sich die **Akaike weights *W~i~***. Nach\n",
        "ihrer Definition summieren sich die Akaike weights aller verglichenen\n",
        "Modelle zu 1. *W~i~* kann daher als die Wahrscheinlichkeit interpretiert\n",
        "werden, dass Modell *i* unter den verglichenen Modellen das beste ist.\n",
        "\n",
        "Da AIC und *p*-Werte aus verschiedenen und nicht kompatiblen\n",
        "statistischen Philosophien stammen, sollte man in einer mit\n",
        "Informationskriterien arbeitenden Studie nicht zusätzlich auch noch\n",
        "*p*-Werte angeben. *R*²-Werte sind dagegen in beiden „statistischen\n",
        "Welten\" sinnvoll und wichtig.\n",
        "\n",
        "### Multimodel inference\n",
        "\n",
        "Der Charme der Informationskriterien ist, dass sie sich besonders gut\n",
        "dann eignen, wenn man viele verschiedene Modelle vergleicht, etwa weil\n",
        "man ein grössere Zahl von potenziellen Prädiktoren erhoben hat, mit\n",
        "denen man eine abhängige Variable erklären will, etwas in einer\n",
        "multiplen Regression oder einer mehrfaktoriellen ANOVA oder einem\n",
        "sonstigen komplexen Modell. Wenn man sich ein globales Modell mit *n*\n",
        "Termen (Achsenabschnitt und neun Steigungen für Prädiktorvariablen,\n",
        "transformierte Prädiktorvariablen oder Interaktionen zwischen\n",
        "Prädiktorvariablen) vorstellt, beinhaltet das 2*^n^* Einzelmodelle für\n",
        "alle möglichen Kombinationen der Terme von 0 bis *n* Prädiktoren. Bei\n",
        "*n* = 10 wären das bereits 1024 verschiedene Modelle. Diese alle zu\n",
        "berechnen ist ein grosser Aufwand, weswegen man früher versucht hat, in\n",
        "solchen Fällen das minimal adäquate Modell in einer weniger\n",
        "rechenaufwändigen Weise zu finden, indem man eine *stepwise\n",
        "forward/backward variable selection* durchgeführt hat (siehe Kapitel\n",
        "„Modellvereinfachung\" oben). Heute ist das Ausrechnen von 1000 Modellen\n",
        "selbst auf einem einfachen Notebook nur noch eine Sache von Sekunden,\n",
        "d.h. man kann seine Entscheidung effektiv auf dem Vergleich aller mit\n",
        "den verfügbaren Variablen möglichen Teilmodelle gründen. Die\n",
        "dredge-Funktion im MuMIn-Paket macht genau dieses. Bis etwa 15 Terme (d.\n",
        "h. 32768 zu vergleichende Modelle) funktioniert dredge auch auf\n",
        "einfachen Notebooks noch im Bereich weniger Minuten (aber man muss schon\n",
        "merklich auf das Ergebnis warten); jeder weitere Term führt aber zu\n",
        "einer Verdopplung der Rechenzeit.\n",
        "\n",
        "Schauen wir uns das anhand des schon bekannten loyn-Datensatzes\n",
        "(Vogelvorkommen in Waldfragmenten) an:\n",
        "\n",
        "```{.r}\n",
        "library(MuMIn)\n",
        "\n",
        "\n",
        "global.model <- lm (ABUND \\~ YR.ISOL + ALT + GRAZE, data=loyn)\n",
        "\n",
        "\n",
        "options(na.action=\"na.fail\")\n",
        "\n",
        "\n",
        "allmodels <- dredge(global.model)\n",
        "\n",
        "\n",
        "allmodels\n",
        "\n",
        "\n",
        "Model selection table\n",
        "\n",
        "\n",
        "(Int) ALT GRA YR.ISO df logLik AICc delta weight\n",
        "\n",
        "\n",
        "3 34.370 -4.981 3 -194.315 395.1 0.00 0.407\n",
        "\n",
        "\n",
        "4 28.560 0.03191 -4.597 4 -193.573 395.9 0.84 0.267\n",
        "\n",
        "\n",
        "7 -62.750 -4.440 0.04898 4 -193.886 396.6 1.46 0.196\n",
        "\n",
        "\n",
        "8 -73.580 0.03285 -4.017 0.05143 5 -193.087 397.4 2.28 0.130\n",
        "\n",
        "\n",
        "6 -348.500 0.07006 0.18350 4 -200.670 410.1 15.03 0.000\n",
        "\n",
        "\n",
        "5 -392.300 0.21120 3 -203.690 413.8 18.75 0.000\n",
        "\n",
        "\n",
        "2 5.598 0.09515 3 -207.358 421.2 26.09 0.000\n",
        "\n",
        "\n",
        "1 19.510 2 -211.871 428.0 32.88 0.000\n",
        "```\n",
        "\n",
        "Wie man sieht, wurde hier zunächst ein globales Modell mit den drei\n",
        "Prädiktoren YR.ISOL, ALT und GRAZE erstellt. Im nächsten Schritt wurde\n",
        "dann mit der dredge-Funktion dann ein Objekt allmodels generiert, das\n",
        "die 2^3^ = 8 möglichen Teilmodelle enthält. In der Tabellenausgabe sieht\n",
        "man, dass unter diesen Modell Nr. 3, das nur einen Achsenabschnitt und\n",
        "GRAZE enthält mit einem Akaike weight von 0.407 das beste Modell ist.\n",
        "Allerdings unterscheiden sich die Modelle Nr. 4 und 7 um weniger als 2\n",
        "AICc-Einheiten, sind also als praktisch gleichwertig zu betrachten. Sie\n",
        "haben daher auch nur etwas geringere Variable importances von 0.267 und\n",
        "0.196.\n",
        "\n",
        "Anders als bei der *frequentist statistician*-Ansatz geht es nicht\n",
        "darum, ein einziges bestes Modell zu finden, sondern eine Aussage über\n",
        "ein Ensemble von plausiblen Modellen zu treffen. Es gibt hier zwei\n",
        "gängige Ansätze, ***Variable importance*** und ***Model averaging***.\n",
        "\n",
        "```{.r}\n",
        "*Variable importance*\n",
        "``` steht dabei für die Summe der *W~i~*-Werte\n",
        "aller Teilmodelle, die eine bestimmte Variable enthalten. Da *W~i~*\n",
        "selbst von 0 bis 1 reicht, gilt dies auch für die Variable importance.\n",
        "Eine Variable importance von 1 bedeutet dabei, dass alle plausiblen\n",
        "Modelle die entsprechende Variable beinhalten. Mithin sagt uns die\n",
        "Variable importance wie bedeutsam eine bestimmte Variable innerhalb der\n",
        "Menge der verglichenen Teilmodelle ist. Aber Achtung: *Variable\n",
        "importance* hat nichts mit Signifikanz oder *p*-Werten zu tun!!! Es gibt\n",
        "keine generelle Konvention, ab welcher Variable importance eine Variable\n",
        "als bedeutsam angesehen wird, aber häufig wird 50 % als Schwelle\n",
        "verwendet. In R geht das folgendermassen:\n",
        "\n",
        "```{.r}\n",
        "importance(allmodels)\n",
        "\n",
        "\n",
        "GRAZE ALT YR.ISOL\n",
        "\n",
        "\n",
        "Importance: 1.00 0.40 0.33\n",
        "\n",
        "\n",
        "N containing models: 4 4 4\n",
        "```\n",
        "\n",
        "Während logischerweise jede der drei Variablen in jeweils vier\n",
        "Teilmodellen vorkommt, unterscheiden sie sich erheblich in der Variable\n",
        "importance. Alle nach der obigen Tabelle relevanten Modelle (∆*~i~* <\n",
        "4) enthalten GRAZE, aber nur je zwei von ihnen auch die beiden anderen\n",
        "Variablen. Entsprechend ist die *Variable importance* von GRAZE nahe 1,\n",
        "während sie von ALT und YR.ISOL unter 0.5 liegt.\n",
        "\n",
        "Model averiging ist eine andere interessante Möglichkeit des Information\n",
        "theoreticion-Ansatzes und der Multimodel inference. Hier werden quasi\n",
        "alle möglichen Modelle oder alle Modelle mit einem ∆*~i~* unter einem\n",
        "bestimmten Schwellenwert zu einem gemittelten Modell zusammengefasst,\n",
        "gewichtet nach ihrem *W~i~*-Wert. Am Ende bekommt man eine einzige\n",
        "gemittelte Funktion, deren Funktionsparameter man interpretieren und die\n",
        "man plotten kann.\n",
        "\n",
        "**avgmodel <-\n",
        "model.avg(get.models(dredge(model,rank=\"AICc\"),subset=TRUE))**\n",
        "\n",
        "```{.r}\n",
        "summary(avgmodel)\n",
        "\n",
        "\n",
        "full average)\n",
        "\n",
        "\n",
        "Estimate Std. Error Adjusted SE z value Pr(\\>\\|z\\|)\n",
        "\n",
        "\n",
        "(Intercept) -0.29874 77.23966 78.39113 0.004 0.997\n",
        "\n",
        "\n",
        "GRAZE -4.64605 0.89257 0.91048 5.103 3e-07 \\*\\*\\*\n",
        "\n",
        "\n",
        "ALT 0.01282 0.02311 0.02340 0.548 0.584\n",
        "\n",
        "\n",
        "YR.ISOL 0.01631 0.03883 0.03941 0.414 0.679\n",
        "```\n",
        "\n",
        "Man beachte, dass der Output auch einen *p*-Wert enthält, obwohl dieser\n",
        "im AIC-Kontext nicht sinnvoll ist.\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- Eine **ANCOVA** kommt zur Anwendung, wenn auf die abhängige Variable\n",
        "    sowohl eine kategoriale als auch eine metrische Prädiktorvariable\n",
        "    einwirken.\n",
        "\n",
        "- Auch eine **polynomiale Regression** ist ein lineares Modell und\n",
        "    kann u. a. dazu dienen, auf einfache Weise einen unimodalen\n",
        "    Zusammenhang zu beschreiben.\n",
        "\n",
        "- **Multiple Regressionen** sind lineare Regressionen mit mehreren\n",
        "    Prädiktoren.\n",
        "\n",
        "- Bei multiplen Regressionen muss man die **weitgehende\n",
        "    Unabhängigkeit** der ins globale Modell eingespeisten Variablen\n",
        "    sicherstellen.\n",
        "\n",
        "- Für die Suche nach dem **minimalen adäquaten Modell** kommen\n",
        "    unterschiedliche Strategien infrage, wie die schrittweise Entfernung\n",
        "    nicht-signifikanter Terme aus dem globalen Modell oder Auswahl des\n",
        "    besten Modells aus allen möglichen Modellen mittels AICc.\n",
        "\n",
        "- **AICc** ist ein Gütemass im ***information theoretician\n",
        "    approach***. AICc-Werte sind nur im Vergleich mit anderen\n",
        "    AICc-Werten für die gleichen Daten informativ; dann bezeichnet der\n",
        "    niedrigste AICc-Wert das beste Modell.\n",
        "\n",
        "- «Frequentist approach» («Standardstatistik») und «information\n",
        "    theoretician approach» sind **zwei verschiedene statistische\n",
        "    «Philosophien»**, die man nicht in ein und derselben Auswertung\n",
        "    kombinieren sollte: also entweder *p*-Werte oder AICc-Werte; *R*²\n",
        "    macht dagegen in beiden «Welten» Sinn.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed.\n",
        "John Wiley & Sons, Chichester, UK: 339 pp.**\n",
        "\n",
        "```{.r}\n",
        "- Chapter 7: Regression (pp. 140--141)\n",
        "\n",
        "\n",
        "- Chapter 9: Analysis of Covariance\n",
        "\n",
        "\n",
        "- Chapter 10: Multiple Regression\n",
        "\n",
        "\n",
        "- Chapter 12: Other Response Variables (p. 233 \\[AIC\\])\n",
        "```\n",
        "\n",
        "Burnham, K.P. & Anderson, D.R. 2002. *Model selection and multimodel\n",
        "inference -- a practical information-theoretic approach*. 2nd ed.\n",
        "Springer, New York, US: 488 pp.\n",
        "\n",
        "Johnson, J.B. & Omland, K.S. 2004. Model selection in ecology and\n",
        "evolution. *Trends in Ecology and Evolution* 19: 101--108.\n",
        "\n",
        "Logan, M. 2010. *Biostatistical design and analysis using R. A practical\n",
        "guide*. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\\\n",
        "- pp. 208-253 (Multiple und nicht-lineare Regressionen)\n",
        "\n",
        "Quinn, P.Q. & Keough, M.J. 2002. *Experimental design and data analysis\n",
        "for biologists*. Cambridge University Press, Cambridge, UK: 537 pp.\n",
        "\n",
        "# Statistik 4: Komplexere Regressionsmethoden\n",
        "\n",
        "**Heute geht es hauptsächlich um *generalized linear models* (GLMs), die\n",
        "einige wesentliche Limitierungen von linearen Modellen überwinden. Indem\n",
        "sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht\n",
        "mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden.\n",
        "Bei *generalized linear regressions* muss man sich zwischen\n",
        "verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch\n",
        "werden wir uns die Poisson-Regressionen für Zähldaten und die\n",
        "logistische Regression für ja/nein-Daten anschauen. Danach folgt ein\n",
        "Einstieg in nicht-lineare Regressionen, die es erlauben, etwa\n",
        "Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum\n",
        "Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und\n",
        "*general additive models* (GAMs).**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *habt den Unterschied zwischen linearen und nicht-linearen\n",
        "    Regressionen verstanden und könnt eine einfache **nicht-lineare\n",
        "    Regression** in R implementieren;*\n",
        "\n",
        "- *habt verstanden, worin sich **GLMs** von linearen Regressionen\n",
        "    unterscheiden und wann sie zur Anwendung kommen; könnt die beiden\n",
        "    häufigsten GLM-Typen l**ogistische Regression** und **(Quasi-)\n",
        "    Poisson-Regression** in R richtig anwenden und die Ergebnisse\n",
        "    interpretieren; und*\n",
        "\n",
        "- *wisst, wofür **LOWESS** und **GAM** stehen und wie man sie\n",
        "    anwendet.*\n",
        "\n",
        "## Von linearen Modellen zu GLMs\n",
        "\n",
        "### Zwei Beispiele\n",
        "\n",
        "Nehmen wir an, wir wollten modellieren, wie viele Besucher an einem\n",
        "Strandabschnitt zur Mittagszeit in Abhängigkeit von der herrschenden\n",
        "Lufttemperatur anzutreffen sind. Unsere Daten sehen folgendermassen aus\n",
        "und mit den bekannten Methoden können wir ein lm rechnen, dessen\n",
        "Ergebnis signifikant ist und sogar recht viel der Gesamtvarianz erklärt:\n",
        "\n",
        "![](./myMediaFolder/media/image68.emf.png){width=\"2.46875in\"\n",
        "height=\"3.0328619860017496in\"}![](./myMediaFolder/media/image69.png){width=\"3.9371708223972in\"\n",
        "height=\"3.15625in\"}\n",
        "\n",
        "Unsere abhängige Variable ist eine Zählung und verhält sich daher anders\n",
        "als eine echte metrische Variable (etwa einer Messung des pH-Wertes).\n",
        "**Zähldaten** stellen lineare Modelle (lm) vor **vier Probleme:**\n",
        "\n",
        "- Lineare Modelle sagen immer auch das Auftreten **negativer Werte**\n",
        "    voraus, wohingegen **absolute Häufigkeiten immer positive\n",
        "    Ganzzahlen** sind (im obigen Beispiel würde das Modell bereits im\n",
        "    gefitteten Bereich, unter etwa 12 °C, negative Menschen\n",
        "    vorhersagen).\n",
        "\n",
        "- Nahezu immer sind Zähldaten **rechtsschief verteilt**, also nicht\n",
        "    normalverteilt und auch nicht symmetrisch\n",
        "\n",
        "- Bei Zähldaten nimmt nahezu immer die **Varianz mit dem Mittelwert\n",
        "    zu**.\n",
        "\n",
        "- Zähldaten folgen keiner kontinuierlichen (wie die Normalverteilung),\n",
        "    sondern einer **diskreten Verteilung**.\n",
        "\n",
        "Theoretisch sind also die Voraussetzungen für ein lineares Modell bei\n",
        "Zähldaten nie erfüllt. In der Praxis gibt es aber Situationen, wo die\n",
        "Verletzung der Annahmen für das Modell nicht weiter problematisch ist\n",
        "und man mit einem lm zu korrekten Aussagen gelangen kann. Relativ\n",
        "problemlos funktioniert das (und wird auch noch häufig getan), wenn (a)\n",
        "alle Werte der Antwortvariablen weit von 0 entfernt sind und (b) die\n",
        "Werte der Antwortvariable um deutlich weniger als eine Grössenordnung\n",
        "(d.h. Faktor 10) variieren. Im obigen Beispiel beträgt der Quotient des\n",
        "grössten und kleinsten Wertes der Antwortvariablen 2000 / 12 = 167. Mit\n",
        "etwas Erfahrung sehen wir schon im Scatterplot, dass hier Linearität und\n",
        "Varianzhomogenität verletzt sind.\n",
        "\n",
        "Ein anderes Beispiel, bei dem ein lineares Modell offensichtlich und\n",
        "immer scheitern würde, wäre eine Befragung von Touristen an Tagen\n",
        "unterschiedlicher Temperatur, ob sie schwimmen gegangen sind. Das\n",
        "Ergebnis könnte wie folgt aussehen (stark gekürzte Tabelle, an jedem Tag\n",
        "(d.h. bei gleicher Temperatur) wurden jeweils mehrere Touristen\n",
        "befragt):\n",
        "\n",
        "![](./myMediaFolder/media/image70.emf.png){width=\"2.468503937007874in\"\n",
        "height=\"2.1618897637795276in\"}\n",
        "\n",
        "Bei solchen „binären Daten\" bestehen zwei hauptsächliche Probleme für\n",
        "lineare Modelle:\n",
        "\n",
        "- Die Werteverteilung ist nach unten und nach oben begrenzt.\n",
        "\n",
        "- Es gibt überhaupt nur zwei mögliche Werte, nein und ja, als 0 und 1\n",
        "    codiert.\n",
        "\n",
        "### Die Idee der Generalized linear models (GLMs)\n",
        "\n",
        "```{.r}\n",
        "*Generalized linear models* (GLMs)** verallgemeinern \n",
        "```lineare Modelle\n",
        "(LMs)**, um Fälle wie die geschilderten (Zähldaten, Binärdaten, für\n",
        "weitere Beispiele siehe Crawley (2015)) modellieren zu können.\n",
        "„Generalisiert\" heissen die GLMs aus folgenden drei Gründen:\n",
        "\n",
        "- Alle LMs sind im Begriff GLM eingeschlossen (aber viele GLMs sind\n",
        "    keine LMs).\n",
        "\n",
        "- Die **Verteilung der „Zufallskomponente\"** (= Residuen) kann sich\n",
        "    **von einer Normalverteilung unterscheiden** (muss aber aus der\n",
        "    exponentiellen Familie von Verteilungen sein).\n",
        "\n",
        "- Die abhängige Variable kann **auf verschiedene Weise mit den\n",
        "    Prädiktoren verknüpft** (*linked*) sein.\n",
        "\n",
        "### Die drei Komponenten eines GLM\n",
        "\n",
        "Ein GLM setzt sich aus drei Komponenten zusammen, die relativ frei\n",
        "kombiniert werden können (aber für bestimmte Zufallskomponenten gibt es\n",
        "Standard-Link-Funktionen):\n",
        "\n",
        "1\\. **Zufallskomponente (d. h. die Verteilung der Residuen):**\n",
        "\n",
        "- normal\n",
        "\n",
        "- binomial: z. B. ja/nein, tot/lebendig\n",
        "\n",
        "- Poisson: Zähldaten (funktioniert aber nicht immer)\n",
        "\n",
        "- gamma\n",
        "\n",
        "- negativ binomial (Dispersionsparameter muss geschätzt werden)\n",
        "\n",
        "2\\. **Systematische Komponente (d. h. die *x*-Werte):** es ist alles\n",
        "möglich, was wir schon von LMs her kennen:\n",
        "\n",
        "- kontinuierliche (metrische) Prädiktoren\n",
        "\n",
        "- kategoriale Prädiktoren\n",
        "\n",
        "- Interaktionen von Prädiktoren\n",
        "\n",
        "- polynomiale Funktionen\n",
        "\n",
        "- jewede Kombination aus den vorhergehenden Elementen\n",
        "\n",
        "```{.r}\n",
        "3. Link-Funktion:\n",
        "```\n",
        "\n",
        "- Identität (*identity*)\n",
        "\n",
        "- log (für Zähldaten)\n",
        "\n",
        "- logit (für Binärdaten)\n",
        "\n",
        "### Mögliche Verteilungen von Werten und von Varianzen\n",
        "\n",
        "Was mit verschiedenen **Verteilungen der Residuen** gemeint ist,\n",
        "veranschaulichen die folgenden beiden Abbildungen von vier\n",
        "Häufigkeitsverteilungen mit dem gleichen Mittelwert. Oben sind die\n",
        "**kontinuierliche Normalverteilung** und unten drei unterschiedliche\n",
        "diskrete Verteilungen (Poisson, negativ-binomial) zu sehen:\n",
        "\n",
        "![](./myMediaFolder/media/image71.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"5.7724584426946635in\"}\n",
        "\n",
        "Auch die **Beziehung von Varianzen zum (vorhergesagten) Mittelwert**\n",
        "müssen keinesfalls immer konstant sein, wie wir das von den linearen\n",
        "Modellen kennen. Vielmehr zeigen viele Datentypen eine systematische\n",
        "Veränderung der Varianz mit dem Mittelwert:\n",
        "\n",
        "![](./myMediaFolder/media/image72.jpeg){width=\"5.8342979002624675in\"\n",
        "height=\"4.59375in\"}\\\n",
        "(aus Crawley 2015)\n",
        "\n",
        "### Typen von GLMs\n",
        "\n",
        "Eine Übersicht gängige GLM-Typen bietet die folgende Tabelle (man\n",
        "beachte die uneinheitliche Gross-/Kleinschreibung der Verteilungen):\n",
        "\n",
        "![](./myMediaFolder/media/image73.emf.png){width=\"6.368204286964129in\"\n",
        "height=\"3.0527569991251093in\"}\\\n",
        "(übersetzt und modifiziert nach Šmilauer 2017)\n",
        "\n",
        "Man beachte, dass ein GLM mit Normalverteilung (gaussian) und\n",
        "identity-Link identisch mit einem LM ist.\n",
        "\n",
        "Wenn man dieser Anleitung strikt folgen würde (was auch Smilauer 2017\n",
        "nicht tut), dürfte man LMs nur dann verwenden, wenn die Antwortvariable\n",
        "auch negative Werte annehmen kann und ansonsten ein Gamma-GLM rechnen.\n",
        "In Realität werden Gamma-GLMs aber fast ausschliesslich für *death and\n",
        "failure*-Daten verwendet, bei denen die Varianz mit dem Quadrat des\n",
        "Mittelwertes zunimmt.\n",
        "\n",
        "GLMs mit binomialer, Poisson, Gamma- und Gauss (Normal)-Verteilung sind\n",
        "in Base R implementiert, für negative.binomial benötigt man das Package\n",
        "MASS. In diesem Kurs gehen wir im Detail nur auf die beiden\n",
        "meistbenutzten GLM-Typen ein, **Poisson-Regression für Zähldaten** und\n",
        "**logistische Regression für Binärdaten**. Mehr zu den übrigen Typen\n",
        "findet man u. a. in Crawley (2015), Dunn & Smyth (2018) und Fox &\n",
        "Weisberg (2019)\n",
        "\n",
        "### Das Fitten und die Modellgüte von GLMs\n",
        "\n",
        "Bei einem **linearen Modell (LM)** wird die Lösung durch **Minimierung\n",
        "der Summe der Abweichungsquadrate** erzielt. Diese Lösung lässt sich\n",
        "direkt, immer eindeutig und sogar von Hand ausrechnen. GLMs dagegen\n",
        "fitten die Modelle in einem iterativen Verfahren, indem die\n",
        "***Likelihood* maximiert** wird. Deswegen spricht man auch von *Maximum\n",
        "likelihood* (ML). Nach erfolgtem Fitten werden die Werte mit der\n",
        "**Umkehrfunktion der Link-Funktion** auf die originale Skala\n",
        "zurücktransformiert.\n",
        "\n",
        "Als Mass der Variabilität oder lack of fit wird bei GLMs die Devianz *D*\n",
        "verwendet, die folgendermassen definiert ist:\n",
        "\n",
        "> *D~i~* = --2 × log likelihood (Modell*~i~* \\| Daten)\n",
        "\n",
        "Je nach GLM-Typ wird die Devianz anders berechnet:\n",
        "\n",
        "![](./myMediaFolder/media/image74.jpeg){width=\"6.346522309711286in\"\n",
        "height=\"2.6145833333333335in\"}\\\n",
        "(aus Crawley 2015)\n",
        "\n",
        "## Poisson-Regressionen für Zähldaten\n",
        "\n",
        "### Berechnung\n",
        "\n",
        "Die Struktur des glm-Befehls in R ist genau identisch mit jenem des\n",
        "lm-Befehls. Nur muss man zusätzlich die Verteilung (family) und ggf. die\n",
        "Link-Funktion (wenn nicht die Standard-Link-Funktion der jeweiligen\n",
        "Verteilung) angeben. Schauen wir uns nun die Ergebnisse für unsere\n",
        "Zähldaten der Strandbesucher an, zunächst mit einem LM, dann mit einem\n",
        "Gauss-GLM und schliesslich mit einem Poisson-GLM:\n",
        "\n",
        "```{.r}\n",
        "lm.strand <- lm(Besucher\\~Temperatur)\n",
        "\n",
        "\n",
        "glm.gaussian <- glm(Besucher\\~Temperatur,family=gaussian)\n",
        "\n",
        "\n",
        "glm.poisson <- glm(Besucher\\~Temperatur,family=poisson)\n",
        "\n",
        "\n",
        "summary(lm.strand)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) -855.01 290.54 -2.943 0.021625 \\*\n",
        "\n",
        "\n",
        "Temperatur 67.62 11.80 5.732 0.000712 \\*\\*\\*\n",
        "\n",
        "\n",
        "summary(glm.gaussian)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) -855.01 290.54 -2.943 0.021625 \\*\n",
        "\n",
        "\n",
        "Temperatur 67.62 11.80 5.732 0.000712 \\*\\*\\*\n",
        "\n",
        "\n",
        "summary(glm.poisson)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error z value Pr(\\>\\|z\\|)\n",
        "\n",
        "\n",
        "(Intercept) 3.500301 0.056920 61.49 <2e-16 \\*\\*\\*\n",
        "\n",
        "\n",
        "Temperatur 0.112817 0.001821 61.97 <2e-16 \\*\\*\\*\n",
        "```\n",
        "\n",
        "Wie nach den Erläuterungen im vorigen Kapitel zu erwarten war, sind die\n",
        "Ergebnisse des LMs und des Gauss-GLMs vollkommen identisch. Jene des\n",
        "Poisson-GLMs sind dagegen anders, insbesondere viel höher signifikant.\n",
        "\n",
        "### Interpretation und Visualisierung der Ergebnisse\n",
        "\n",
        "Im Falle des lm können wir aus den Parameter-Schätzungen (Spalte\n",
        "Estimate im summary) direkt die sich ergebende Funktionsgleichung\n",
        "aufschreiben:\n",
        "\n",
        "> Besucher = --855 + 68 ∙ Temperatur/°C\n",
        "\n",
        "Bei einem glm sind die Parameter-Schätzungen dagegen nicht direkt\n",
        "interpretierbar, da sie sich auf eine transformierte Skala beziehen,\n",
        "welche durch die Link-Funktion angegeben ist. Die Standard-Link-Funktion\n",
        "bei einem Poisson-GLM ist log, also der natürliche Logarithmus (ln).\n",
        "Unser Ergebnis lässt sich damit wie folgt schreiben:\n",
        "\n",
        "> ln (Besucher) = 3.50 + 0.11 ∙ Temperatur/°C\n",
        "\n",
        "Da uns aber nicht ln (Besucher), sondern die Besucherzahl selbst\n",
        "interessiert, müssen wir die Umkehrfunktion der Link-Funktion anwenden,\n",
        "bei ln also exp. Es ergibt sich:\n",
        "\n",
        "> Besucher = exp (3.50 + 0.11 ∙ Temperatur/°C)\n",
        "\n",
        "Damit können wir auch die vorhergesagten Werte für verschiedene\n",
        "Temperaturen berechnen:\n",
        "\n",
        "> 0 °C: Besucher = exp (3.50) = 33\n",
        ">\n",
        "> 30 °C: Besucher = exp (3.50 + 30 ∙ 0.11) = exp. (6.83) = 925\n",
        "\n",
        "Wenn wir das Ganze Plotten wollen, benötigen wir den predict- und den\n",
        "lines-Befehl. Wie man, sieht muss auch hier auf die vorhergesagten Werte\n",
        "beim Plotten noch die Umkehrfunktion (exp) angewandt werden:\n",
        "\n",
        "```{.r}\n",
        "xv <- rep(0:40,by=.1)\n",
        "\n",
        "\n",
        "plot(Temperatur,Besucher,xlim=c(0,40))\n",
        "\n",
        "\n",
        "yv <- predict(lm.strand,list(Temperatur=xv))\n",
        "\n",
        "\n",
        "lines(xv, yv,lwd=3,col=\"blue\")\n",
        "\n",
        "\n",
        "yv2 <- predict(glm.poisson,list(Temperatur=xv))\n",
        "\n",
        "\n",
        "lines(xv, exp(yv2),lwd=3,col=\"red\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image75.png){width=\"5.059561461067367in\"\n",
        "height=\"4.042016622922135in\"}\n",
        "\n",
        "### Overdispersion als Problem\n",
        "\n",
        "Mathematisch beschreibt die Poisson-Verteilung Ereignisse pro\n",
        "Zeiteinheit, wenn sie mit einer bestimmten Rate (Mittelwert) erfolgen,\n",
        "die Ereignisse selbst aber unabhängig voneinander sind. Für\n",
        "ökologische/umweltwissenschaftliche Zähldaten sind diese Voraussetzungen\n",
        "oft nicht exakt gegeben, sie folgen daher nicht immer genau einer\n",
        "Poisson-Verteilung, sondern weisen teilweise eine *Overdispersion* auf.\n",
        "Für eine Poisson-Regression wird eine **Dispersion = *Residual deviance*\n",
        "/ *Residual degrees of freedom* = 1** angenommen. Wenn die Dispersion\n",
        "wesentlich/signifkant grösser als 1 ist, liegt *Overdispersion* vor.\n",
        "Residual deviance und Residual degrees of freedom findet man im summary\n",
        "des glm:\n",
        "\n",
        "```{.r}\n",
        "summary(glm.poisson)\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "(Dispersion parameter for poisson family taken to be 1)\n",
        "\n",
        "\n",
        "Null deviance: 6011.8 on 8 degrees of freedom\n",
        "\n",
        "\n",
        "Residual deviance: 1113.7 on 7 degrees of freedom\n",
        "\n",
        "\n",
        "AIC: 1185.1\n",
        "```\n",
        "\n",
        "Man sieht hier, dass der Quotient von 1113.7 und 7 weit höher als 1 ist.\n",
        "Mit dem Dispersionstest im Package AER kann man formal auf einen\n",
        "signifikanten Unterschied testen:\n",
        "\n",
        "```{.r}\n",
        "library(AER)\n",
        "\n",
        "\n",
        "dispersiontest(glm.poisson)\n",
        "\n",
        "\n",
        "data: glm.poisson\n",
        "\n",
        "\n",
        "z = 3.8576, p-value = 5.726e-05\n",
        "\n",
        "\n",
        "alternative hypothesis: true dispersion is greater than 1\n",
        "\n",
        "\n",
        "sample estimates:\n",
        "\n",
        "\n",
        "dispersion\n",
        "\n",
        "\n",
        "116.5467\n",
        "```\n",
        "\n",
        "Wenn man eine signifikante *Overdispersion* gefunden hat, gibt es zwei\n",
        "Lösungsmöglichkeiten:\n",
        "\n",
        "```{.r}\n",
        "1. Quasi-Poisson-Verteilung\n",
        "```. Hierbei schätzt der Algorithmus den\n",
        "Dispersionsparameter aus den Daten und passt die angenommene Verteilung\n",
        "entsprechend an. Die Methode ist im Befehl glm in Base R implementiert:\n",
        "\n",
        "```{.r}\n",
        "glm.quasi <- glm(Besucher\\~Temperatur,family=quasipoisson)\n",
        "\n",
        "\n",
        "summary(glm.quasi)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) 3.50030 0.69639 5.026 0.00152 \\*\\*\n",
        "\n",
        "\n",
        "Temperatur 0.11282 0.02227 5.065 0.00146 \\*\\*\n",
        "\n",
        "\n",
        "\\-\\--\n",
        "\n",
        "\n",
        "Signif. codes: 0 '\\*\\*\\*' 0.001 '\\*\\*' 0.01 '\\*' 0.05 '.' 0.1 ' ' 1\n",
        "\n",
        "\n",
        "(Dispersion parameter for quasipoisson family taken to be 149.6826)\n",
        "```\n",
        "\n",
        "Man sieht, dass im Vergleich zur Berechnung mit einem einfachen\n",
        "Poisson-GLM die Parameterschätzungen nicht verändert haben, jedoch die\n",
        "Signifikanzen niedriger ausgefallen sind (d. h. höhere *p*-Werte).\n",
        "\n",
        "```{.r}\n",
        "2. Negativ-binomiale Verteilung:\n",
        "``` Oftmals erzielt man damit ähnliche,\n",
        "in besonderen Fällen allerdings auch deutlich andere Ergebnisse. Was\n",
        "besser ist, hängt vom Einzelfall ab und ist u. U. recht „tricky\".\n",
        "Weitere Details, siehe Ver Hoef & Boveng (2007).\n",
        "\n",
        "## Logistische Regressionen für Binärdaten\n",
        "\n",
        "Logistische Regressionen werden für alle binären Antwortvariablen\n",
        "verwendet, etwa für Vorkommensdaten (Inzidenzdaten). Das folgende\n",
        "Abbildungspaar zeigt links, was passieren würde, wenn man solche Daten\n",
        "mit einem lm fitten würde und rechts, die korrekte Modellierung mit\n",
        "einem logistischen glm:\n",
        "\n",
        "![](./myMediaFolder/media/image76.jpeg){width=\"6.031201881014873in\"\n",
        "height=\"2.547169728783902in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "### Prinzipielles Vorgehen\n",
        "\n",
        "- Die abhängige Variable muss als Vektor vorliegen, der entweder nur 0\n",
        "    und 1 enthält oder aber ein Faktor mit genau zwei Level ist.\n",
        "\n",
        "- Es wird ein **glm mit family=binomial** gerechnet.\n",
        "\n",
        "- Der voreingestellte **Link ist logit**, alternativ geht auch\n",
        "    log-log.\n",
        "\n",
        "- Overdispersion ist bei Binärdaten nicht relevant.\n",
        "\n",
        "- Wie bei allen (multiplen) Modellen müssen wir eine\n",
        "    **Modellvereinfachung** des vollen Modells vornehmen, wofür im\n",
        "    Prinzip die gleichen drei Methoden zur Verfügung stehen, die wir\n",
        "    schon kennen:\n",
        "\n",
        "    - **Modellselektion I:** sukzessive Vereinfachung durch Entfernen\n",
        "        nicht-signifkanter Terme.\n",
        "\n",
        "    - **Modellselektion II:** sukzessive Vereinfachung mittels\n",
        "        Vergleich der Devianzen zweier Modelle mit Chi-Quadrat-Test\n",
        "        (Achtung: Unterschied zu lm, wo wir eine ANOVA, d. h. eine\n",
        "        F-Test verwendet haben).\n",
        "\n",
        "    - **Modellselektion III:** mittels AICc: Berechnung aller\n",
        "        möglichen Modelle und dann entweder Auswahl jenes mit dem\n",
        "        niedrigsten AICc oder Multimodel inference.\n",
        "\n",
        "### Die Theorie dahinter\n",
        "\n",
        "Das **„logit\" (*L*)** ist ein zentrales Element der logistischen\n",
        "Regression. Ein logit ist als der natürliche Logarithmus eines „odds\"\n",
        "definiert. **„Odds\"** hatten wir schon kurz beim\n",
        "Vierfelder-Assoziationstest (Chi-Quadrat- bzw. Fishers exakter Test).\n",
        "Sie bezeichnen die Wahrscheinlichkeit eines Ereignisses durch die\n",
        "„Gegenwahrscheinlichkeit\". Es gilt also Folgendes:\n",
        "\n",
        "$$L = \\ln\\left( \\frac{p}{1 - p} \\right)$$\n",
        "\n",
        "Warum arbeitet man mit „odds\" und „logits\"? Wenn man nur p modellieren\n",
        "würde, wären die möglichen Werte auf 0 ... 1 begrenzt. „Odds\" dagegen\n",
        "können Werte zwischen 0 und ∞ annehmen. Der Logarithmus schliesslich\n",
        "sorgt für eine symmetrische Verteilung der originalen\n",
        "Wahrscheinlichkeiten unter 50 % (jetzt zwischen --∞ und 0) und der\n",
        "originalen Wahrscheinlichkeiten über 50 % (jetzt zwischen 0 und +∞).\n",
        "\n",
        "Bei GLMs wir ja immer die abhängige Variable mit der Link-Funktion\n",
        "transformiert. Damit modelliert eine logistische Regression das folgende\n",
        "Modell (in einer multiplen logistischen Regression ggf. auch mit *x*~1~,\n",
        "*x*~2~ usw.):\n",
        "\n",
        "$$\\ln\\left( \\frac{\\pi(y)}{1 - \\pi(y)} \\right) = \\ \\beta_{0} + \\beta_{1}x$$\n",
        "\n",
        "### Modelldiagnostik und Ergebnisse\n",
        "\n",
        "Die Beurteilung von Validität und Güte/Relevanz eines logistischen\n",
        "Modells unterscheidet sicher erheblich von einem lm:\n",
        "\n",
        "- Eine visuelle Inspektion der Residualplots ist hier nicht\n",
        "    informativ.\n",
        "\n",
        "- Es gibt diverse numerische ***Goodness-of-fit*-Tests** für das\n",
        "    Modell, am einfachten der Vergleich der Abweichung der Devianz\n",
        "    (*G*²) von der geforderten Χ²-Verteilung.\n",
        "\n",
        "- Das konventionelle Gütemass *R*² funktioniert ebenfalls nicht. Statt\n",
        "    dessen kann man die Modellgüte mit einem **Pseudo-*R*²** ausdrücken:\n",
        "\n",
        "> $R^{2}$= $1 -$ $\\frac{Devianz\\ Total}{Devianz\\ Residuen}$\n",
        "\n",
        "Da nicht die abhängige Variable (d. h. die\n",
        "Auftretenswahrscheinlichkeit), sondern ihr *logit* modelliert wurde,\n",
        "muss man die beiden Parameterschätzungen erst in informative Grössen\n",
        "übersetzen. Es sind dies:\n",
        "\n",
        "- **Lagemass** (d. h. bei welchem *x*~1~-Wert ist die\n",
        "    Wahrscheinlichkeit von 0 und 1 gleich hoch; auch als „LD50\" =\n",
        "    „lethal\" does for 50% of the individuals\" bezeichnet, basierend auf\n",
        "    Anwendungen on logistischen Regressionen in Toxizitätstests):\n",
        "    **--β~0~ / β~0~**\n",
        "\n",
        "- **Steilheitsmass** (d.h. wie scharf/steil ist der Übergang von 0 zu\n",
        "    1, ausgedrückt als die relative Änderung der „odds\" bei Zunahme von\n",
        "    *x*~1~ um eine Einheit): **exp (β~1~)**\n",
        "\n",
        "### Umsetzung in R\n",
        "\n",
        "Schauen wir uns diese ganzen Schritte im Fall unseres Bade-Beispiels an,\n",
        "also der Wahrscheinlichkeit, dass eine Person am Strand schwimmen geht\n",
        "in Abhängigkeit von der Temperatur. Die Definition des Modells in R ist\n",
        "wie gehabt einfach:\n",
        "\n",
        "```{.r}\n",
        "model <- glm(bathing\\~temperature,data=bathing,family=\"binomial\")\n",
        "\n",
        "\n",
        "summary(model)\n",
        "\n",
        "\n",
        "Coefficients\n",
        "\n",
        "\n",
        "Estimate Std. Error z value Pr(\\>\\|z\\|)\n",
        "\n",
        "\n",
        "(Intercept) -5.4652 2.8501 -1.918 0.0552 .\n",
        "\n",
        "\n",
        "temperature 0.2805 0.1350 2.077 0.0378 \\*\n",
        "```\n",
        "\n",
        "Die uns interessierenden Aspekte **Modelldiagnostik, Modellgüte und\n",
        "Kurvenverlauf** müsse wir uns daher erst händisch aus dem\n",
        "abgespeicherten Objekt model extrahieren, indem wir auf einzelne darin\n",
        "abgespeicherte Daten zurückgreifen:\n",
        "\n",
        "```{.r}\n",
        "#Modeldiagnostik (wenn nicht signifikant, dann OK)\n",
        "\n",
        "\n",
        "1 - pchisq (model$deviance,model$df.resid)\n",
        "\n",
        "\n",
        "\\[1\\] 0.6251679\n",
        "\n",
        "\n",
        "#Modellgüte (pseudo-R²)\n",
        "\n",
        "\n",
        "1 - (model$dev / model$null)\n",
        "\n",
        "\n",
        "\\[1\\] 0.4775749\n",
        "```\n",
        "\n",
        "**#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs.\n",
        "x)**\n",
        "\n",
        "```{.r}\n",
        "exp(model$coefficients\\[2\\])\n",
        "\n",
        "\n",
        "temperature\n",
        "\n",
        "\n",
        "1.323807\n",
        "\n",
        "\n",
        "#LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n",
        "\n",
        "\n",
        "-model$coefficients\\[1\\]/model$coefficients\\[2\\]\n",
        "\n",
        "\n",
        "(Intercept)\n",
        "\n",
        "\n",
        "19.48311\n",
        "```\n",
        "\n",
        "Der erste Wert gibt die Steilheit der Beziehung an und ob sie ansteigend\n",
        "oder fallend ist, wobei 1 keinen Effekt, \\>1 eine ansteigende Häufigkeit\n",
        "und < 1 eine fallende Häufigkeit bezeichnen. Der zweite Wert (man\n",
        "beachte das Minus-Zeichen in der Formel!) gibt den *x*-Wert an, für den\n",
        "die berechnete Wahrscheinlichkeit (Vorkommenswahrscheinlichkeit,\n",
        "Sterbewahrscheinlichkeit, usw.) genau 50 % ist.\n",
        "\n",
        "Ganz einfach vorzustellen ist eine logistische Funktion auch mit diesen\n",
        "Werten noch nicht. Deswegen sollten wir im Falle signifkanter\n",
        "logistischer Regressionen immer zwei Dinge tun: (1) Die\n",
        "Funktionsgleichung angeben und (2) Das Ergebnis visualisieren.\n",
        "\n",
        "\\(1\\) Die Funktionsgleichung zu extrahieren, ist etwas vertrackt, da wir\n",
        "ja nicht die Auftretenswahrscheinlichkeit *y*, sondern ihren *logit*\n",
        "modelliert haben. Übersetzt bedeuten die Estimate-Werte unseres summary\n",
        "also:\n",
        "\n",
        "> ln (*y* / (1 -- *y*)) = *b*~0~ + *b*~1~ *x*\n",
        "\n",
        "Wir formen sukzessive um, um nach y aufzulösen:\n",
        "\n",
        "> *y* / (1 -- *y*) = exp (*b*~0~ + *b*~1~ *x*)\n",
        ">\n",
        "> *y* = (exp (*b*~0~ + *b*~1~ *x*) (1 -- y)\n",
        ">\n",
        "> *y* = exp (*b*~0~ + *b*~1~ *x*) -- y exp (*b*~0~ + *b*~1~ *x*)\n",
        ">\n",
        "> *y* + y exp (*b*~0~ + *b*~1~ *x*) = exp (*b*~0~ + *b*~1~ *x*)\n",
        ">\n",
        "> *y* (1 + exp (*b*~0~ + *b*~1~ *x*)) = exp (*b*~0~ + *b*~1~ *x*)\n",
        ">\n",
        "> *y =* exp (*b*~0~ + *b*~1~ *x*) / (1 + exp (*b*~0~ + *b*~1~ *x*))\n",
        "\n",
        "Oder mit den Werten in unserem Fall:\n",
        "\n",
        "> *y =* exp (--5.47 + 0.28 *x*) / (1 + exp (--5.47 + 0.28 *x*))\n",
        "\n",
        "\\(2\\) Das Visualisieren geht relativ einfach mit dem predict-Befehl\n",
        "(hier einschliesslich Standardfehler):\n",
        "\n",
        "```{.r}\n",
        "xs <- seq(0,30,l=1000)\n",
        "```\n",
        "\n",
        "**model.predict\n",
        "<-predict(model,type=\"response\",se=T,newdata=data.frame(temperature=xs))**\n",
        "\n",
        "**plot(bathing\\~temperature,data=bathing,xlab=\"Temperature\n",
        "(°C)\",ylab=„% Bathing\",pch=16, col=\"red\")**\n",
        "\n",
        "```{.r}\n",
        "points(model.predict$fit \\~ xs,type=\"l\")\n",
        "```\n",
        "\n",
        "**lines(model.predict$fit+model.predict$se.fit \\~ xs,\n",
        "type=\"l\",lty=2)**\n",
        "\n",
        "**lines(model.predict$fit-model.predict$se.fit \\~ xs,\n",
        "type=\"l\",lty=2)**\n",
        "\n",
        "![](./myMediaFolder/media/image77.png){width=\"4.139785651793526in\"\n",
        "height=\"3.974983595800525in\"}\n",
        "\n",
        "## Nicht-lineare Regressionen\n",
        "\n",
        "### Beispiele\n",
        "\n",
        "Nicht-lineare Regressionen finden für funktionelle Beziehungen\n",
        "Anwendung, bei der sich die abhängige Grösse nicht als Linearkombination\n",
        "der Prädiktorvariable(n) darstellen lässt, z. B. wenn diese in Potenzen\n",
        "oder Quotienten auftaucht. (Eine polynomiale Regression ist dagegen, wie\n",
        "wir gesehen haben, immer noch ein lineares Modell, wenngleich eine\n",
        "nicht-lineare Beziehung modelliert wird.)\n",
        "\n",
        "Zwei häufige Anwendungen nicht-linearer Regressionen sind die\n",
        "Potenzfunktion und verschiedene Sättigungsfunktionen:\n",
        "\n",
        "```{.r}\n",
        "Beispiel 1: Potenzfunktion\n",
        "```\n",
        "\n",
        "> $\\mathbf{y\\  = \\ }\\mathbf{b}_{\\mathbf{0}}\\mathbf{\\ x}^{\\mathbf{b}_{\\mathbf{1}}}$,\n",
        "> oft auch als $\\mathbf{y\\  = \\ c\\ }\\mathbf{x}^{\\mathbf{z}}$\n",
        "\n",
        "- Dieses dürfte die am häufigeten verwendet nicht-lineare Funktion\n",
        "    sein; sie tritt in fast allen Wissensdisziplinen auf (Nekola & Brown\n",
        "    2007).\n",
        "\n",
        "- *b*~0~ bzw. *c* bezeichnen dabei den vorhergesagten Wert der\n",
        "    abhängigen Variable, wenn die unabhängige den Wert 1 hat (da log (1)\n",
        "    = 0); der Exponent *b*~1~ bzw. *z* beschreibt dagegen die\n",
        "    Geschwindigkeit der relativen Zunahme (*z* = 1 wäre eine lineare\n",
        "    Beziehung).\n",
        "\n",
        "- Solange nicht-lineare Regressionen nicht als einfach verfügbares\n",
        "    statistisches Tool bereitstanden, wurden Potenzgesetze durch\n",
        "    Logarithmierung beider Achsen in eine lineare Beziehung überführt\n",
        "    und mit linearen Modellen analysiert (log *y* = log *b*~0~ + *b*~1~\n",
        "    log *x*).\n",
        "\n",
        "- Das geht gut, solange keine Nullwerte von *y* vorliegen (für die der\n",
        "    Logarithmus nicht definiert wäre).\n",
        "\n",
        "- Man muss aber beachten, dass sich die Ergebnisse unterscheiden, je\n",
        "    nachdem, ob man *y* oder log (*y*) als abhängige Variable hat. Die\n",
        "    beiden Parameterschätzungen sind meist ähnlich, *p*- und *R*²-Werte\n",
        "    können sich dagegen erheblich unterscheiden und sind zwischen beiden\n",
        "    Herangehensweisen nicht vergleichbar. Je nach Situation können aber\n",
        "    beide ihre Berechtigung haben (vgl. Dengler 2009).\n",
        "\n",
        "```{.r}\n",
        "Beispiel 2: Sättigungsfunktionen\n",
        "```\n",
        "\n",
        "- Sogenannte Sättigungsfunktionen finden Anwendung, wenn es nach der\n",
        "    Theorie einen oberen Grenzwert für *y* gibt, dem sich die Funktion\n",
        "    mit zunehmendem *x* asymptotisch annähert.\n",
        "\n",
        "- Eine aus der Enzymkinetik stammende, wegen ihrer Einfachheit aber\n",
        "    auch in diversen anderen Disziplinen angewandte Sättigungsfunktion\n",
        "    ist die Michaelis-Menten-Funktion:\n",
        "\n",
        "$$\\mathbf{y\\  = \\ }\\mathbf{b}_{\\mathbf{0}}\\mathbf{x/(}\\mathbf{b}_{\\mathbf{1}}\\mathbf{+ x)}$$\n",
        "\n",
        "- Hierbei steht *b*~0~ für den oberen Grenzwert, *b*~1~ steht für die\n",
        "    Steilheit des Anstiegs.\n",
        "\n",
        "- Es gibt zahlreiche weitere Sättigungsfunktionen, etwa auch eine\n",
        "    Verallgemeinerung der logistischen Funktion (die wir als eines der\n",
        "    GLM-Modelle kennengelernt haben). Siehe dazu das Unterkapitel\n",
        "    „Umsetzung in R\" unten.\n",
        "\n",
        "### Unterschiede von linearen und nicht-linearen Regressionen\n",
        "\n",
        "**\\\n",
        "Lineare Regression**\n",
        "\n",
        "```{.r}\n",
        "*Y* \\~ β~0~ + β~1~ *X*~1~ + β~2~ *X*~2~ + β~3~ *X*~3~ *+ ...*\n",
        "```\n",
        "\n",
        "*X~i~* kann sein - ein einzelner Prädiktor\\\n",
        "- ein transformierter Prädiktor, z.B. log (*X*), *X*²\\\n",
        "- eine Interaktion *X~j~* × *X~k~*\n",
        "\n",
        "**\\\n",
        "Nicht-lineare Regression**\n",
        "\n",
        "```{.r}\n",
        "*Y* \\~ beliebige Funktion von *X*~1~*, X*~2~*, ...*\n",
        "```\n",
        "\n",
        "„beliebige Funktion\" schliesst ein:\\\n",
        "- Verhältnisse, z. B.: 1/*X*; *X~i~*/*X~j~*\\\n",
        "- Potenzen, z. B. *X^b^*, *b^X^*\\\n",
        "- *breakpoints*, z. B.: for *X* < *b*: ...; for *X* ≥ *b*: ...\n",
        "\n",
        "Bei der Berechnung von linearen vs. nicht-linearen Regressionen gelten\n",
        "folgende Besonderheiten:\n",
        "\n",
        "- **Lineare Regressionen** haben eindeutige Ergebnisse, die **direkt\n",
        "    berechnet** werden können.\n",
        "\n",
        "- Ergebnisse **nicht-linearer Regressionen** sind nicht direkt\n",
        "    analytisch zugängiglich, sondern nur über eine **iterative\n",
        "    Optimierungsprozedur**. Das hat folgende Implikationen:\n",
        "\n",
        "    - Für die Iteratation sind Startwerte und (anfängliche)\n",
        "        Schrittweiten erforderlich\n",
        "\n",
        "    - Man weiss nie sicher, ob man das globale Optimum gefunden hat\n",
        "        (oder in einem lokalen Optimum geendet ist).\n",
        "\n",
        "    - Bei ungünstig gewählten Startwerten konvergiert die Iteration\n",
        "        möglicherweise gar nicht.\n",
        "\n",
        "### Umsetzung in R\n",
        "\n",
        "Der Befehl für nicht-lineare Regressionen ist nls, seine Syntax ganz\n",
        "ähnlich zu lm und glm. Die zu schätzenden Parameter muss man selbst\n",
        "benennen. Da die Lösung iterativ gefunden wird, muss man dem Befehl\n",
        "Startwerte für diese Parameter mitgeben.\n",
        "\n",
        "Man kann beliebige Funktionen selbst definieren, hier gezeigt am\n",
        "Beispiel einer Potenzfunktion:\n",
        "\n",
        "```{.r}\n",
        "#Selbsdefinierte Funktionen#\n",
        "\n",
        "\n",
        "power.model <- nls(ABUND\\~c\\*AREA\\^z, start=(list(c=0,z=1)))\n",
        "\n",
        "\n",
        "summary(power.model)\n",
        "\n",
        "\n",
        "Formula: ABUND \\~ c \\* AREA\\^z\n",
        "\n",
        "\n",
        "Parameters:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "c 13.39416 1.30721 10.246 2.87e-14 \\*\\*\\*\n",
        "\n",
        "\n",
        "z 0.16010 0.02438 6.566 2.09e-08 \\*\\*\\*\n",
        "```\n",
        "\n",
        "Oder man greift auf die in R bereits vordefinierten Funktionen\n",
        "(sogenannte **Selbststartfunktionen** \\[SS\\] zurück). Hier am Beispiel\n",
        "der logistischen Funktion als einer möglichen Sättigungsfunktion gezeigt\n",
        "(Man beachte, dass diese logistische Funktion nicht identisch mit jener\n",
        "aus der logistischen Regression ist, da wir es (a) mit einer\n",
        "nicht-binären Antwortvariable zu tun haben und (b) der Sättigungswert\n",
        "nicht automatisch 1 ist, sondern aus den Daten geschätzt wird). Mehr zu\n",
        "Selbststartfunktionen von nls findet man in der R-Hilfe, im Buch von\n",
        "Ritz & Streibig (2008), sowie dem Auszug daraus, der in Moodle\n",
        "bereitsteht.\n",
        "\n",
        "```{.r}\n",
        "#Vordefinierte \"Selbststartfunktionen\"#\n",
        "\n",
        "\n",
        "logistic.model <- nls(ABUND\\~SSlogis(AREA, Asym, xmid, scal))\n",
        "\n",
        "\n",
        "summary(logistic.model)\n",
        "\n",
        "\n",
        "Formula: ABUND \\~ SSlogis(ABUND, Asym, xmid, scal)\n",
        "\n",
        "\n",
        "Parameters:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "Asym 31.306 2.207 14.182 < 2e-16 \\*\\*\\*\n",
        "\n",
        "\n",
        "xmid 6.501 2.278 2.854 0.00614 \\*\\*\n",
        "\n",
        "\n",
        "scal 9.880 3.152 3.135 0.00280 \\*\\*\n",
        "```\n",
        "\n",
        "Die grösste Herausforderung bei nls sind die **Startwerte**, da bei\n",
        "ungeeigneten Startwerten, das **Modell möglicherweise gar nicht\n",
        "konvegiert oder in einem lokalen Optimum hängen bleibt** und das globale\n",
        "Optimum nicht findet. Hier ist es wichtig, ein gutes Verständnis für die\n",
        "Funktionsparameter der jeweiligen Funktion zu haben und damit eine\n",
        "Erwartungshaltung, wie gross sie im Allgemeinen sind bzw. wie gross sie\n",
        "im konkreten Fall sein könnten. Für's Allgemeine können wir die Theorie\n",
        "und ähnliche Untersuchungen in der Literatur konsultieren. Für den\n",
        "*z*-Wert einer Artenzahl-Areal-Beziehung, die mit Potenzgesetz\n",
        "modelliert wird, sagt uns die Theorie, dass dieser zwischen 0 und 1\n",
        "liegen muss, und empirische Ergebnisse zeigen, dass er meist zwischen\n",
        "0.2 und 0.25 liegt. Wenn wir hier also einen Startwert für *z* von --1\n",
        "oder 1000 eingeben würden, hätte nls vermutlich ein Problem und würde\n",
        "kein Ergebnis oder ein falsches Ergebnis ausspucken. Bei der\n",
        "logistischen Regression wissen wir, dass der Parameter Asym für den\n",
        "Sättigungswert steht. In unserem Fall wäre also die maximale\n",
        "tatsächliche Artenzahl ein brauchbarer Startwert, den wir mit Blick auf\n",
        "die Originaldaten (summary oder Scatterplot) ermitten können.\n",
        "\n",
        "Wenn wir zwischen unterschiedlichen nicht-linearen Modellen auswählen\n",
        "wollen, dann kommen dafür nur die Informationskriterien in Frage, da\n",
        "eine ANOVA hier nicht funktioniert (diese funktioniert nur für\n",
        "geschachtelte Modelle). Wollen wir unsere beiden zuvor berechneten\n",
        "Modelle vergleichen, brauchen wir das Package AICcmodavg:\n",
        "\n",
        "```{.r}\n",
        "library(AICcmodavg)\n",
        "\n",
        "\n",
        "cand.models <- list()\n",
        "\n",
        "\n",
        "cand.models\\[\\[1\\]\\] <- power.model\n",
        "\n",
        "\n",
        "cand.models\\[\\[2\\]\\] <- logistic.model\n",
        "\n",
        "\n",
        "Modnames <- c(\"Power\", \"Logistic\")\n",
        "\n",
        "\n",
        "aictab(cand.set = cand.models, modnames = Modnames)\n",
        "\n",
        "\n",
        "K AICc Delta_AICc AICcWt Cum.Wt LL\n",
        "\n",
        "\n",
        "Logistic 4 386.86 0.00 0.99 0.99 -189.04\n",
        "\n",
        "\n",
        "Power 3 396.17 9.31 0.01 1.00 -194.86\n",
        "```\n",
        "\n",
        "In unserem Fall wäre also das logistische Modell trotz einem\n",
        "zusätzlichen gefitteten Parameter (*k* = 4 statt *k* = 3; hier ist die\n",
        "geschätzte Varianz mitgezählt) das klar bessere Modell (*Akaike Weight*\n",
        "von 0.99).\n",
        "\n",
        "## Glättungsfunktionen und GAMs\n",
        "\n",
        "### Glättungsfunktionen\n",
        "\n",
        "```{.r}\n",
        "Glättungsfunktionen (*smoother*)** sind \n",
        "```keine statistischen\n",
        "Verfahren** im eigentlichen Sinn. Vielmehr dienen sie der Visualisierung\n",
        "eines komplexen Zusammenhanges und können so helfen, geeignete\n",
        "inferenzstatistische Verfahren auszuwählen. Es gibt zahlreiche solche\n",
        "*smoother*:\n",
        "\n",
        "- Gleitender Median\n",
        "\n",
        "- LOESS\n",
        "\n",
        "- LOWESS\n",
        "\n",
        "- Kernel\n",
        "\n",
        "- Splines\n",
        "\n",
        "- \\[...\\]\n",
        "\n",
        "Anhand von **LOWESS (*Locally weighte scatterplot smoothing*)** soll\n",
        "gezeigt werden, was ein smoother macht. In der Regel hat eine\n",
        "Glättungsfunktion zumindest einen wählbaren Parameter, welcher bestimmt,\n",
        "wie stark die Glättung ausfällt, im Fall von LOWESS ist dies f:\n",
        "\n",
        "```{.r}\n",
        "plot(ABUND\\~log_AREA)\n",
        "\n",
        "\n",
        "lines(lowess(log_AREA,ABUND,f=0.25), lwd=2, col=\"red\")\n",
        "\n",
        "\n",
        "lines(lowess(log_AREA,ABUND,f=0.5), lwd=2, col=\"blue\")\n",
        "\n",
        "\n",
        "lines(lowess(log_AREA,ABUND,f=1), lwd=2, col=\"green\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image78.emf.png){width=\"4.396225940507437in\"\n",
        "height=\"3.8867924321959757in\"}\n",
        "\n",
        "### GAMs (Generalized additive models)\n",
        "\n",
        "```{.r}\n",
        "*Generalised additive modesls* (GAMs)\n",
        "``` arbeiten auf den ersten Blick\n",
        "ähnlich wie *Smoother*, doch handelt es sich bei GAMs um ein\n",
        "inferenzstatistisches Verfahren:\n",
        "\n",
        "- Bei einem GAM handelt es sich im Prinzip um ein lineares Modell\n",
        "    (oder ein GLM), bei dem die einzelnen **Parameter nicht fix, sondern\n",
        "    eine *smoothing function*** sind:\\\n",
        "    *y* = *β*~0~ + *f*~1~ (*x*) + *f*~2~ (*x*) + ...\n",
        "\n",
        "- Man bekommt ein Modell mit den üblichen Gütemassen wie *p* oder\n",
        "    AICc.\n",
        "\n",
        "- Die Freiheitsgrade sind geschätzt und nicht ganzzahlig.\n",
        "\n",
        "- Man muss *smoothing function* und *smoothing parameter* definieren.\n",
        "\n",
        "- (Man muss auch Link-Funktion und Wahrscheinlichkeitsverteilung\n",
        "    angeben, wie bei GLMs).\n",
        "\n",
        "In R geht das folgendermassen (für den gleichen Datensatz, über den wir\n",
        "vorhin die *Smoother* haben laufen lassen). Da das Festlegen der\n",
        "smoothing parameter eine Kunst für sich ist, nehmen wir hier die\n",
        "default-Werte des Programms.\n",
        "\n",
        "```{.r}\n",
        "library(mgcv)\n",
        "\n",
        "\n",
        "model <- gam(ABUND\\~s(log_AREA))\n",
        "\n",
        "\n",
        "summary(model)\n",
        "\n",
        "\n",
        "Parametric coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error t value Pr(\\>\\|t\\|)\n",
        "\n",
        "\n",
        "(Intercept) 19.5143 0.9309 20.96 <2e-16 \\*\\*\\*\n",
        "\n",
        "\n",
        "\\-\\--\n",
        "\n",
        "\n",
        "Signif. codes: 0 '\\*\\*\\*' 0.001 '\\*\\*' 0.01 '\\*' 0.05 '.' 0.1 ' ' 1\n",
        "\n",
        "\n",
        "Approximate significance of smooth terms:\n",
        "\n",
        "\n",
        "edf Ref.df F p-value\n",
        "\n",
        "\n",
        "s(log_AREA) 2.884 3.628 21.14 6.63e-11 \\*\\*\\*\n",
        "\n",
        "\n",
        "\\-\\--\n",
        "\n",
        "\n",
        "Signif. codes: 0 '\\*\\*\\*' 0.001 '\\*\\*' 0.01 '\\*' 0.05 '.' 0.1 ' ' 1\n",
        "\n",
        "\n",
        "R-sq.(adj) = 0.579 Deviance explained = 60.1%\n",
        "\n",
        "\n",
        "GCV = 52.145 Scale est. = 48.529 n = 56\n",
        "```\n",
        "\n",
        "Wie wir sehen, bekommen wir für die Beziehung geschätzte Freiheitsgrade\n",
        "und einen geschätzten p-Wert. Der eigentliche Kurvenverlauf wird dagegen\n",
        "nicht in Parametern ausgedrückt und ist nicht direkt zugänglich. Wir\n",
        "können ihn jedoch plotten:\n",
        "\n",
        "```{.r}\n",
        "plot(log_AREA, ABUND, pch=16)\n",
        "\n",
        "\n",
        "xv <- seq(-1, 4, by=0.1)\n",
        "\n",
        "\n",
        "yv <- predict(model, list(log_AREA=xv))\n",
        "\n",
        "\n",
        "lines(xv, yv, lwd=2, col=\"red\")\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image79.emf.png){width=\"4.397637795275591in\"\n",
        "height=\"3.9167836832895886in\"}\n",
        "\n",
        "Zusammenfassend lässt sich sagen, dass GAMs zwar zu den\n",
        "inferenzstatistischen Verfahren gehörten, aber anders als alle anderen\n",
        "derartigen Verfahren, die wir im Kurs kennenlernen kein direkt\n",
        "zugängliches und interpretierbares Modell auspucken. Es ist also kaum\n",
        "möglich, GAMs zwischen verschiedenen Situationen zu vergleichen oder\n",
        "GAMs heranzuziehen, um ein mechanistisches Verständnis der\n",
        "zugrundeliegenden Prozesse zu entwickeln. GAMs sind vor allem dann\n",
        "beliebt, wenn man mutmasslich komplexe Beziehungen mit vielen\n",
        "Prädiktoren hat und es einem nicht um das Modell und seine Parameter an\n",
        "sich geht, sondern um möglichst gute Inter- und Extrapolation auf neue\n",
        "*x*-Werte. Ein beliebtes Feld sind sogenannte *species distribution\n",
        "models* (SDMs), die mit aktuellen Artvorkommens- und Umweltdaten\n",
        "„gefüttert\" werden, um dann vorherzusagen, wie die Artverbreitung sich\n",
        "unter geänderten Umweltbedingungen (*global change*-Szenarien) ändern\n",
        "wird.\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- ***Generalized linear models* (GLMs)** erlauben Regressionen mit\n",
        "    **anderen Varianzstrukturen und Residuenverteilungen** als lineare\n",
        "    Regressionen.\n",
        "\n",
        "- Unter den GLMs sind zwei besonders gebräuchlich: **logistische\n",
        "    Regressionen** werden für **binäre Daten**, **(Quasi-)\n",
        "    Poisson-Regressionen** für **Zähldaten** verwendet.\n",
        "\n",
        "- **Nicht-lineare Regressionen** erlauben die direkte **Modellierung\n",
        "    nicht-linearer und nicht-polynomialer Beziehungen**.\n",
        "\n",
        "- Typische Fälle für nicht-lineare Regressionen sind die\n",
        "    **Potenzfunktion** und verschiedene **«Sättigungsfunktionen»**\n",
        "    (z. B. Michaelis-Menten-Funktion).\n",
        "\n",
        "- **LOWESS** dient der **Visualisierung eines Trends** (explorative\n",
        "    Datenanalyse).\n",
        "\n",
        "- ***Generalized additive models* (GAMs)** können sowohl zum selben\n",
        "    Zweck aber auch zum Aufbauen von **prädiktiven Modellen** verwendet\n",
        "    werden, haben aber anders als typische Regressionstechniken keine\n",
        "    leicht interpretier- und vergleichbare Parameter.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed.\n",
        "John Wiley & Sons, Chichester, UK: 339 pp.**\n",
        "\n",
        "**- Chapter 7: Regression (pp. 142--145 \\[Non-linear regression\\], pp.\n",
        "146--148 \\[GAMs\\])**\n",
        "\n",
        "```{.r}\n",
        "- Chapter 12: Other Response Variables\n",
        "\n",
        "\n",
        "- Chapter 13: Count Data\n",
        "\n",
        "\n",
        "- Chapter 15: Binary Response Variable\n",
        "```\n",
        "\n",
        "Dengler, J. 2009. Which function describes the species-area relationship\n",
        "best? -- A review and empirical evaluation. *Journal of Biogeography*\n",
        "36: 728--744.\n",
        "\n",
        "Dunn, P.K. & Smyth, G.K. 2018. *Generalized linear models with examples\n",
        "in R*. Springer, New York, US: 562 pp.\n",
        "\n",
        "Fox, J. & Weisberg, S. 2019. *An R companion to applied regression*. 3rd\n",
        "ed. SAGE Publications, Thousand Oaks, CA, US: 577 pp.\n",
        "\n",
        "Logan, M. 2010. *Biostatistical design and analysis using R. A practical\n",
        "guide*. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\\\n",
        "- pp. 178-179 (Smoother)\\\n",
        "- pp. 208-253 (Multiple und nicht-lineare Regressionen)\\\n",
        "- pp. 525-530 (GAMs)\\\n",
        "- pp. 483-530 (GLMs)\n",
        "\n",
        "Nekola, J.C. & Brown, J.H. 2007. The wealth of species: ecological\n",
        "communities, complex systems and the legacy of Frank Preston. *Ecology\n",
        "Letters* 10: 188--196.\n",
        "\n",
        "Quinn, P.Q. & Keough, M.J. 2002. *Experimental design and data analysis\n",
        "for biologists*. Cambridge University Press, Cambridge, UK: 537 pp.\n",
        "\n",
        "Ritz, C. & Streibig, J.C. 2008. *Nonlinear regression with R*. Springer,\n",
        "New York, US: 114 pp.\n",
        "\n",
        "Šmilauer, P. 2017. *Modern regression methods. Chapter 2: Generalised\n",
        "linear models for counts and ratios*. Unpublished script, České\n",
        "Budějovice*,* CZ.\n",
        "\n",
        "Ver Hoef, J.M. & Boveng, P.L. 2007. Quasi-Poisson vs. negative binomial\n",
        "regression: how should we model overdispersed count data? *Ecology* 88:\n",
        "2766--2772.\n",
        "\n",
        "# Statistik 5: Von linearen Modellen zu GLMMs\n",
        "\n",
        "**In Statistik 5 lernen die Studierenden Lösungen kennen, welche die\n",
        "diversen Limitierungen von linearen Modellen überwinden. Während\n",
        "*generalized linear models* (GLMs) aus Statistik 4 bekannt sind, geht es\n",
        "jetzt um *linear mixed effect models* (LMMs) und *generalized linear\n",
        "mixed effect models* (GLMMs). Dabei bezeichnet *generalized* die\n",
        "explizite Modellierung anderer Fehler- und Varianzstrukturen und *mixed*\n",
        "die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den\n",
        "Beobachtungen. Einfachere Fälle von LMMs, wie *split-plot* und\n",
        "*repeated-measures* ANOVAs, lassen sich noch mit dem aov-Befehl in Base\n",
        "R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle\n",
        "R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die\n",
        "eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen\n",
        "Abhängigkeiten, erlauben.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *habt verstanden, welche Versuchsdesigns mit einer **normalen\n",
        "    (Typ I) zweifaktoriellen ANOVA** analysiert werden können und welche\n",
        "    die **Spezifikation eines random factors** erfordern;*\n",
        "\n",
        "- *könnt einfache Fälle von **Repeated measures- und Split-plot\n",
        "    ANOVAs** in R spezifizieren und durchführen (mit aov bzw. lme); und*\n",
        "\n",
        "- *wisst, wann man **generalized linear mixed effect models (GLMMs)**\n",
        "    anwenden sollte und wie das im Prinzip geht.*\n",
        "\n",
        "## Split-plot und Repeated-measures ANOVAs\n",
        "\n",
        "### Die Idee\n",
        "\n",
        "Beginnen wir mit einer **konventionellen 2-faktoriellen ANOVA** wie wir\n",
        "sie aus Statistik 2 kennen. Wie in allen linearen Modellen (und ebenso\n",
        "in GLMs) ist eine wesentliche Modellvoraussetzung die Unabhängigkeit der\n",
        "Beobachtungen voneinander. In der folgenden Abbildung ist das für ein\n",
        "experimentelles Setting veranschaulicht, etwa unseren Sortenversuch mit\n",
        "Sorte A und B und den beiden Treatments Freiland und Gewächshaus:\n",
        "\n",
        "![](./myMediaFolder/media/image80.png){width=\"4.3491940069991255in\"\n",
        "height=\"4.3491940069991255in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "Wir sehen, dass alle denkbaren Faktorenkombinationen (hier vier)\n",
        "auftreten (optimalerweise gleich häufig: balanciertes Design), sie aber\n",
        "räumlich zufällig, d. h. voneinander unabhängig angeordnet sind.\n",
        "\n",
        "Im Gegensatz dazu stehen mehrfaktorielle ANOVAs, bei denen **nicht alle\n",
        "Faktorenkombinationen existieren oder es Abhängigkeiten zwischen den\n",
        "Treatments** gibt. Hier gibt es zwei Typen:\n",
        "\n",
        "```{.r}\n",
        "(1) *Split plot*-Design:\n",
        "``` Dies bezeichnet Situationen, bei denen die\n",
        "Kombinationen der beiden Faktoren nicht unabhängig voneinander räumlich\n",
        "verteilt sind, etwa weil dies mit zu grossem Aufwand verbunden wäre.\n",
        "Stellen wir etwa das Beispiel mit dem Gewächshaus-Freiland-Versuch von\n",
        "oben vor: Schon für die extrem geringe Replizierung von nur drei\n",
        "Wiederholungen pro Faktorenkombination müsste man sechs Gewächshäuser\n",
        "haben, jedes entweder mit Sorte A oder mit Sorte B, die man zudem\n",
        "räumlich zufällig platzieren kann. Logischerweise geht das oftmals\n",
        "nicht. Stattdessen könnte man drei Gewächshäuser haben, in denen man\n",
        "jeweils beide Sorten pflanzt. Dann wäre das Gewächshaus bzw. das\n",
        "entsprechende Freilandbeet der „*plot*\", der dann zwischen den beiden\n",
        "Sorten aufgeteilt (*split*) wird. Damit ist aber die\n",
        "Unabhängigkeitsannahme linearer Modelle verletzt, da sich ja die\n",
        "Gewächshäuser unterscheiden könnten, etwa in ihrer Thermoregulation,\n",
        "ihrer Lichtdurchlässigkeit oder ihrer Beschattung durch umstehende Bäume\n",
        "oder Gebäude. Deshalb hat potenziell die Frage, in welche Gewächshaus\n",
        "die Pflanzen standen, auch einen Einfluss auf das Ergebnis, muss mithin\n",
        "im statistischen Modell berücksichtigt werden\n",
        "\n",
        "![](./myMediaFolder/media/image81.png){width=\"3.7401574803149606in\"\n",
        "height=\"3.7992125984251968in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "```{.r}\n",
        "(2) *Repeated measures*-Design:\n",
        "``` Hier geht es nicht um eine räumliche\n",
        "Bindung (enges Nebeneinander), sondern um eine zeitliche Bindung\n",
        "(zeitliches Nacheinander). Das heisst, an bestimmten\n",
        "Untersuchungsobjekten (Personen, Pflanzenindividuen,\n",
        "Untersuchungsflächen) wird zu verschiedenen Zeitpunkten eine\n",
        "Untersuchung vorgenommen, wie die folgende Abbildung es veranschaulicht:\n",
        "\n",
        "![](./myMediaFolder/media/image82.png){width=\"3.7401574803149606in\"\n",
        "height=\"3.7992125984251968in\"}\\\n",
        "(aus Logan 2010)\n",
        "\n",
        "Während *split plot*-Design und *repeated measures*-Design auf den\n",
        "ersten Blick wie etwas Verschiedenes aussehen, so sind sie statistisch\n",
        "doch äquivalent.\n",
        "\n",
        "![](./myMediaFolder/media/image5.png){width=\"0.3087696850393701in\"\n",
        "height=\"0.30561898512685914in\"}\n",
        "\n",
        "### Ein Beispiel\n",
        "\n",
        "```{.r}\n",
        "Fragestellung:\n",
        "``` Uns interessiert die Reaktionszeit von Personen auf\n",
        "Signale in Abhängigkeit von der Art der Signale (akustisch, visuell).\n",
        "\n",
        "```{.r}\n",
        "Versuchsanordnung:\n",
        "```\n",
        "\n",
        "- 8 Versuchspersonen (VP1--VP8)\n",
        "\n",
        "- Je 4 davon zufällig den beiden Signaltypen (akustisch, visuell)\n",
        "    zugeordnet\n",
        "\n",
        "- Messung der Reaktionszeit nach 1, 2, 3 und 4 h (H1--H4)\n",
        "\n",
        "**Wir haben hier drei wesentliche Abweichungen von einer normalen Typ\n",
        "I-ANOVA:**\n",
        "\n",
        "- Wir sind nicht am spezifischen Verhalten der Versuchspersonen\n",
        "    VP1--VP8 interessiert, sondern haben sie „zufällig\" ausgewählt um\n",
        "    alle möglichen Personen zu repräsentieren.\n",
        "\n",
        "- Jede Versuchsperson bekommt nur ein „Treatment\", d. h. es gibt nicht\n",
        "    alle VP × Signal-Kombinationen.\n",
        "\n",
        "- Die vier gemessenen Reaktionszeiten einer Person sind nicht\n",
        "    unabhängig voneinander: So könnten bestimmte Personen vielleicht\n",
        "    immer etwas schneller oder langsamer sein als andere.\n",
        "\n",
        "### Umsetzung in R\n",
        "\n",
        "In unserem Fall ist also der Block-Faktor die Versuchperson (VP),\n",
        "einerseits, da jede Person nur einem der beiden Signaltypen ausgesetzt\n",
        "wurde, andererseits, weil wir mehrere Messungen über die Zeit mit ihr\n",
        "durchgeführt haben. Im aov-Befehl lässt sich das mit dem Error-Term\n",
        "spezifizieren:\n",
        "\n",
        "```{.r}\n",
        "spf.aov <- aov(Reaktion\\~Signal\\*Messung + Error(VP), data = spf))\n",
        "\n",
        "\n",
        "summary(spf.aov)\n",
        "\n",
        "\n",
        "Error: VP\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "Signal 1 3.125 3.125 2 0.207\n",
        "\n",
        "\n",
        "Residuals 6 9.375 1.562\n",
        "\n",
        "\n",
        "Error: Within\n",
        "\n",
        "\n",
        "Df Sum Sq Mean Sq F value Pr(\\>F)\n",
        "\n",
        "\n",
        "Messung 3 194.50 64.83 127.89 2.52e-12 \\*\\*\\*\n",
        "\n",
        "\n",
        "Signal:Messung3 19.37 6.46 12.74 0.000105 \\*\\*\\*\n",
        "\n",
        "\n",
        "Residuals 18 9.13 0.51\n",
        "```\n",
        "\n",
        "Im Ergebnis erhalten wir eine zweigeteilte ANOVA-Tabelle: Der obere Teil\n",
        "sagt uns, dass der Effekt von Signal (Art des Signals), der in den\n",
        "Personen (VP) geblockt ist, nicht signifikant (*p* = 0.207) ist. Der\n",
        "untere Teil sagt uns, dass es einen signifikanten Effekt der Zeit sowie\n",
        "eine signifikante Interaktion Signaltyp × Zeit gibt. Ein\n",
        "Interkationsplot zeigt uns genau dieses:\n",
        "\n",
        "```{.r}\n",
        "interaction.plot(spf$Messung, spf$Signal, spf$Reaktion)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image83.png){width=\"5.626570428696413in\"\n",
        "height=\"3.858267716535433in\"}\n",
        "\n",
        "Der Plot macht klar, dass sich die Reaktionsheiten zwischen akustisch\n",
        "und optisch im Mittel nicht unterscheiden, sie aber im Fall von A2\n",
        "schneller ansteigen als im Fall von A1\n",
        "\n",
        "Mit dem Error-Term kann man auch mehrfache Schachtelungen codieren,\n",
        "jeweils links beginnend mit der obersten Ebene der Schachtelung:\n",
        "\n",
        "**model2 <- aov (Y \\~ A \\* B \\* C + Error (Block/A/B), data =\n",
        "beispiel)**\n",
        "\n",
        "## Linear mixed effect models (LMMs)\n",
        "\n",
        "### Die Idee\n",
        "\n",
        "*Linear mixed effect models* (LMMs) verallgemeinern LMs, um Folgendes\n",
        "modellieren zu können:\n",
        "\n",
        "- Abhängigkeiten/Schachtelungen zwischen Faktoren (um der Verletzung\n",
        "    der LM-Voraussetzungen Rechnung zu tragen).\n",
        "\n",
        "- Faktoren, die uns nicht interessieren. Diese werden als sogenannte\n",
        "    random factors modelliert, damit „sparen\" wir Freiheitsgrade und\n",
        "    gewinnen Teststärke für die uns interssierenden Faktoren.\n",
        "\n",
        "Die einfachsten LMMs, d. h. *Repeated measures*- und *Split plot*-ANOVA\n",
        "gehen (mit Limitierungen) noch mit dem aov-Befehl. Für komplexere\n",
        "Situationen bzw. im allgemeinen Fall (einschliesslich Regressionen und\n",
        "ANCOVAs) benötigt man dagegen lme aus dem Package nlme.\n",
        "\n",
        "Analog zum Error-Term in aov spezifiziert man hier einen random-Term,\n",
        "wobei es zusätzlich die Möglichkeit gibt, zu entscheiden, ob man nur\n",
        "einen zufälligen Achsenabschnitt (*random intercept*) oder auch eine\n",
        "zufällige Steigung (*random slope*) modellieren möchte:\n",
        "\n",
        "### Umsetzung in R\n",
        "\n",
        "```{.r}\n",
        "library(nlme)\n",
        "```\n",
        "\n",
        "**spf.lme.1 <- lme(Reaktion\\~Signal\\*Messung, random = \\~Messung \\| VP,\n",
        "data = spf) #mit random intercept (VP) und random slope (Messung)**\n",
        "\n",
        "**spf.lme.2 <- lme(Reaktion\\~Signal\\*Messung, random = \\~1 \\| VP, data\n",
        "= spf) #nur random intercept**\n",
        "\n",
        "```{.r}\n",
        "anova(spf.lme.1)\n",
        "\n",
        "\n",
        "numDF denDF F-value p-value\n",
        "\n",
        "\n",
        "(Intercept) 1 18 1488.1631 <.0001\n",
        "\n",
        "\n",
        "Signal 1 6 2.0808 0.1993\n",
        "\n",
        "\n",
        "Messung 3 18 70.7887 <.0001\n",
        "\n",
        "\n",
        "Signal:Messung 3 18 11.8592 0.0002\n",
        "\n",
        "\n",
        "anova(spf.lme.2)\n",
        "\n",
        "\n",
        "numDF denDF F-value p-value\n",
        "\n",
        "\n",
        "(Intercept) 1 18 591.6800 <.0001\n",
        "\n",
        "\n",
        "Signal 1 6 2.0000 0.2070\n",
        "\n",
        "\n",
        "Messung 3 18 127.8904 <.0001\n",
        "\n",
        "\n",
        "Signal:Messung 3 18 12.7397 0.0001\n",
        "```\n",
        "\n",
        "LMMs, ihr korrekte Implementierung und Interpretation können u. U. sehr\n",
        "komplex sein, weswegen wir sie in unserem Kurs nicht mit viel Details\n",
        "besprechen können. Wer weitergehende benutzerfreundliche Informationen\n",
        "sucht, sei insbesondere auf Logan (2010: pp. 360--447) verwiesen.\n",
        "\n",
        "## Generalized linear mixed effect models (GLMMs)\n",
        "\n",
        "### Die Idee\n",
        "\n",
        "*Generlized linear mixed effect models* (GLMMs) verallgemeinern GLMs, um\n",
        "Folgendes modellieren zu können:\n",
        "\n",
        "- Geschachtelte Daten\n",
        "\n",
        "- Zeitliche Korrelationen zwische Beobachtungen\n",
        "\n",
        "- Räumliche Korrelationen zwischen Beobachtungen\n",
        "\n",
        "- Heterogenität\n",
        "\n",
        "- Messwiederholungen\n",
        "\n",
        "Während dies alles wundervolle und oft benötigte Eigenschaften sind,\n",
        "sollte man sich auch der Nachteile/Limitierungen bewusst sein, wie die\n",
        "folgenden Zitate aus einem der führenden Lehrbücher zu GLMMs (Zuur et\n",
        "al. 2009) zeigen:\n",
        "\n",
        "- *„GLMM are at the frontier of statistical research\"*\n",
        "\n",
        "- *„This means that available documentation is rather technical and\n",
        "    there are only few, if any, textbooks aimed at ecologists\"*\n",
        "\n",
        "- *„There are multiple approaches for obtaining estimated parameters\"*\n",
        "\n",
        "- *„There are at least four packages in R that can be used for GLMM\"*\n",
        "\n",
        "- *„This makes model selection in GLMM more of an art than a science\"*\n",
        "\n",
        "Bezüglich der Anwendung von GLMMs, kommen Zuur et al. (2009) daher zu\n",
        "folgendem Schluss (der natürlich auch sonst in der Statistik gilt, hier\n",
        "aber besonders wichtig ist): ***When applying GLMM, try to keep the\n",
        "models simple or you may get numerical estimation problems***.\n",
        "\n",
        "### Ein Beispiel und seine Umsetzung in R\n",
        "\n",
        "Befall von Rothirschen (*Cervus elaphus*) in spanischen Farmen mit dem\n",
        "Parasiten *Elaphostrongylus cervi*. Modelliert wird\n",
        "Vorkommen/Nichtvorkommen von L1-Larven dieser Nematode in Abhängigkeit\n",
        "von Körperlänge und Geschlecht der Hirsche. Erhoben wurden die Daten auf\n",
        "24 Farmen.\n",
        "\n",
        "Wir können das Ganze wie bisher mit einem binomialen GLM analysieren:\n",
        "\n",
        "```{.r}\n",
        "DE.glm <- glm(Ecervi.01 \\~ CLength \\* fSex+fFarm,\n",
        "\n",
        "\n",
        "family = binomial, data = DeerEcervi)\n",
        "\n",
        "\n",
        "summary(DE.glm)\n",
        "\n",
        "\n",
        "Coefficients:\n",
        "\n",
        "\n",
        "Estimate Std. Error z value Pr(\\>\\|z\\|)\n",
        "\n",
        "\n",
        "(Intercept) -1.796e+00 5.900e-01 -3.044 0.002336 \\*\\*\n",
        "\n",
        "\n",
        "CLength 4.062e-02 7.132e-03 5.695 1.24e-08 \\*\\*\\*\n",
        "\n",
        "\n",
        "fSex2 6.280e-01 2.292e-01 2.740 0.006150 \\*\\*\n",
        "\n",
        "\n",
        "fFarmAU 3.340e+00 7.841e-01 4.259 2.05e-05 \\*\\*\\*\n",
        "\n",
        "\n",
        "fFarmBA 3.510e+00 7.150e-01 4.908 9.19e-07 \\*\\*\\*\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "fFarmVY 3.974e+00 1.257e+00 3.162 0.001565 \\*\\*\n",
        "\n",
        "\n",
        "CLength:fSex2 3.618e-02 1.168e-02 3.097 0.001953 \\*\\*\n",
        "```\n",
        "\n",
        "Das Modell, das wir erzeugt haben, liesse sich folgendermassen\n",
        "visualisieren:\n",
        "\n",
        "![](./myMediaFolder/media/image84.emf.png){width=\"6.380456036745406in\"\n",
        "height=\"3.7708858267716536in\"}\\\n",
        "(aus Zuur et al. 2009)\n",
        "\n",
        "Für unseren Zweck hat die Lösung mit einem GLM zwei Nachteile:\n",
        "\n",
        "- fFarm „verbraucht\" 23 Freiheitsgrade, obwohl wir nicht am Farmeffekt\n",
        "    interessiert sind.\n",
        "\n",
        "- Wir bekommen ein Modell für jede einzelne Farm, aber kein\n",
        "    farmunabhängiges Modell.\n",
        "\n",
        "Beispiehaft analysieren wir dieses GLMM mit glmm.PQL aus dem Package\n",
        "MASS:\n",
        "\n",
        "```{.r}\n",
        "library(MASS)\n",
        "\n",
        "\n",
        "DE.PQL <- glmmPQL(Ecervi.01 \\~ CLength \\* fSex,\n",
        "\n",
        "\n",
        "random = \\~ 1 \\| fFarm, family = binomial, data = DeerEcervi)\n",
        "\n",
        "\n",
        "summary(DE.PQL)\n",
        "\n",
        "\n",
        "Random effects:\n",
        "\n",
        "\n",
        "Formula: \\~1 \\| fFarm\n",
        "\n",
        "\n",
        "(Intercept) Residual\n",
        "\n",
        "\n",
        "StdDev: 1.462108 0.9620576\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "Fixed effects: Ecervi.01 \\~ CLength \\* fSex\n",
        "\n",
        "\n",
        "Value Std.Error DF t-value p-value\n",
        "\n",
        "\n",
        "(Intercept) 0.8883697 0.3373283 799 2.633547 0.0086\n",
        "\n",
        "\n",
        "CLength 0.0378608 0.0065269 799 5.800768 0.0000\n",
        "\n",
        "\n",
        "fSex2 0.6104570 0.2137293 799 2.856216 0.0044\n",
        "\n",
        "\n",
        "CLength:fSex2 0.0350666 0.0108558 799 3.230228 0.0013\n",
        "```\n",
        "\n",
        "Wie wir das schon von ANOVAs mit Error-Term oder LMMs kennen, ist die\n",
        "Ergebnistabelle in einen Teil für die Random effects und einen Teil für\n",
        "die Fixed effects aufgeteilt. Für fFarm gibt es jetzt aber anders als\n",
        "beim GLM nicht 23 Schätzwerte, sondern nur einen für die\n",
        "Standardabweichung. Der untere Teil entspricht dagegen dem Output eines\n",
        "GLMs, wenn wir fFarm völlig ignoriert hätten: wir haben die Effekte von\n",
        "Grösse, Geschlecht und deren Interaktion (alle signifikant).\n",
        "\n",
        "```{.r}\n",
        "Was sagen uns die Ergebnisse nun?\n",
        "```\n",
        "\n",
        "- Wahrscheinlichkeit des Parasitenbefalls für weibliche Hirsche:\n",
        "\n",
        "> logit (*p~ij~*) = 0.888 + 0.037 ∙ Length*~ij~*\n",
        "\n",
        "- Wahrscheinlichkeit des Parasitenbefalls für männliche Hirsche:\n",
        "\n",
        "> logit (*p~ij~*) = (0.888 + 0.610) + (0.037 + 0.035) ∙ Length*~ij~*\n",
        ">\n",
        "> logit (*p~ij~*) = 1.498 + 0.072 ∙ Length*~ij~*\n",
        "\n",
        "Da die Codierung Sex2 = „männlich\" war und wir sowohl ein „random\n",
        "intercept\" als auch ein „random slope\" modelliert haben, ergibt sich der\n",
        "Achsenabschnitt für die männlichen Hirsche durch die Addition des\n",
        "allgemeinen Achsenabschnitts (der sich auf Sex1 = „weiblich\" bezieht)\n",
        "und dem Effekt von Sex2, während sich die Steigung für die männlichen\n",
        "Hirsche aus jener für die weiblichen + den Interkationsterm ergibt.\n",
        "\n",
        "Da wir es mit einem Binomial-GLMM zu tun haben, sagen uns die gefundenen\n",
        "Gleichungen immer noch nicht unmittelbar etwas über die Beziehungen, da\n",
        "auf der linken Seite der Gleichung jeweils logit (*p~ij~*) und nicht\n",
        "*p~ij~* steht. Wir könnten wie in Statistik 4 nach *p~ij~* auflösen oder\n",
        "wir nutzen eine Visualisierung. Im Folgenden ist z. B. die\n",
        "GLMM-Vorhersage für weibliche Hirsche mit Konfidenzintervall geplottet,\n",
        "was schön den Unterschied zum GLM zeigt:\n",
        "\n",
        "![](./myMediaFolder/media/image85.png){width=\"6.323562992125984in\"\n",
        "height=\"3.3597462817147856in\"}\\\n",
        "(aus Zuur et al. 2009)\n",
        "\n",
        "### Verschiedene R-packages für GLMMs\n",
        "\n",
        "Es gibt mehrere R-packages für GLMMs, von denen die folgenden die\n",
        "gängisten sind:\n",
        "\n",
        "- **library(MASS): glmmPQL**\n",
        "\n",
        "- **library(lme4): glmer**\n",
        "\n",
        "- **library(glmmML): glmmML**\n",
        "\n",
        "Die Syntax der verschiedenen Packages unterscheidet sich im Detail,\n",
        "bitte bei Bedarf die jeweilige Hilfe-Funktion konsultieren.\n",
        "\n",
        "Da ein GLMM ein sehr komplexes Verfahren ist, sind die verschiedenen\n",
        "Implementierungen nicht genau gleich. Insofern kann es auch leichte\n",
        "Divergenzen in den Parameterschätzungen und den Parametern geben, wie\n",
        "die folgende Auswertung für unser Hirschbeispiel zeigt:\n",
        "\n",
        "![](./myMediaFolder/media/image86.emf.png){width=\"6.367928696412949in\"\n",
        "height=\"3.1142333770778654in\"}\\\n",
        "(aus Zuur et al. 2009)\n",
        "\n",
        "In diesem Fall (und meist) sind die Abweichungen zwischen den drei GLMMs\n",
        "aber gering. Dagegen ist die Aussage deutlich verschieden von der mit\n",
        "dem GLM ermittelten (massiv andere Parameterschätzung für Sex, etwas\n",
        "andere für Length und Length × Sex).\n",
        "\n",
        "### Random vs. fixed factors\n",
        "\n",
        "Wann sollten wir *random factors* nehmen, wann *fixed factors*? Im\n",
        "Hirsch-Beispiel ist statistisch klar, dass wir die Farm-Identität in\n",
        "unser statistisches Modell aufnehmen müssen, da auf jeder Farm mehrere\n",
        "Hirsche untersucht wurden und unser wissen über as universelle Phänomen\n",
        "der **räumlichen Autokorrelation** es höchstwahrscheinlich macht, dass\n",
        "sich die Hirsche einer einzelnen Farm (wg. räumlicher Nähe) ähnlicher\n",
        "verhalten als zufällig herausgegriffene Paare von Farm-Hirschen aus ganz\n",
        "Spanien.\n",
        "\n",
        "Ob wir die Farm-Identität dagegen als *fixed factor* aufnehmen (d. h.\n",
        "ein GLM rechnen) oder als *random factor* (d. h. ein GLMM rechnen),\n",
        "hängt von unserer Frage ab. In der Beschreibung der Studie wurde\n",
        "suggeriert, dass es uns um ein allgemeines farmunabhängiges Modell ging,\n",
        "wie sich der Parasitenbefall in Abhängigkeit von Geschlecht und Grösse\n",
        "entwickelt. Dann wäre unser Vorgehen richtig, fFarm als *random factor*\n",
        "zu definieren. Wir dürfen und können dann aber keine Aussage über eine\n",
        "einzelne Farm treffen. Wenn uns dagegen interessiert, ob und wie sich\n",
        "die Farmen bezüglich Parasitenbefall unterscheiden, etwa weil sie\n",
        "unterschiedliche Hygienekonzepte oder Populationsdichten haben, dann\n",
        "müssen wir fFarm als *fixed factor* einführen (also ein GLM rechnen). Ob\n",
        "wir in einer solchen Situation ein GLM oder ein GLMM rechnen, hängt also\n",
        "von unserer genauen Frage ab.\n",
        "\n",
        "## LMs, GLMs, LMMs und GLMMs im Rückblick und Überblick\n",
        "\n",
        "Zum Abschluss der fünf inferenzstatistischen Lektionen seien noch einmal\n",
        "die grundlegenden Ähnlichkeiten und Unterschiede von LMs, GLMs, LMMs und\n",
        "GLMMs zusammengefasst:\n",
        "\n",
        "```{.r}\n",
        "LMs: *Linear models*\n",
        "\n",
        "\n",
        "GLMs: *Generalized linear models*\n",
        "\n",
        "\n",
        "LMMs: *Linear mixed effect models*\n",
        "\n",
        "\n",
        "GLMMs: *Generalized linear mixed effects models*\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image87.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"3.5905435258092737in\"}\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- Wenn in einem ANOVA-Design **Schachtelungen oder Abhängigkeiten**\n",
        "    vorliegen, muss man diese im Modell spezifizieren, was entweder als\n",
        "    *Error* in *aov* oder als *random* in *lme* (package *nlme*) geht.\n",
        "\n",
        "- Während GLMs lineare Modelle bezüglich der geforderten Residuen- und\n",
        "    Varianzstruktur verallgemeinern, leisten ***linear mixed effect\n",
        "    models* (LMMs)** dies bezüglich unterschiedlichster Abhängigkeiten\n",
        "    zwischen Beobachtungen.\n",
        "\n",
        "- ***Generalized linear mixed effect models* (GLMMs)** schliesslich\n",
        "    ermöglichen, beide Typen von Abweichungen von den Voraussetzungen\n",
        "    linearer Modelle zu berücksichtigen.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Crawley, M.J. 2015. *Statistics -- An introduction using R*. 2nd ed.\n",
        "John Wiley & Sons, Chichester, UK: 339 pp.**\n",
        "\n",
        "```{.r}\n",
        "- Chapter 8: Analysis of Variance (pp. 173--182)\n",
        "```\n",
        "\n",
        "Logan, M. 2010. *Biostatistical design and analysis using R. A practical\n",
        "guide*. Wiley-Blackwell, Oxford, UK: 546 pp., v.a.\\\n",
        "- pp. 399-447 (split-plot und repeated measures ANOVAs)\n",
        "\n",
        "Zuur, A. E., Ieno, E. N., Walker, N. J., Saveliev, A. A., Smith, G. M.\n",
        "(eds.) 2009. *Mixed effects models and extension in ecology with R*.\n",
        "Springer, New York: 576 pp.\n",
        "\n",
        "Zuur, A.E., Hilbe, J.M. & Ieno, E.N. 2013. *A beginner's guide to GLM\n",
        "and GLMM with R -- A frequentist and Bayesian perspective for\n",
        "ecologists*. Highland Statistics, Newburgh: 253 pp.\n",
        "\n",
        "# Statistik 6: Einführung in „multivariate\" Methoden und Ordinationen I\n",
        "\n",
        "**Statistik 6 führt in multivariat-deskriptive Methoden ein, die dazu\n",
        "dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen\n",
        "Variablen effektiv zu analysieren. Dabei betonen Ordinationen\n",
        "kontinuierliche Gradienten und fokussieren auf zusammengehörende\n",
        "Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf\n",
        "zusammengehörende Beobachtungen fokussieren. Es folgt eine\n",
        "konzeptionelle Einführung in die Idee von Ordinationen als einer Technik\n",
        "der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen\n",
        "via Dimensionsreduktion visualisiert. Das Prinzip und die praktische\n",
        "Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse\n",
        "(PCA) erklärt. Danach folgen kurze Einführungen in weitere\n",
        "Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen\n",
        "der PCA überwinden, namentlich CA, DCA und NMDS.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *versteht, **was Ordinationen sollen**, was sie leisten können und\n",
        "    was nicht;*\n",
        "\n",
        "- *könnt das **Prinzip einer PCA** beschreiben, sie implementieren,\n",
        "    und ihren Ergebnisoutput interpretieren;*\n",
        "\n",
        "- *Die Annahmen einer PCA kennt, und wisst welche «Artefakte» bei\n",
        "    einer Verletzung herauskommen; und*\n",
        "\n",
        "- *habt das Vorgehen im Prinzip verstanden, wie **DCA und NMDS** diese\n",
        "    Probleme angehen.*\n",
        "\n",
        "## Einführung in „multivariate\" Methoden\n",
        "\n",
        "### Was ist mit „multivariat\" gemeint?\n",
        "\n",
        "Was ist mit **„multivariat\"** gemeint? Zunächst einmal sagt das nur,\n",
        "dass pro Beobachtung (*observation*) **mehr als zwei** Variablen erhoben\n",
        "werden, deren Beziehungen zueinander analysiert werden. Im Wortsinne\n",
        "waren also auch schon die zweifaktorielle ANOVA und die multiple\n",
        "Regression „multivariate\" Methoden.\n",
        "\n",
        "Die folgende Tabelle fasst die schon besprochenen und noch kommenden\n",
        "statistischen Verfahren bezüglich der Anzahl von Prädiktor- und\n",
        "Antwortvariablen zusammen:\n",
        "\n",
        "![](./myMediaFolder/media/image88.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"2.148604549431321in\"}\n",
        "\n",
        "In der Literatur wird der Begriff **„multivariat\"** jedoch oft nur für\n",
        "die letzte Gruppe von Verfahren, also **Ordinationen und\n",
        "Cluster-Analysen**, gebraucht. Diese bilden den Gegenstand von Statistik\n",
        "6--8.\n",
        "\n",
        "### Inferenzstatistik vs. deskriptive Statistik\n",
        "\n",
        "```{.r}\n",
        "Bislang\n",
        "``` haben wir statistische Verfahren überwiegend zum Testen von\n",
        "Hypothesen verwendet (inklusive des impliziten Hypothesentestens, wenn\n",
        "man eine offene Forschungsfrage beantwortet): **Inferenzstatistik\n",
        "(schliessende Statistik)**.\n",
        "\n",
        "```{.r}\n",
        "Ordinationen und Cluster-Analysen** sind überwiegend \n",
        "```deskriptive\n",
        "Statistik** (ohne spezielle Zusatzschritte erlauben sie kein Testen von\n",
        "Hypothesen!).\n",
        "\n",
        "### Beispiele multivariater Datensätze\n",
        "\n",
        "Multivariate Datensätze sind in unserer „datenreichen\" Welt\n",
        "allgegenwärtig z. B.:\n",
        "\n",
        "- **Bodenproben**, an denen viele unterschiedliche physikalische und\n",
        "    chemische Variablen, ggf. auch noch in verschiedenen Horizonten\n",
        "    gemessen wurden.\n",
        "\n",
        "- **Klimadaten** von Messstationen: zahlreiche Variablen wie\n",
        "    Mittel/Minima/Maxima von\n",
        "    Temperatur/Niederschlag/Sonnenschein/Bewölkung/Windstärke usw. und\n",
        "    das für jeden Monat.\n",
        "\n",
        "- Zusammensetzungen von lokalen **Pflanzengesellschaften oder\n",
        "    Tiergemeinschaften**: hier sind die Deckungen bzw. Individuenzahlen\n",
        "    der einzelnen Arten die Variablen\n",
        "\n",
        "- Ergebnisse von **Befragungen von Konsumenten**: viele Variablen zu\n",
        "    Präferenzen, Einstellungen usw.\n",
        "\n",
        "### Ziele multivariat-deskriptiver Analysen\n",
        "\n",
        "Im Prinzip können wir auch bei solchen Beobachtungsdaten mit vielen\n",
        "abhängigen Variablen wie bisher jede einzeln testen:\n",
        "\n",
        "- Das kann **vorteilhaft** sein, **wenn man konkrete Hypothesen testen\n",
        "    will** (was ja mit multivariat-deskriptiven Methoden normalerweise\n",
        "    nicht geht).\n",
        "\n",
        "- Ein Problem sind die vielen Tests mit dem gleichen Datensatz, die zu\n",
        "    einer **„Inflation\" der Typ I-Fehlerrate** führen (wenn ich 20 Tests\n",
        "    durchführe, würde ja bei α = 0.05 einer rein zufällig eine\n",
        "    Signifikanz anzeigen, selbst wenn eigentlich für keinen einen\n",
        "    Beziehung besteht). Für dieses Problem gibt es aber\n",
        "    Korrekturmöglichkeiten (z. B. „Bonferroni\"-Korrektur).\n",
        "\n",
        "- Problematischer ist, dass es sehr **schwierig** ist, **aus den\n",
        "    vielen Einzelergebnissen am Ende ein aussagekräftiges Gesamtbild zu\n",
        "    synthetisieren**.\n",
        "\n",
        "Hier setzen die multivariat-deskriptiven Methoden mit ihren beiden\n",
        "Hauptzielen an:\n",
        "\n",
        "- **Muster und Beziehungen** im *n*-dimensionalen Hyperraum erkennen\n",
        "    und beschreiben.\n",
        "\n",
        "- **Dimensionsreduktion**: die wesentliche Information aus den n\n",
        "    Dimensionen wird auf 2 bis wenige Dimensionen reduziert, die\n",
        "    vorstellbar und visualisierbar sind.\n",
        "\n",
        "Der ***n*-dimensionale Hyperraum** ist das Konzept, das uns durchgängig\n",
        "bei den multivariat-deskriptiven Methoden begleitet. Dahinter verbirgt\n",
        "sich die Idee, dass jede der *n* Variablen eine orthogonale Achse ist,\n",
        "auf der die Ausprägungen der Variablen (metrisch oder kategorial)\n",
        "aufgetragen sind. Während wir uns einen 3-dimensionalen Raum noch\n",
        "vorstellen können, ist es mit der Vorstellungskraft bei vier oder gar\n",
        "100 Dimensionen schnell zu Ende. Aber das ist ja genau der Grund für die\n",
        "multivariat-deskriptiven Methoden...\n",
        "\n",
        "### Zwei komplementäre Ansätze\n",
        "\n",
        "Innerhalb der multivariat-deskriptiven Statistik stellen\n",
        "**Ordinationen** und **Cluster-Analysen (Klassifikationen)** zwei\n",
        "**komplementäre Ansätze** dar. Sie betonen unterschiedliche Aspekte des\n",
        "Datensatzes und können oftmals sogar sinnvoll parallel verwendet werden.\n",
        "Die wesentlichen Unterschiede zeigt die folgende Tabelle:\n",
        "\n",
        "![](./myMediaFolder/media/image89.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"2.349395231846019in\"}\n",
        "\n",
        "## Die Idee von Ordinationen\n",
        "\n",
        "Ordinationen versuchen nun im Prinzip im *n*-dimensionalen Raum der\n",
        "(Antwort-) Variablen **diejenigen Ebenen zu finden, welche die meiste\n",
        "Varianz erklären**. Dies geschieht durch die folgenden Schritte:\n",
        "\n",
        "- **Zentrieren** der Punktwolke, so dass der Schwerpunkt im Ursprung\n",
        "    des Koordinatensystems liegt.\n",
        "\n",
        "- **Rotieren** der Punktwolke, bis die erste Achse die maximal\n",
        "    mögliche Varianz abbildet.\n",
        "\n",
        "- Nach Fixierung der ersten Achse Fortsetzen des Rotierens, bis die\n",
        "    zweite Achse wiederum das maximal Mögliche der verbleibenden Varianz\n",
        "    abbildet, usw. bis zur *n*-ten Achse.\n",
        "\n",
        "- **Visualisierung** der Ergebnisse bei Beschränkung auf die\n",
        "    relevanten ersten Achsen.\n",
        "\n",
        "Um diese Idee zu visualisieren, nehmen wir ein System von nur zwei\n",
        "Variablen, da wir diese noch auf einer Ebene (d. h. im gedruckten\n",
        "Skript) visualisieren können. Stellen wir uns sechs Beobachtungspunkte\n",
        "entlang eines Umweltgradienten (z. B. Meereshöhe) vor. An jedem dieser\n",
        "Beobachtungspunkte wird die Häufigkeit von zwei Arten ermittelt, etwa\n",
        "folgendermassen:\n",
        "\n",
        "![](./myMediaFolder/media/image90.png){width=\"3.937007874015748in\"\n",
        "height=\"3.231169072615923in\"}\n",
        "\n",
        "Wenn wir das jetzt **im „Artenraum\"** zeigen, also mit der Häufigkeit\n",
        "von Art 1 auf der *x*-Achse und der Häufigkeit von Art 2 auf der\n",
        "*y*-Achse, dan bekämen wir das **grüne** Muster. **Zentriert** (d. h. so\n",
        "dass die Mittelwerte aller *x*- und *y*-Werte jeweils 0 sind), ergibt\n",
        "sich die **rote** Figur. Dies wird schliesslich so **rotiert**, dass die\n",
        "maximale Varianz (hier im simplen Fall einfach die Distanz zwischen den\n",
        "extremen Punkten) paralle zur *x*-Achse liegt (**blau**).\n",
        "\n",
        "![](./myMediaFolder/media/image91.png){width=\"3.937007874015748in\"\n",
        "height=\"3.2873622047244093in\"}\n",
        "\n",
        "## Hauptkomponentenanalyse (PCA)\n",
        "\n",
        "### Das Prinzip\n",
        "\n",
        "Das im vorigen Abschnitt skizzierte Vorgehen, ist genau das, was eine\n",
        "**Hauptkomponentenanalyse (*Principal component analysis*, PCA)** macht:\n",
        "\n",
        "- Basiert auf einer **linearen Beziehung** zwischen den Attributen.\n",
        "\n",
        "- Achsen sind **orthogonal** (und die Varianzen daher additiv).\n",
        "\n",
        "- Die ursprünglichen **Distanzen** zwischen den Objekten\n",
        "    (Beobachtungen) bleiben daher **unverändert**.\n",
        "\n",
        "```{.r}\n",
        "PCAs eignen sich\n",
        "``` für:\n",
        "\n",
        "- Einfache Visualisierung, wenn die Linearität gegeben ist.\n",
        "\n",
        "- Bei multiplen Regressionen mit vielen, korrelierten Prädiktoren kann\n",
        "    man die PCA-Achsen als **synthetische Prädiktoren** verwenden, da\n",
        "    sie vollständig unkorreliert sind.\n",
        "\n",
        "```{.r}\n",
        "PCAs eignen sich [nicht]{.underline}\n",
        "``` (und das gilt fast immer für\n",
        "für Daten zur Artenzusammensetzung ökologischer Gemeinschaften) für:\n",
        "\n",
        "- Nicht-lineare Beziehungen.\n",
        "\n",
        "- Viele Nullen in der Matrix.\n",
        "\n",
        "Die PCA findet die beste Rotation mittels der sogenannten\n",
        "**„Eigenanalyse\"**, wie die folgende Abbildung veranschaulicht:\n",
        "\n",
        "![](./myMediaFolder/media/image92.jpeg){width=\"5.118110236220472in\"\n",
        "height=\"1.8922594050743657in\"}![](./myMediaFolder/media/image93.jpeg){width=\"5.118110236220472in\"\n",
        "height=\"2.4275863954505685in\"}\\\n",
        "(aus Wildi 2013)\n",
        "\n",
        "Dabei gilt:\n",
        "\n",
        "> **α = Eigenvektormatrix\\\n",
        "> **= Korrelationskoeffizient (der Arten/Variablen) mit den\n",
        "> Ordinationsachsen\n",
        ">\n",
        "> **Eigenwerte einre Achse = *Sum of squares* dieser Achse**\n",
        "\n",
        "### In R\n",
        "\n",
        "PCAs sind z. B. im Package labdsv implementiert:\n",
        "\n",
        "```{.r}\n",
        "library(labdsv)\n",
        "\n",
        "\n",
        "o.pca <- pca(raw)\n",
        "\n",
        "\n",
        "o.pca$scores\n",
        "\n",
        "\n",
        "PC1 PC2\n",
        "\n",
        "\n",
        "r1 -1.9216223 -0.09357697\n",
        "\n",
        "\n",
        "r2 -0.6353776 -0.68143293\n",
        "\n",
        "\n",
        "r3 0.4762699 -0.80076373\n",
        "\n",
        "\n",
        "r4 2.3503705 -0.10237502\n",
        "\n",
        "\n",
        "r5 0.8895287 0.95400610\n",
        "\n",
        "\n",
        "r6 -1.1591692 0.72414255\n",
        "\n",
        "\n",
        "o.pca$loadings\n",
        "\n",
        "\n",
        "PC1 PC2\n",
        "\n",
        "\n",
        "spec.1 0.3491944 -0.9370503\n",
        "\n",
        "\n",
        "spec.2 0.9370503 0.3491944\n",
        "\n",
        "\n",
        "#Erklärte Varianz der Achsen\n",
        "\n",
        "\n",
        "E <- o.pca$sdev\\^2/o.pca$totdev\\*100\n",
        "\n",
        "\n",
        "E\n",
        "\n",
        "\n",
        "\\[1\\] 82.40009 17.59991\n",
        "```\n",
        "\n",
        "Zunächst wird die PCA ausgeführt und das Ergebnis in einem Objekt\n",
        "(o.pca) gespeichert. Die uns Interessierten Informationen kann man wie\n",
        "oben gezeigt abrufen: ...$scores enthält die resultierenden Koordinaten\n",
        "der Beobachtungen nach der Ordination; ...$loadings gibt die Vektoren\n",
        "wieder, die nach der Rotation den beiden Arten entsprechen (Art 1 hat\n",
        "also den Vektor 0.35/--0.94). Die erklärte Varianz ist ein uns schon\n",
        "bekanntes Konzept. Die Gesamtvarianz ist alles, was im Datensatz mit\n",
        "seinen *n* Achsen drin steckt (100%), hier wird dieser Wert auf die\n",
        "Achsen aufgeteilt, also 82 % auf der ersten Achse, 18 % auf der zweiten.\n",
        "Alle *n* Achsen zusammen ergeben immer 100 %.\n",
        "\n",
        "Ziel einer PCA ist ja meist eine Visualisierung. Für unsere sechs\n",
        "Beobachtungen von zwei Arten haben wir das oben ja schon gemacht (und da\n",
        "hat es auch keine Dimensionsreduktion gebracht, da es eh nur zwei Arten\n",
        "waren). Wenn wir uns nun aber einen Datensatz mit 63 Beobachtungspunkten\n",
        "(hier: Vegetationsaufnahmen) und 119 Variablen (hier: Pflanzenarten)\n",
        "anschauen, dann haben wir eine Dimensionsreduktion von 119 auf 2. Das\n",
        "aufbereitete Ergebnis kann dann wie folgt aussehen (den R Code dazu gibt\n",
        "es im Demoskript):\n",
        "\n",
        "![](./myMediaFolder/media/image94.png){width=\"4.703498468941382in\"\n",
        "height=\"4.046511373578303in\"}![](./myMediaFolder/media/image95.png){width=\"4.7034951881014875in\"\n",
        "height=\"4.0in\"}\n",
        "\n",
        "Bitte beachten, dass wir hier eine PCA für einen Fall gerechnet haben\n",
        "(ökologische Gemeinschaftsdaten), für den sie mit seltenen Ausnahmen\n",
        "ungeeignet ist. Warum sie hier problematisch war, werden wir weiter\n",
        "unten ansehen wie auch Lösungen dafür.\n",
        "\n",
        "### Beispiele von Anwendungen von PCAs\n",
        "\n",
        "Zunächst sollen aber einige gängige und korrekte Anwendungen auf sehr\n",
        "grossen Datensets gezeigt werden:\n",
        "\n",
        "```{.r}\n",
        "(a) Visualisierung 1:\n",
        "``` Hier wurden etwa 20 verschiedene\n",
        "bioklimatische Variablen für alle Rasterzellen der Erdoberfläche\n",
        "(Farbkodierung gibt die Häufigkeit wieder) einer PCA unterworfen. Die\n",
        "Klimadaten sind so hoch korreliert, dass die ersten beiden Achen\n",
        "(Hauptkomponenten) PC1 und PC2 zusammen 76 % der Varianz im\n",
        "Gesamtdatensatz kodieren. Es wäre also unsinnig, die 20 Variablen\n",
        "einzeln zu analysieren. Durch die rechts gezeigten Korrelationen der\n",
        "Originalvariablen mit PC1 und PC2 kann man die beiden synthetischen\n",
        "Achsen näherungsweise interpretieren (siehe die Achsenbeschriftung\n",
        "links).\n",
        "\n",
        "![](./myMediaFolder/media/image96.emf.png){width=\"3.173741251093613in\"\n",
        "height=\"2.927345800524934in\"}![](./myMediaFolder/media/image97.emf.png){width=\"3.31826334208224in\"\n",
        "height=\"2.5441502624671917in\"}\\\n",
        "(aus Bruelheide et al. 2019)\n",
        "\n",
        "```{.r}\n",
        "(b) Visualisierung 2:\n",
        "``` Hier wurden 6 funktionelle Merkmale (*traits*)\n",
        "von Pflanzenarten weltweit einer PCA unterworfen. Diese erweisen sich so\n",
        "weit korreliert, dass die ersten beiden Achen (Hauptkomponenten) PC1 und\n",
        "PC2 zusammen 74% der Varianz kodieren. Der eine wesentliche Gradient\n",
        "(etwas gegen PC1 nach links verdreht) ist jender von winizigen,\n",
        "kleinsamigen Arten zu grossen Arten mit schweren Samen. Dazu weitgehend\n",
        "orthogonal ist der Gradient von Pflanzen mit stickstoffreichen Blättern\n",
        "(links oben) zu Pflanzen mit stickstoffarmen Blättern (rechts unten).\n",
        "\n",
        "![](./myMediaFolder/media/image98.emf.png){width=\"6.353766404199475in\"\n",
        "height=\"4.569767060367454in\"}\\\n",
        "(aus Díaz et al. 2016)\n",
        "\n",
        "```{.r}\n",
        "(c) Principal Components (PCs) in multiplen Regressionen:\n",
        "``` Hier\n",
        "rechnet man zunächst eine PCA mit vielen Umweltvariablen ohne Rücksicht\n",
        "auf ihre wechselseitigen Korrelationen. Dann nimmt man die (ersten)\n",
        "PC-Achsen mit der meisten Information als sogenannte „synthetische\"\n",
        "Prädiktoren.\n",
        "\n",
        "- **Vorteil:** Die PC-Achsen sind vollständig unkorreliert.\n",
        "\n",
        "- **Nachteil:** Die PC-Achsen sind nicht so direkt interpretierbar wie\n",
        "    die Original-Umweltparameter, das sie zwar oft stark mit mehreren\n",
        "    Umweltparametern korrelieren, aber eben nicht 100 %.\n",
        "\n",
        "- **Wichtig:** Hochladende Achsen sind nicht unbedingt auch die\n",
        "    wichtigsten für die Regression.\n",
        "\n",
        "## Ordinationen für „problematische\" Fälle\n",
        "\n",
        "### Wann sind PCAs problematisch?\n",
        "\n",
        "Wie schon erwähnt, ist die Anwendung von PCAs problematisch/falsch, wenn\n",
        "einer oder beide der folgenden Fälle vorliegen:\n",
        "\n",
        "- Nicht-lineare Beziehungen.\n",
        "\n",
        "- Vielen Nullen in der Matrix.\n",
        "\n",
        "In der Ökologie ist das besonders relevant, da beides für Artdaten in\n",
        "der Gemeinschaftsökologie (*community ecology*) nicht die Ausnahme,\n",
        "sondern der Normalfall ist. Arten reagieren auf Umweltfaktoren meist\n",
        "nicht linear, sondern unimodal (*humpshaped*) und in grossen Matrizen\n",
        "von Artvorkommen in Vegetationsaufnahmen und Gebieten ist es normal,\n",
        "dass die meisten Arten in den meisten Aufnahmeflächen nicht vorkommen,\n",
        "also ihre Deckung oder Abundanz Null ist. Dagegen lassen sich Matrizen\n",
        "von Umweltdaten der Untersuchungsgebiete (etwa von Boden- und\n",
        "Klimadaten) problemlos mit einer PCA analysieren (siehe Beispiel (a) im\n",
        "vorigen Abschnitt, da es ja keine Nullwerte gibt).\n",
        "\n",
        "Warum sind nicht-lineare Beziehungen in einer PCA problematisch? Sehen\n",
        "wir uns dazu noch einmal unser Eingangsbeispiel der zwei Arten entlang\n",
        "eines Umweltgradienten von 1 bis 6 an:\n",
        "\n",
        "![](./myMediaFolder/media/image90.png){width=\"3.2283464566929134in\"\n",
        "height=\"3.0407852143482064in\"}![](./myMediaFolder/media/image91.png){width=\"3.2283464566929134in\"\n",
        "height=\"3.0407852143482064in\"}\n",
        "\n",
        "Aufgrund des Umweltgradienten sollten die Beobachtungen/Standorte 1 und\n",
        "6 maximal unähnlich sein. Tatsächlich kommen sie aber im\n",
        "Ordinationsdiagramm sehr nahe beieinander zu liegen. Das liegt daran,\n",
        "dass beide Arten unimodal (mit einer Optimumskurve) auf den\n",
        "Umweltgradienten reagieren. Wenn der Umweltgradient etwa die\n",
        "Bodenfeuchte wäre, hiesse das, dass beide bei mittlerer Bodenfeuchte am\n",
        "häufigsten sind und Richtung sehr nasser oder sehr trockener Böden\n",
        "seltener werden. Das heisst, an den Standorten 1 und 6 sind beide\n",
        "relativ selten, wenn auch aus unterschiedlichen Gründen, die\n",
        "Artenzusammensetzung daher ingesamt ähnlich.\n",
        "\n",
        "Man bezeichnet dieses Phänomen/Problem: als **Hufeisen- oder\n",
        "Bogeneffekt** (***horse shoe/arch effect***).\n",
        "\n",
        "### Korrespondenzanalyse (CA)\n",
        "\n",
        "Ein Verfahren, um solche Probleme (vor allem in der\n",
        "Gemeinschaftsökologie) anzugehen, ist die **Korrespondenzanalyse\n",
        "(*Correspondence Analysis*, CA)**. Sie wird auch als ***Reciprocal\n",
        "Averging*** bezeichnet. Wichtige Aspekte der CA sind:\n",
        "\n",
        "- Hier wie in allen folgenden Ordinationsmethoden wird der\n",
        "    **Ordinationsraum transformiert** (im Gegensatz zur PCA) durch die\n",
        "    Anwendung eines **Distanzmasses**.\n",
        "\n",
        "- CA hat als Distanzmass implizit die **Χ²-Metrik**.\n",
        "\n",
        "- CA ist spezifisch gedacht für **Artenverteilungen entlang von\n",
        "    Umweltgradiente**n, wobei jede Art für sich **unimodal** reagiert.\n",
        "\n",
        "- Wie die meisten weiteren Ordinationstechniken implementiert im\n",
        "    package vegan für *community ecology*.\n",
        "\n",
        "In R wird das wie folgt umgesetzt (man beachte, dass häufig die\n",
        "Artdeckungen eingangs noch wurzeltransformiert werden (\\^0.5), um Arten\n",
        "mit geringer Deckung relativ mehr Gewicht zu geben):\n",
        "\n",
        "```{.r}\n",
        "library(vegan)\n",
        "\n",
        "\n",
        "ca.1 <- cca(sveg\\^0.5)\n",
        "\n",
        "\n",
        "#Arten (o) und Communities (+) plotten\n",
        "\n",
        "\n",
        "plot(ca.1)\n",
        "\n",
        "\n",
        "#Nur Arten plotten\n",
        "\n",
        "\n",
        "plot(ca.1, display = \"species\", type = \"points\")\n",
        "\n",
        "\n",
        "#Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird\n",
        "\n",
        "\n",
        "o.ca$CA$eig\\[1:2\\]/sum(o.ca$CA$eig)\n",
        "```\n",
        "\n",
        "Wenn wir jetzt die Anwendung der PCA und der CA auf den Moordatensatz\n",
        "(63 Vegetationsaufnahmen mit 119 Arten) anschauen, den wir oben schon\n",
        "einmal kurz hatten, dann zeigt sich, dass aus dem Hufeisen im Prinzip\n",
        "ein (umgekehrtes) U oder V wird, die extremen Punkte des Gradienten also\n",
        "nicht mehr so nahe beisammen stehen:\n",
        "\n",
        "![](./myMediaFolder/media/image99.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"3.3719510061242346in\"}\n",
        "\n",
        "Wie dieser Unterschied zustande kommt, visualisiert die folgende\n",
        "konzeptionelle Abbildung mit drei Arten:\n",
        "\n",
        "![](./myMediaFolder/media/image100.jpeg){width=\"6.3679779090113735in\"\n",
        "height=\"5.372093175853018in\"}\\\n",
        "(aus Wildi 2013)\n",
        "\n",
        "### DCA\n",
        "\n",
        "Wie wir im vorigen Abschnitt gesehen haben, löst die CA die Probleme der\n",
        "PCA bei *Community*-Daten in der Ökologie, aber eben nur teilweise. Aus\n",
        "einem Hufeisen wird ein U, aber eigentlich war der Umweltgradient (hier\n",
        "von feucht nach trocken) ja linear, nur die Artantworten waren eben\n",
        "unimodal. Insofern wurde die CA noch weiter verfeinert, um den sich\n",
        "ergebenden Hauptumweltgradienten möglichst linear abzubilden. Wir landen\n",
        "bei der **Detrended Correspondence Analysis (DCA)**, man könnte auf\n",
        "Deutsch von einer **„trendbereinigten Korrespondenzanalyse\"** sprechen,\n",
        "aber dieser deutsche Begriff wird eigentlich nie gebraucht.\n",
        "\n",
        "Es gibt verschiedene *Detrending*-Methoden, die gängigste ist\n",
        "„*detrending by segments*\". wie sie in folgendem Schema visualisiert\n",
        "ist:\n",
        "\n",
        "![](./myMediaFolder/media/image101.emf.png){width=\"6.405783027121609in\"\n",
        "height=\"8.755813648293964in\"}\\\n",
        "(aus Leyer & Wesche 2007)\n",
        "\n",
        "Die mathematischen Schritte dahinter und die daraus resultierenden\n",
        "methodischen Entscheidungen sind etwas komplexer, so dass wir sie nicht\n",
        "im Detail behandeln. Wer die Dinge im Einzelnen nachvollziehen möchte,\n",
        "sei auf Leyer & Wesche (2007) bzw. Oksanen (2015) verwiesen. Der R Code\n",
        "(Funktion decorana im Package vegan) ist auch etwas länger, sodass wir\n",
        "ihn nicht hier im Skript wiedergeben, sondern nur in den R-Demos.\n",
        "\n",
        "Aus dem Gesagten wird evident, dass eine DCA nach all den erfolgten\n",
        "Transformationen des Ordinationsraumes keine Methode der schliessenden\n",
        "Statistik ist, sondern ein (durchaus leistungsfähiges)\n",
        "Visualisierungstool komplexer Community-Daten. Da, wie geschildert, eine\n",
        "CA die Probleme der Ordination von Community-Daten nur unzureichend\n",
        "löst, findet sie als solche hier eigentlich nie Anwendung (siehe jedoch\n",
        "die CCA in Statistik 7), sondern entweder PCA oder DCA (oder eben NMDS,\n",
        "vgl. folgenden Abschnitt).\n",
        "\n",
        "Warum wird jetzt doch wieder die PCA für Community-Daten genannt,\n",
        "nachdem sie bislang mehrfach als ungeeignet angeführt wurde? Meist passt\n",
        "sie methodisch nicht, aber es gibt Fälle, bei denen die Umweltgradienten\n",
        "so kurz sind, dass die Artenreaktionen auf den oder die Umweltgradienten\n",
        "in guter Näherung als linear betrachtet werden können. Das ist dann der\n",
        "Fall, wenn man lauter sehr ähnliche Standorte untersucht hat, dann ist\n",
        "eine PCA ausnahmsweise das bessere Modell. Wie weiss man, ob das bei\n",
        "einem bestimmten Datensatz der Fall ist?\n",
        "\n",
        "Zunächst vielleicht etwas überraschend lautet die Antwort: man berechnet\n",
        "zuerst eine DCA. Ein Standard-Output der DCA ist die geschätzte\n",
        "Gradientenlänge der ersten Achse. Die Länge des Gradienten wird in\n",
        "Standardabweichungen (SD) quantifiziert, was zunächst „schräg\" klingt.\n",
        "Das bezieht sich auf die Annahme, dass die Artenhäufigkeit entlang des\n",
        "Umweltgradienten näherungsweise einer Normalverteilung folgt. Vielleicht\n",
        "habt ihr im Hinterkopf, dass 95 % aller Werte einer\n",
        "Normalverteilungskurve im Bereich von Mittelwert ± 2 SD liegt. Wenn der\n",
        "geschätzte Gradient also 4 SD-Einheiten oder mehr ist, gibt es zwischen\n",
        "den beiden Enden des untersuchten Umweltgradienten praktisch keine\n",
        "gemeinsamen Arten (bzw. sie treten mit weniger als 1 % ihrer\n",
        "Maximalhäufigkeit auf), man spricht von einem vollständigen\n",
        "Arten-Turnover. Bei einer Gradientenlänge von 8 SD-Einheiten hätte man\n",
        "sogar zwei vollständige Arten-Turnovers, also letztlich drei komplett\n",
        "verschiedene Gesellschaften ohne Überlappung.\n",
        "\n",
        "Die Faustregel für die Anwendung von DCA vs. PCA besagt, dass bei einer\n",
        "Länge der ersten Achse von < 3 SD-Einheiten mit der PCA gearbeitet\n",
        "werden sollte, bei einer Länge von 3--4 SD-Einheiten beide Methoden\n",
        "gehen und bei \\>4 SD-Einheiten man bei der DCA bleiben sollte. Man\n",
        "könnte aber auch argumentieren, dass die Annahmen der PCA theoretisch\n",
        "für solche Datensätze nie zutreffen, man also *per se* mit der DCA\n",
        "arbeiten sollte.\n",
        "\n",
        "Schauen wir uns den Effekt noch im Fall unseres Moor-Datensatzes an:\n",
        "\n",
        "![](./myMediaFolder/media/image102.emf.png){width=\"6.498611111111111in\"\n",
        "height=\"3.4694586614173226in\"}\n",
        "\n",
        "Wie wir sehen, wurde aus dem umgekehrten U und eine relativ homogene\n",
        "Punktwolke, mit der längsten Ausdehnung entlang der ersten Achse (was ja\n",
        "die Grundidee einer Ordination ist). Die Gradientenlänge können wir auf\n",
        "der *x*-Achse ablesen, sie beträgt etwa 3.2 SD-Einheiten (Differenz der\n",
        "Position zwischen dem Punkt ganz links und dem Punkt ganz rechts).\n",
        "\n",
        "### NMDS\n",
        "\n",
        "```{.r}\n",
        "NMDS** steht für ***Non-metric Multi-Dimensional Scaling*\n",
        "```, wofür es\n",
        "keine gute/gängige deutsche Übersetzung gibt. Die wichtigsten Aspekte\n",
        "einer NMDS sind:\n",
        "\n",
        "- **„Non-metric\"**, da mit **Rängen**, nicht mit Distanzen gearbeitet\n",
        "    wird.\n",
        "\n",
        "- NMDS arbeitet mit einem Iterationsalgorithmus, der jedes Mal ein\n",
        "    geringfügig anderes Ergebnis liefert.\n",
        "\n",
        "- Startet mit einer beliebigen vorgegebenen Ordination, etwa einer\n",
        "    PCA.\n",
        "\n",
        "- Danach werden sukzessive die Punkte im niedrig-dimensionalen\n",
        "    Ordinationsraum (meist 2D) geringfügig verschoben und geschaut, ob\n",
        "    die originale Distanzmatrix besser wiedergegeben wird, so lange, bis\n",
        "    ein (lokales) Optimum erreicht ist.\n",
        "\n",
        "In R geht das folgendermassen. Dabei steht der Parameter *k* für die\n",
        "Zahl der gewünschten Dimensionen (normalerweise wählt man 2) (weitere\n",
        "Details dann in der Demo im Klassenverband):\n",
        "\n",
        "```{.r}\n",
        "#Distanzmatrix als Start erzeugen\n",
        "\n",
        "\n",
        "mde <- vegdist(sveg, method=\"euclidean\")\n",
        "\n",
        "\n",
        "mde\n",
        "\n",
        "\n",
        "#Zwei verschiedene NMDS-Methoden\n",
        "```\n",
        "\n",
        "**set.seed(1) #macht man, wenn man bei einer Wiederholung exakt die\n",
        "gleichen Ergebnisse will**\n",
        "\n",
        "```{.r}\n",
        "imds <- isoMDS(mde, k=2)\n",
        "\n",
        "\n",
        "set.seed(1)\n",
        "\n",
        "\n",
        "mmds <- metaMDS(mde, k=2)\n",
        "\n",
        "\n",
        "plot(imds$points)\n",
        "\n",
        "\n",
        "plot(mmds$points)\n",
        "\n",
        "\n",
        "plot(o.imds$points)\n",
        "\n",
        "\n",
        "plot(o.mmds$points)\n",
        "```\n",
        "\n",
        "**#Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der\n",
        "originalen Distanzmatrix**\n",
        "\n",
        "```{.r}\n",
        "stressplot(o.imds,mde)\n",
        "```\n",
        "\n",
        "Das Ergebnis (hier mit dem Algorithmus isoMDS) sieht man links. Wie gut\n",
        "die NMDS die originale Struktur wiedergibt, zeigt sich rechts (erzeugt\n",
        "mit stressplot):\n",
        "\n",
        "![](./myMediaFolder/media/image103.png){width=\"3.2283464566929134in\"\n",
        "height=\"3.0407852143482064in\"}![](./myMediaFolder/media/image104.png){width=\"3.2283464566929134in\"\n",
        "height=\"3.040786307961505in\"}\n",
        "\n",
        "Zwei wichtige Aspekte sollte man hier noch erwähnen: Da NMDS mit einem\n",
        "interativen Algorithmus arbeitet, der eine Zufallskomponente enthält,\n",
        "kommen bei jedem Durchlauf geringfügig andere Ergebnisse heraus. Wenn\n",
        "man das verhindern will, kann man mit set.seed arbeiten, was erzwingt,\n",
        "dass die gleiche „Zufallswahl\" auch bei neuerlichen Durchläufen des\n",
        "R-Scriptes getroffen wird. Das Mass für die Güte einer NMDS ist der\n",
        "sogenanante Stress:\n",
        "\n",
        "> **Stress = 1 -- *R*²**\n",
        "\n",
        "In unserem Fall wäre der Stress also 1 -- 0.977, also 2.3%, mithin sehr\n",
        "niedrig. Nur in 2.3% der Fälle würde die Lage im zweidimensionalen\n",
        "NMDS-Raum also das Ranking der Distanzen anders als das Ranking der\n",
        "Distanzen im ursprünglichen *n*-dimensionalen Hyperraum wiedergeben.\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- **Ordinationen** sind im Kern **deskriptive Verfahren für\n",
        "    multivariate** (abhängige) **Variablen** und komplementär zu\n",
        "    Cluster-Analysen.\n",
        "\n",
        "- Ihre Ziele sind **Dimensionsreduktion und Visualisierung**.\n",
        "\n",
        "- Die basale Form einer Ordination ist die **PCA**. Sie setzt\n",
        "    **lineare Beziehungen und wenige Nullwerte** in der Matrix voraus.\n",
        "\n",
        "- Abgesehen von Visualisierungen kann man PCAs auch zum **Generieren\n",
        "    unkorrelierter synthetischer Variablen** für nachfolgende multiple\n",
        "    Regressionsanalysen verwenden.\n",
        "\n",
        "- Auf ökologische Gemeinschafts-Daten angewandt, ergeben PCA und CA\n",
        "    normalerweise einen **Hufeisen-Effekt**, wobei standörtlich\n",
        "    besonders unähnliche Plots nahe beieinander zu liegen kommen.\n",
        "\n",
        "- **DCA und NMDS** versuchen das zu verhindern, indem sie entweder das\n",
        "    Hufeisen «herausrechnen» oder von vornherein nur mit Rängen\n",
        "    arbeiten.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Borcard, D., Gillet, F. & Legendre, P. 2018. *Numerical ecology with\n",
        "R*. 2nd ed. Springer, Cham: 435 pp. \\[mit R\\]**\n",
        "\n",
        "Crawley, M.J. 2013. *The R book*. 2nd ed. John Wiley & Sons, Chichester,\n",
        "UK: 1051 pp. \\[mit R\\]\n",
        "\n",
        "Everitt, B. & Hothorn, T. 2011. *An introduction to applied multivariate\n",
        "analysis with R*. Springer, New York: 273 pp. \\[mit R\\]\n",
        "\n",
        "Leyer, I. & Wesche, K. 2007. *Multivariate Statistik in der Ökologie*.\n",
        "Springer, Berlin: 221 pp. \\[einfache Erklärung von Ordinationsmethoden,\n",
        "ohne R\\]\n",
        "\n",
        "McCune, B., Grace, J.B. & Urban, D.L. 2002. *Analysis of ecological\n",
        "communities*. MjM Software Design, Gleneden Beach, Oregon, US: 300 pp.\n",
        "\\[gut erklärte und detaillierte Einführung in Ordinationen u.a., ohne\n",
        "R\\]\n",
        "\n",
        "Oksanen, L. 2015. *Multivariate analysis of ecological communities in R:\n",
        "vegan tutorial*. URL:\n",
        "<http://cc.oulu.fi/~jarioksa/opetus/metodi/vegantutor.pdf>. \\[gute\n",
        "Einführung in das R-package *vegan* mit vielen Ordinationsmethoden\\]\n",
        "\n",
        "Wildi, O. 2013. *Data analysis in vegetation ecology*. 2nd\n",
        "ed.Wiley-Blackwell, Chichester, UK: 301 pp. \\[mit R\\]\n",
        "\n",
        "Wildi, O. 2017. *Data analysis in vegetation ecology*. 3rd ed. CABI,\n",
        "Wallingford, UK: 333 pp. \\[mit R\\]\n",
        "\n",
        "## Quellen der Beispiele\n",
        "\n",
        "Bruelheide, H., Dengler, J., Purschke, O., Lenoir, J., Jiménez-Alfaro,\n",
        "B., Hennekens, S.M., Botta-Dukát, Z., Chytrý, M., Field, R., (...) &\n",
        "Jandt, U. 2018. Global trait--environment relationships of plant\n",
        "communities. *Nature Ecology and Evolution* 2: 1906--1917.\n",
        "\n",
        "Díaz, S., Kattge, J., Cornelissen, J.H.C., Wright, I.J., Lavorel, S.,\n",
        "Dray, S., Reu, B., Kleyer, M., Wirth, C.(...) & Gorné, L.D. 2016. The\n",
        "global spectrum of plant form and function. *Nature* 529: 167--171.\n",
        "\n",
        "# Statistik 7: Ordinationen II\n",
        "\n",
        "**In Statistik 7 beschäftigen wir uns zunächst damit, wie wir\n",
        "Ordinationsdiagramme informativer gestalten können, etwa durch die\n",
        "Beschriftung der Beobachtunge, post-hoc-Projektion der\n",
        "Prädiktorvariablen oder *Response surfaces*. Während wir bislang mit\n",
        "*«unconstrained»* Ordinationen gearbeitet haben, welche die\n",
        "Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die\n",
        "jeweiligen *«constrained»-*Varianten derselben Ordinationsmethoden die\n",
        "Betrachtung auf den Teil der Variabilität, welcher durch eine\n",
        "Linearkombination der berücksichtigen Prädiktoren erklärt werden kann.\n",
        "Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der\n",
        "*«constrained»*-Variante der PCA und gehen einen kompletten analytischen\n",
        "Ablauf mit Aufbereitung, Interpretation und Visualisierung der\n",
        "Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes\n",
        "(Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *wisst, wie man durch post-hoc gefittete Umweltvariablen (als\n",
        "    Vektoren oder response surfaces) **Ordinationen informativer\n",
        "    machen** kann;*\n",
        "\n",
        "- *habt verstanden, was **«constrained» Ordinationen** von **normalen\n",
        "    Ordinationen** unterscheidet; und*\n",
        "\n",
        "- *könnt eine **RDA anwenden und ihre Ergebnisse interpretieren**, um\n",
        "    einen multivariaten Datensatz effektiv zu analysieren.*\n",
        "\n",
        "## Interpretation von Ordinationsergebnissen\n",
        "\n",
        "### Beschriftung der Variablen\n",
        "\n",
        "Die Interpretation eines Ordinationsdiagramms wird durch Beschriftung\n",
        "der Variablen (und ggf. der Beobachtungen) wesentlich unterstützt. Bei\n",
        "der Ordination von gemeinschaftsökologischen Daten stellen allerdings\n",
        "die grosse Zahl der Artnamen und ihre grosse Länge eine Herausforderung\n",
        "dar. Wenn man in unserem Moordatensatz aus der letzten Lektion mit\n",
        "seinen 119 Arten einfach alle ungefiltert und ungekürzt in das Diagramm\n",
        "plotten würde, wären weder die Punkte des Diagramms erkennbar, noch die\n",
        "Namen lesbar. Insofern bietet es sich an, eine Teilmenge besonders\n",
        "aussagekräftiger Arten (d. h. Variablen) auszuwählen. Mit dem in vegan\n",
        "implementierten Befehl make.cepnames werden diese auf 8 Buchstaben\n",
        "gekürzt (4 vom Gattungsnamen und 4 vom Artepithet), was in fast allen\n",
        "Fällen eindeutig ist. Zudem kann man die relative Position der\n",
        "Beschriftung zum jeweiligen Punkt durch den Parameter pos steuern (oben,\n",
        "unten, rechts, links)).\n",
        "\n",
        "```{.r}\n",
        "#4+4-Abkürzung der Namen\n",
        "\n",
        "\n",
        "snames <- make.cepnames(snames)\n",
        "\n",
        "\n",
        "#Individuelle Position der Namen\n",
        "\n",
        "\n",
        "text(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image105.png){width=\"4.52755905511811in\"\n",
        "height=\"3.693733595800525in\"}\n",
        "\n",
        "### Post hoc-Korrelation von Umweltvariablen\n",
        "\n",
        "In gemeinschaftsökologischen Datensätzen ist ja eine wichtige Frage\n",
        "meist, welche Umweltvariablen für die Verteilung der Arten in den\n",
        "Gemeinschaften/Vegetationsaufnahmen verantwortlich sind. Zur\n",
        "Rekapitulation: unsere bisherigen Ordinationsmethoden haben einzig die\n",
        "Artenvorkommen als Informationen (Variablen) genutzt. Eine\n",
        "Interpretationen der dahinterliegenden Umweltgradienten geschah bislang\n",
        "nur auf Basis unseres ökologischen Wissens über die Arten (sofern\n",
        "vorhanden). Sofern es jedoch auch erhobene Umweltdaten zu jeder\n",
        "Beobachtung gibt, können wir diese nachträglich (*post hoc*) zur\n",
        "Interpretation heranziehen. Wichtig ist dabei, dass diese zusätzlichen\n",
        "Umweltvariablen hier nicht die eigentliche Ordination beeinflusst haben,\n",
        "sondern nur zur **nachträglichen Interpretation** herangezogen werden\n",
        "(daher *post hoc*). Für unseren Moordatensatz gibt es tatsächich auch\n",
        "einen zusätzlichen Datensatz mit Umweltvariablen, die in jeder\n",
        "Vegetationsaufnahme erhoben wurden (enthalten im data frame ssit). Wir\n",
        "wählen davon fünf aus, um das Prinzip *post hoc*-gefitteter\n",
        "Umweltvariablen im Fall einer CA vorzustellen:\n",
        "\n",
        "**sel.sites <- c(\"pH.peat\", \"Acidity.peat\", \"CEC.peat\",\n",
        "\"P.peat\", \"Waterlev.max\")**\n",
        "\n",
        "```{.r}\n",
        "ev <- envfit(ca, ssit\\[,sel.sites\\])\n",
        "\n",
        "\n",
        "plot(ca, display = \"sites\", type = \"point\")\n",
        "\n",
        "\n",
        "plot(ev, add=T, cex=0.8)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image106.png){width=\"4.52755905511811in\"\n",
        "height=\"3.6156178915135606in\"}\n",
        "\n",
        "### Response surfaces\n",
        "\n",
        "Die nachträglich gefitteten Vektoren der Umweltvariablen suggerieren\n",
        "allerdings eine Linearität im Ordinationsraum, die oftmals nicht gegeben\n",
        "ist. Daher ist es oft angemessener stattdessen *Response surfaces* zu\n",
        "visualisieren, was mit dem Befehl ordisurf in vegan geht. Diese werden\n",
        "vom Programm mit GAMs gefittet. Allerdings kann man so kaum mehr als\n",
        "zwei Variablen auf einmal darsellen, weswegen die Variante mit den\n",
        "Vektorpfeilen oben weiterhin ihre Berechtigung hat:\n",
        "\n",
        "**plot(ca, display = \"sites\", type = \"point\")ordisurf(ca,\n",
        "ssit$pH.peat, add=T)**\n",
        "\n",
        "![](./myMediaFolder/media/image107.png){width=\"4.52755905511811in\"\n",
        "height=\"3.6379363517060366in\"}\n",
        "\n",
        "### Zeitliche Entwicklung\n",
        "\n",
        "Besonders aufschlussreich können Ordinationen von\n",
        "gemeinschaftsökologischen Daten sein, wenn zeitliche Entwicklungen\n",
        "analysiert, d. h. die gleiche Gemeinschaft mehrfach im Abstand von\n",
        "Jahren oder Jahrzehnten erhebt. Dies zeigt die Abbildung aus einer\n",
        "unserer Publikationen, wo 16 Vegetationsaufnahmen aus vier verschiedenen\n",
        "Vegetationstypen im Abstand von zwanzig Jahren wieder aufgenommen\n",
        "wurden. Die Vegetationstypen sind farbig codiert, die alten Aufnahmen\n",
        "gestrichelt, die neuen gefüllt und die Richtung der Veränderung wurde\n",
        "für jeden Vegetationstyp als Vektor zwischen dem alten und neuen\n",
        "Zentroid des Vegetationstyps dargestellt. Der zugehörige R-Code ist\n",
        "allerdings etwas komplexer, so dass wir ihn hier nicht besprechen:\n",
        "\n",
        "![](./myMediaFolder/media/image108.png){width=\"5.94186132983377in\"\n",
        "height=\"3.6133792650918637in\"}\\\n",
        "(aus Hüllbusch et al. 2016)\n",
        "\n",
        "## Einführung Constrained Ordinations\n",
        "\n",
        "Bislang haben wir mit normalen (unconstrained) Ordinationen gearbeitet,\n",
        "was das gängige Verfahren für Datensätze aus allen Disziplinen ist. Hier\n",
        "wurde die Transformation des ursprünglichen *n*-dimensionalen\n",
        "Hyperraumes auf eine oder wenige Ordinationsebenen allein basierend auf\n",
        "den Informationen in unseren Variablen vorgenommen.\n",
        "\n",
        "Im Fall von gemeinschaftsökologischen Daten sind unsere Variablen die\n",
        "einzelnen Arten (bzw. deren Häufigkeit in den einzelnen\n",
        "Gemeinschaften/Vegetationsaufnahmen). In diesem Fall interessiert uns\n",
        "aber oft primär, welche Umweltvariablen für das sich ergebende\n",
        "Ordinationsmuster hauptsächlich verantwortlich sind. Dafür können wir\n",
        "zwei Wege wählen:\n",
        "\n",
        "1.  Wir können ***post hoc*** die Umweltvariablen als Vektoren oder\n",
        "    Response surfaces in das Ordinationsdiagramm plotten, das ohne sie\n",
        "    gerechnet wurde (siehe voriges Kapitel).\n",
        "\n",
        "2.  Wir können die Umweltvariablen schon **direkt bei der Berechnung**\n",
        "    der Ordination einbeziehen. Dann spricht man von einer\n",
        "    **„constrained\" = „canonical\" Ordination**. Diese betrachtet nur den\n",
        "    Anteil der Artverteilungsmuster, der durch die erhobenen\n",
        "    Umweltvariablen erklärt werden kann.\n",
        "\n",
        "![](./myMediaFolder/media/image5.png){width=\"0.3087696850393701in\"\n",
        "height=\"0.30561898512685914in\"}\n",
        "\n",
        "Für die beiden wesentlichen besprochenen Ordinationsverfahren PCA (für\n",
        "lineare Beziehungen) und CA (für unimodale Beziehungen) gibt es jeweils\n",
        "eine *unconstrained-* und eine *constrained-*Variante:\n",
        "\n",
        "![](./myMediaFolder/media/image109.png){width=\"4.638543307086614in\"\n",
        "height=\"1.6998786089238844in\"}\n",
        "\n",
        "Das Prinzip und der konzeptionelle Ablauf einer „constrained\" Ordination\n",
        "sei am Beispiel eines gemeinschaftsökologischen Datensatzes kurz\n",
        "skizziert:\n",
        "\n",
        "- Man hat für jede Vegetationsaufnahme (o. ä.) zusätzlich zu den\n",
        "    Artdaten (abhängige Variablen) ein Set von dort erhobenen\n",
        "    Umweltvariablen (unabhängige Variablen).\n",
        "\n",
        "- Zunächst werden die Artmächtigkeiten der einzelnen Arten zu den\n",
        "    betrachteten Umweltvariablen jeweils mit einer **multiplen linearen\n",
        "    Regression** in Beziehung gesetzt.\n",
        "\n",
        "- Für die Ordination (PCA bzw. CA) werden dann statt der tatsächlichen\n",
        "    Artmächtigkeiten die von der multiplen Regression **vorhergesagten\n",
        "    Artmächtigkeiten** genommen\n",
        "\n",
        "- Man kann anschliessend ermitteln, wie viel der Gesamtvarianz durch\n",
        "    die verwendeten Umweltvariablen erklärt wird\n",
        "\n",
        "In R passiert all das automatisch, wenn wir in vegan z. B. den Befehl\n",
        "cca für Canonical Correspondence Analysis wählen:\n",
        "\n",
        "**s5 <-\n",
        "c(\"pH.peat\",\"P.peat\",\"Waterlev.av\",\"CEC.peat\",\"Acidity.peat\")**\n",
        "\n",
        "```{.r}\n",
        "ssit5 <- ssit\\[s5\\]\n",
        "\n",
        "\n",
        "o.cca <- cca(sveg\\~. ,data=ssit5)\n",
        "\n",
        "\n",
        "plot(o.cca)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image110.png){width=\"4.52755905511811in\"\n",
        "height=\"3.535843175853018in\"}\n",
        "\n",
        "## Redundancy Analysis (RDA) im Detail\n",
        "\n",
        "### Die Idee\n",
        "\n",
        "Wir schauen uns nun die Redundanzanalyse (RDA) im Detail an, welche die\n",
        "„constrained\"-Variante der Hauptkomponentenanalyse (PCA) ist (deswegen\n",
        "werden in vegan beide mit dem gleichen Befehl rda gerechnet, vgl.\n",
        "Statistik 6).\n",
        "\n",
        "Eine RDA wird für Datensätze angewandt, in denen man **zahlreiche\n",
        "Objekte** (*observations*) mit jeweils **vielen abhängigen und vielen\n",
        "unabhängigen Variablen** hat und erklären will, welche von den\n",
        "unabhängigen Variablen für die **multivariate Antwort** verantwortlich\n",
        "sind.\n",
        "\n",
        "Zwei typische Beispie sollen das Prinzip verdeutlichen, das natürlich\n",
        "auch in anderen Disziplinen auftreten kann (Die Tilde \\~ wird hier in\n",
        "typischer R-Schreibweise genutzt, um die abhängigen Variablen links von\n",
        "den unabhängigen rechts zu trennen):\n",
        "\n",
        "- Zusammensetzung von Pflanzengesellschaften (Anteile von Arten in\n",
        "    Probeflächen) \\~ Umweltparameter in diesen Probeflächen\n",
        "\n",
        "- Politische Einstellungen von Menschen (z. B. als Beantwortung\n",
        "    diverser Fragen auf einer Skala) \\~ sozioökonomische Eigenschaften\n",
        "    dieser Personen (z. B. Geschlecht, Alter, Bildung, Einkommen,\n",
        "    Wohnort,...)\n",
        "\n",
        "### Notwendige Datentransformation für gemeinschaftsökologische Daten\n",
        "\n",
        "Wir erinnern uns, dass in Statistik 5, von der Verwendung der PCA im\n",
        "Fall von gemeinschaftsökologischen Daten generell abgeraten wurde. Eine\n",
        "Hauptursache für die schlechte Eignung in diesen Fällen, ist dass die\n",
        "PCA (und damit auch die RDA) standardmässig mit der euklidischen Distanz\n",
        "zwischen zwei Objekten arbeitet, also der Länge der Gerade zwischen den\n",
        "beiden Objekten im multivariaten Raum (im zweidimensionalen Fall wäre\n",
        "das die Hypothenuse des rechtwinkligen Dreiecks, das durch die\n",
        "*x*/*y*-Koordinaten der beiden Beobachtungen gebildet wird; die\n",
        "Entfernung (= euklidische Distanz) berechnet sich dann einfach mit dem\n",
        "Satz des Pythagoras, analog auch für alle höheren Dimensionen). Für\n",
        "Daten von Artengemeinschaften (mit typischerweie vielen Nullwerten und\n",
        "unimodalen Verteilungen) ist die euklidische Distanz aber ungeeignet, da\n",
        "sie unerwünschte Artefakte (wie den diskutierten Hufeiseneffekt)\n",
        "erzeugt.\n",
        "\n",
        "Dies haben Legendre & Gallagher (2001) schön mit einer Simulation\n",
        "gezeigt. Zugleich konnten sie zeigen, dass ein anderes Distanzmass, die\n",
        "Hellinger-Distanz diese Probleme in viel geringerem Umfang hat. Hier\n",
        "zunächst noch einmal die Definition der beiden Distanzmasse, mit *x*~1~,\n",
        "*x*~2~: Standort, *j* = 1... *p*: Arten, *y~i,j~*: Artmächtigkeit Art\n",
        "*j* an Standort *i*:\n",
        "\n",
        "```{.r}\n",
        "Euklidische Distanz:\n",
        "```\n",
        "\n",
        "> ![](./myMediaFolder/media/image111.emf.png){width=\"3.1496062992125986in\"\n",
        "> height=\"0.809669728783902in\"}\n",
        "\n",
        "```{.r}\n",
        "Hellinger-Distanz:\n",
        "```\n",
        "\n",
        "> ![](./myMediaFolder/media/image112.emf.png){width=\"3.1496062992125986in\"\n",
        "> height=\"0.6857742782152231in\"}\n",
        "\n",
        "Um das „Verhalten\" dieser beiden Distanzmasse wurde ein Datensatz mit\n",
        "einem geografischen bzw. Umweltgradienten simuliert, entlang dem\n",
        "insgesamt neun Arten mit unimodalen Verteilungen (ungefähr Gauss'schen\n",
        "*response curves*) auftreten. Nach unserer Notation von Statistik 6\n",
        "würden diese 19 Beobachtungspunkte (sites) zusammen einen\n",
        "Diversitätsgradienten von mehr als 8 SD-Einheiten repräsentieren (d.h.\n",
        "zwei vollständige Artenturnovers, vgl. die Kurven für Species 2 and\n",
        "Species 4). Wie man sieht, ist die Rangkorrelation zwischen Distanzmass\n",
        "und tatsächlicher geographischer Distand nach erfolgter\n",
        "Hellinger-Transformation viel besser (95 %), allerdings findet auch hier\n",
        "bei einer geografischen Distanz \\> 8 keine weitere Differenzierung\n",
        "statt, da die Artengemeinschaften dann keine gemeinsame Art mehr haben.\n",
        "\n",
        "![](./myMediaFolder/media/image113.emf.png){width=\"5.2144520997375325in\"\n",
        "height=\"2.1189523184601926in\"}\\\n",
        "![](./myMediaFolder/media/image114.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"1.5304571303587051in\"}![](./myMediaFolder/media/image115.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"1.5398403324584427in\"}\\\n",
        "(aus Legendre & Gallagher 2001)\n",
        "\n",
        "Die Schlussfolgerung ist, dass man mit der Hellinger-Distanz auch für\n",
        "gemeinschaftsökologische Daten RDAs (und PCAs) andwenden kann.\n",
        "\n",
        "### Ein Beispiel\n",
        "\n",
        "Unser Beispiel stammt aus dem sehr empfehlenswerten Buch von Borcard et\n",
        "al. (2018), das insbesondere deskriptiv-multivariate Verfahren im\n",
        "Bereich der Ökologie umfangreich erklärt und dazu die R-Codes liefert:\n",
        "\n",
        "Einer der Datensätze aus dem Buch beschreibt die Fischgemeinschaften an\n",
        "30 Probestellen (sites) des Flusses Doubs im schweizerisch-französischen\n",
        "Grenzgebiet. An allen Probestellen wurden relative Abundanzen von 27\n",
        "Fischarten (jeweils 0--5; dependent variables) und 11 Umweltvariablen\n",
        "(independent variables) erhoben. Die folgende Abbildung zeigt für vier\n",
        "häufige Arten die Vereilungsmuster in simplen R-genierten Kärtchen:\n",
        "\n",
        "![](./myMediaFolder/media/image116.emf.png){width=\"4.306437007874016in\"\n",
        "height=\"4.294797681539808in\"}\n",
        "\n",
        "### Generelles zum rda-Befehl\n",
        "\n",
        "Hier seien kurz drei Syntax-Varianten des rda-Befehls im Package vegan\n",
        "vorgestellt:\n",
        "\n",
        "```{.r}\n",
        "simpleRDA <- rda (Y, X, W)\n",
        "```\n",
        "\n",
        "> **Y** = Antwort-Matrix**\\\n",
        "> X** = Matrix der erklärenden Variablen (nur numerisch)**\\\n",
        "> W** = Matrix der Co-Variablen (optional, für partielle RDAs)\n",
        "\n",
        "**formulaRDA <- rda (Y \\~ var1 + factorA + var2\\*var3 +\n",
        "Condition(var4),\\\n",
        "data = Xwdata)**\n",
        "\n",
        "> **Hier auch möglich\\\n",
        "> **- Faktoren (d. h. kategoriale Variable)\\\n",
        "> - Interaktionen\n",
        "\n",
        "```{.r}\n",
        "spe.rda <- rda (spe.hel \\~ ., env3)\n",
        "```\n",
        "\n",
        "> **Kurzschreibweise\\\n",
        "> **\\> bedeutet: alle Variablen aus dataframe env3\n",
        "\n",
        "### Interpretation der Ergebnisse\n",
        "\n",
        "Wir schauen uns nun die Ergebnisse an, wenn wir die RDA mit\n",
        "Hellingertransformierten Arthäufigkeiten und allen 10 Umweltvariablen\n",
        "rechnen:\n",
        "\n",
        "**rda(formula = spe.hel \\~ ele + slo + dis + pH + har + pho + nit +\n",
        "amm + oxy + bod, data = env3)**\n",
        "\n",
        "```{.r}\n",
        "Partitioning of variance:\n",
        "\n",
        "\n",
        "Inertia Proportion\n",
        "\n",
        "\n",
        "Total 0.5025 1.0000\n",
        "\n",
        "\n",
        "Constrained 0.3654 0.7271\n",
        "\n",
        "\n",
        "Unconstrained 0.1371 0.2729\n",
        "```\n",
        "\n",
        "Wie wir sehen, enthält der erste Teil des Ergebnis-Outputs eine\n",
        "Varianzpartitionierung. Die **Gesamtvarianz wird aufgeteilt** in jenen\n",
        "Anteil der **durch die Umweltvariablen erklärt** wird (*constrained*)\n",
        "und die **unerklärte Restvarianz** (*unconstrained*). Der Wert\n",
        "entspricht *R*² in linearen Modellen, hat aber einen *bias* (s. u.).\n",
        "\n",
        "Der Output geht wie folgt weiter:\n",
        "\n",
        "```{.r}\n",
        "Importance of components:\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3 RDA4 RDA5 RDA6\n",
        "\n",
        "\n",
        "Eigenvalue 0.2281 0.0537 0.03212 0.02321 0.008699 0.007218\n",
        "\n",
        "\n",
        "Proportion Explained 0.4539 0.1069 0.06392 0.04618 0.017311 0.014363\n",
        "```\n",
        "\n",
        "**Cumulative Proportion 0.4539 0.5607 0.62466 0.67084 0.688155\n",
        "0.702518**\n",
        "\n",
        "```{.r}\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "RDA12 PC1 PC2 PC3 PC4\n",
        "\n",
        "\n",
        "Eigenvalue 0.0003405 0.04581 0.02814 0.01528 0.01399\n",
        "\n",
        "\n",
        "Proportion Explained 0.0006776 0.09116 0.05601 0.03042 0.02784\n",
        "\n",
        "\n",
        "Cumulative Proportion 0.7270922 0.81825 0.87425 0.90467 0.93251\n",
        "```\n",
        "\n",
        "Wir sehen 12 RDA-Achsen (12 statt 10, da eine der Variablen ein Faktor\n",
        "war, der in drei dummy-Variablen zerlegt wurde). Die restliche Varianz\n",
        "findet sich dann auf den „unconstrained\"-Achsen, die mit PC1, PC2 usw.\n",
        "benannt sind. Die Varianz auf diesen Achsen steht für nicht gemessene\n",
        "Variablen (oder auch Interkationen und unimodale Beziehungen\n",
        "dergemessenen Variablen).\n",
        "\n",
        "```{.r}\n",
        "Accumulated constrained eigenvalues\n",
        "\n",
        "\n",
        "Importance of components:\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3 RDA4 RDA5 RDA6\n",
        "\n",
        "\n",
        "Eigenvalue 0.2281 0.0537 0.03212 0.02321 0.008699 0.007218\n",
        "\n",
        "\n",
        "Proportion Explained 0.6243 0.1470 0.08791 0.06351 0.023808 0.019755\n",
        "```\n",
        "\n",
        "**Cumulative Proportion 0.6243 0.7712 0.85913 0.92264 0.946448\n",
        "0.966202**\n",
        "\n",
        "In diesem Fall erklärt die erste RDA-Achse schon ungewöhnlich hohe 62%\n",
        "der Gesamtvarianz, mit der zweiten Achse zusammen gar 77%. Der Output\n",
        "geht aber noch weiter...\n",
        "\n",
        "```{.r}\n",
        "Scaling 2 for species and site scores\n",
        "\n",
        "\n",
        "\\* Species are scaled proportional to eigenvalues\n",
        "\n",
        "\n",
        "\\* Sites are unscaled: weighted dispersion equal on all dimensions\n",
        "\n",
        "\n",
        "\\* General scaling constant of scores: 1.93676\n",
        "\n",
        "\n",
        "Species scores\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3 RDA4 RDA5 RDA6\n",
        "\n",
        "\n",
        "Cogo 0.13386 0.11619 -0.238205 0.018531 0.043161 -0.029728\n",
        "\n",
        "\n",
        "Satr 0.64240 0.06654 0.123649 0.181606 -0.009584 0.029785\n",
        "\n",
        "\n",
        "Phph 0.47477 0.07009 -0.010153 -0.115349 -0.045312 -0.030034\n",
        "\n",
        "\n",
        "Babl 0.36260 0.06966 0.041311 -0.190563 -0.046944 0.006446\n",
        "\n",
        "\n",
        "Thth 0.13081 0.10707 -0.239273 0.043512 0.065818 0.003468\n",
        "\n",
        "\n",
        "\\[...\\]\n",
        "\n",
        "\n",
        "*Species scores*\n",
        "``` sind die Koordinaten der Spitzen von Artvektoren in\n",
        "Bi- und Triplots. Es gibt zwei *Scaling*-Optionen, wobei Scaling 2 der\n",
        "*default* ist. Und es geht noch weiter:\n",
        "\n",
        "```{.r}\n",
        "Site scores (weighted sums of species scores)\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3 RDA4 RDA5 RDA6\n",
        "\n",
        "\n",
        "1 0.40149 -0.154133 0.55506 1.601005 0.193044 0.916850\n",
        "\n",
        "\n",
        "2 0.53522 -0.025131 0.43393 0.294832 -0.518997 0.458849\n",
        "\n",
        "\n",
        "3 0.49429 -0.014617 0.49415 0.169258 -0.246061 0.163409\n",
        "\n",
        "\n",
        "4 0.33451 0.001188 0.51644 -0.320793 0.089569 -0.219820\n",
        "\n",
        "\n",
        "*Site scores*\n",
        "``` sind die Koordinaten der Untersuchungsflächen im Raum\n",
        "der abhängigen Variablen **Y** (hier also der Arten).\n",
        "\n",
        "```{.r}\n",
        "Site constraints (linear combinations of constraining variables)\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3 RDA4 RDA5 RDA6\n",
        "\n",
        "\n",
        "1 0.55130 0.002681 0.47744 0.626961 -0.210684 0.31503\n",
        "\n",
        "\n",
        "2 0.29736 0.105880 0.64854 0.261364 -0.057127 0.09312\n",
        "\n",
        "\n",
        "3 0.36843 -0.185333 0.59805 0.324556 -0.001611 0.31093\n",
        "\n",
        "\n",
        "4 0.44346 -0.066361 0.33293 -0.344230 -0.279546 -0.37077\n",
        "\n",
        "\n",
        "*Site constraints*\n",
        "``` sind die Koordinaten der Untersuchungsflächen im\n",
        "Raum der Prädiktorvariablen **X** (hier also der Umweltvariablen).\n",
        "\n",
        "Während dieser primäre Output schon sehr aufschlussreich war, gibt es\n",
        "noch weitere Dinge, die uns interessieren (sollten):\n",
        "\n",
        "```{.r}\n",
        "coef(spe.rda)\n",
        "\n",
        "\n",
        "RDA1 RDA2 RDA3\n",
        "\n",
        "\n",
        "ele 0.0004483347 7.795777e-05 0.0005188756\n",
        "\n",
        "\n",
        "slo.moderate -0.0123140760 -1.655649e-02 0.0160736225\n",
        "\n",
        "\n",
        "slo.steep 0.0480170930 4.905556e-02 0.1023432587\n",
        "\n",
        "\n",
        "slo.very_steep 0.0181630025 -5.708251e-02 0.2326204779\n",
        "\n",
        "\n",
        "dis -0.0014041126 4.456720e-03 0.0089169975\n",
        "```\n",
        "\n",
        "coef (spe.rda) sind die Regressionskoeffizienten der Variablen zu den\n",
        "Achsen.\n",
        "\n",
        "```{.r}\n",
        "\\# Unadjusted R\\^2 und Adjusted R\\^2\n",
        "\n",
        "\n",
        "(R2 <- RsquareAdj(spe.rda))\n",
        "\n",
        "\n",
        "$r.squared\n",
        "\n",
        "\n",
        "\\[1\\] 0.7270922\n",
        "\n",
        "\n",
        "$adj.r.squared\n",
        "\n",
        "\n",
        "\\[1\\] 0.5224114\n",
        "```\n",
        "\n",
        "Der originale (*unadjusted*) *R*² ist derselbe, den wir oben im\n",
        "Haupt-Output bekommen haben. ***R*^2^-adjusted** dagegen misst die\n",
        "**erklärte Varianz ohne *bias*** (*bias* resultiert daraus, dass bei\n",
        "vielen Variablen zwischen diesen auch rein zufällig Korrelationen\n",
        "auftreten).\n",
        "\n",
        "### Visualisierung der Ergebnisse\n",
        "\n",
        "Da eine RDA ein statistisch komplexes Verfahren ist, gibt es auch nicht\n",
        "nur eine Art und Weise, die Ergebnisse zu visualisieren, sondern zwei,\n",
        "Scaling 1 und Scaling 2. Diese sind im Folgenden gezeigt und ihre\n",
        "Unterschiede stichpunktartig erklärt. Scaling 1 eignet sich meist besser\n",
        "für die Visualisierung von Objekten (*sites*) und Scaling 2 meist\n",
        "bessser für die Visualisierung von Antwortvariablen (*species*).\n",
        "\n",
        "```{.r}\n",
        "Distanz-Triplot (*Scaling* 1):\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image117.emf.png){width=\"5.511811023622047in\"\n",
        "height=\"4.008590332458443in\"}\n",
        "\n",
        "(1) **Winkel zwischen Antwort- und erklärenden Variablen** entsprechen\n",
        "    > deren Korrelationen (aber nicht jene zwischen Antwortvariablen)\n",
        "\n",
        "(2) Die Beziehung von **Zentroiden qualitativer Variablen (Faktoren) und\n",
        "    > Antwortvariablen** ergibt sich aus der Projektion der Zentroide im\n",
        "    > rechten Winkel auf die Anwortvariable.\n",
        "\n",
        "(3) **Distanzen zwischen Zentroiden und zwischen individuellen\n",
        "    > Objekten** (*sites*) entsprechen ungefähr deren Distanzen im\n",
        "    > multivariaten Raum.\n",
        "\n",
        "```{.r}\n",
        "Korrelations-Triplot (*Scaling* 2):\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image118.emf.png){width=\"5.511811023622047in\"\n",
        "height=\"4.034889545056868in\"}\n",
        "\n",
        "(1) **Die Projektion eines** Objektes im rechten Winkel auf eine\n",
        "    > Antwort- oder eine numerische Prädiktorvariable entspricht dessen\n",
        "    > Wert entlang dieser Achse.\n",
        "\n",
        "(2) **Winkel zwischen Antwort- und erklärenden Variablen wie auch\n",
        "    > innerhalb beider Gruppen entsprechen deren Korrelationen**\n",
        "\n",
        "(3) Die Beziehung eines **Zentroids** einer qualitativen Variablen und\n",
        "    > der Antwortvariablen, ergibt sich aus seiner rechtwinkligen\n",
        "    > Projektion auf letztere.\n",
        "\n",
        "(4) **Distanzen zwischen Zentroiden und zwischen individuellen\n",
        "    > Objekten** (*sites*) entsprechen **nicht** deren Distanzen im\n",
        "    > multivariaten Raum.\n",
        "\n",
        "### Signifikanz der Achsen\n",
        "\n",
        "Eine RDA produziert immer viele Achsen, aber die entscheidende Frage\n",
        "ist, **welche davon signifikant sind** (eine Frage, die wir nur im Falle\n",
        "von *constrained*-Ordinationen stellen können, da diese im Gegensatz zu\n",
        "den rein deskriptiven *unconstrained*-Ordinationen eine\n",
        "inferenzstatistische Komponente haben). Da die Voraussetzungen\n",
        "parametrischer Tests in der Regel massiv verletzt sind, kann die\n",
        "Signifikanz nur mit Permutationen gestestet werden:\n",
        "\n",
        "```{.r}\n",
        "\\# Global test of the RDA result\n",
        "\n",
        "\n",
        "anova(spe.rda, permutations = how(nperm = 999))\n",
        "\n",
        "\n",
        "Permutation test for rda under reduced model\n",
        "\n",
        "\n",
        "Permutation: free\n",
        "\n",
        "\n",
        "Number of permutations: 999\n",
        "```\n",
        "\n",
        "**Model: rda(formula = spe.hel \\~ ele + slo + dis + pH + har + pho +\n",
        "nit + amm + oxy + bod, data = env3)**\n",
        "\n",
        "```{.r}\n",
        "Df Variance F Pr(\\>F)\n",
        "\n",
        "\n",
        "Model 12 0.36537 3.5523 0.001 \\*\\*\\*\n",
        "\n",
        "\n",
        "Residual 16 0.13714\n",
        "\n",
        "\n",
        "\\# Tests of all canonical axes\n",
        "\n",
        "\n",
        "anova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n",
        "\n",
        "\n",
        "Permutation test for rda under reduced model\n",
        "\n",
        "\n",
        "Forward tests for axes\n",
        "\n",
        "\n",
        "Permutation: free\n",
        "\n",
        "\n",
        "Number of permutations: 999\n",
        "```\n",
        "\n",
        "**Model: rda(formula = spe.hel \\~ ele + slo + dis + pH + har + pho +\n",
        "nit + amm + oxy + bod, data = env3)**\n",
        "\n",
        "```{.r}\n",
        "Df Variance F Pr(\\>F)\n",
        "\n",
        "\n",
        "RDA1 1 0.228083 26.6105 0.001 \\*\\*\\*\n",
        "\n",
        "\n",
        "RDA2 1 0.053698 6.2649 0.004 \\*\\*\n",
        "\n",
        "\n",
        "RDA3 1 0.032119 3.7473 0.333\n",
        "\n",
        "\n",
        "RDA4 1 0.023206 2.7074 0.775\n",
        "\n",
        "\n",
        "RDA5 1 0.008699 1.0149 1.000\n",
        "```\n",
        "\n",
        "Wir sehen, dass in diesem Fall die ersten beiden Achsen (RDA1, RDA2)\n",
        "signifikant sind. Nur diese sollten abgebildet werden!\n",
        "\n",
        "### Partielle RDA und Varianzpartitionierung\n",
        "\n",
        "Bei vielen Umweltvariablen können ggf. partielle RDAs aufschlussreich\n",
        "sein, die im Prinzip analog zu partiellen Regressionsplots (vgl.\n",
        "Statistik 3) funktionieren. Man kann dies für einzelne Variablen oder\n",
        "für Gruppen von Variablen machen. Zum Beispiel könnten wir fragen: Wie\n",
        "viel von der Zusammensetzung der Firschgemeinschaften erklärt die\n",
        "Wasserchemie, wenn man die topografischen Variablen konstant hält? Mit\n",
        "vegan geht das folgendermassen, einschliesslich Visualisierung in einem\n",
        "sogenannten Venn-Diagramm:\n",
        "\n",
        "```{.r}\n",
        "\\# Formula interface; X and W variables must be in the same\n",
        "\n",
        "\n",
        "\\# data frame\n",
        "\n",
        "\n",
        "(spechem.physio2 <-\n",
        "\n",
        "\n",
        "rda(spe.hel \\~ pH + har + pho + nit + amm + oxy + bod\n",
        "\n",
        "\n",
        "+ Condition(ele + slo + dis), data = env2))\n",
        "\n",
        "\n",
        "anova(spechem.physio2, permutations = how(nperm = 999))\n",
        "```\n",
        "\n",
        "**anova(spechem.physio2, permutations = how(nperm = 999), by =\n",
        "\"axis\")**\n",
        "\n",
        "```{.r}\n",
        "(spe.part.all <- varpart(spe.hel, envchem, envtopo))\n",
        "\n",
        "\n",
        "\\# Plot of the partitioning results\n",
        "\n",
        "\n",
        "dev.new(title = \"Variation partitioning - all variables\",\n",
        "\n",
        "\n",
        "noRStudioGD = TRUE)\n",
        "\n",
        "\n",
        "plot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n",
        "\n",
        "\n",
        "Xnames = c(\"Chemistry\", \"Physiography\"),\n",
        "\n",
        "\n",
        "id.size = 0.7)\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image119.png){width=\"4.858616579177602in\"\n",
        "height=\"3.6846445756780404in\"}\n",
        "\n",
        "Das **Venn-Diagramm visualisiert die Varianzaufteilung** zwischen zwei\n",
        "(oder mehr Variablen oder Gruppen von Variablen). Hier erkären die\n",
        "chemischen Variablen 24 %, die pysiographischen (topographischen) 11 %\n",
        "jeweils unabhängig voneinander, wohingegen ein grosser Teil der Varianz\n",
        "(23 %) von beiden Variablengruppen gemeinsam erklärt wird (weil sie\n",
        "nicht völlig unkorreliert sind).\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- **Post-hoc gefittete Umweltvariablen** dienen der nachträglichen\n",
        "    Beschreibung der allein aufgrund der Artdaten gefundenen\n",
        "    Ähnlichkeitsmuster.\n",
        "\n",
        "- **«Constrained» Ordinationen (RDA, CCA)** betrachten dagegen von\n",
        "    vornherein nur den Anteil der Ähnlichkeitsmuster in der Artenmatrix,\n",
        "    der sich (in linearen Modellen) durch die gemessenen Umweltvariablen\n",
        "    erklären lässt.\n",
        "\n",
        "- Eine RDA kann nicht nur deskriptiv gebraucht werden, sondern man\n",
        "    kann auch die Signifikanz von Achsen analysieren oder Varianz\n",
        "    partitionieren.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Borcard, D., Gillet, F. & Legendre, P. 2018. *Numerical ecology with\n",
        "R*. 2nd ed. Springer, Cham: 435 pp. \\[mit R\\]**\n",
        "\n",
        "Everitt, B. & Hothorn, T. 2011. *An introduction to applied multivariate\n",
        "analysis with R*. Springer, New York: 273 pp. \\[mit R\\]\n",
        "\n",
        "Legendre, P. & Gallagher, E.D. 2001. Ecologically meaningful\n",
        "transformation for ordination of species data. *Oecologia* 129:\n",
        "271--280.\n",
        "\n",
        "Leyer, I. & Wesche, K. 2007. *Multivariate Statistik in der Ökologie*.\n",
        "Springer, Berlin: 221 pp. \\[einfache Erklärung von Ordinationsmethoden,\n",
        "ohne R\\]\n",
        "\n",
        "McCune, B., Grace, J.B. & Urban, D.L. 2002. *Analysis of ecological\n",
        "communities*. MjM Software Design, Gleneden Beach, Oregon, US: 300 pp.\n",
        "\\[gut erklärte und detaillierte Einführung in Ordinationen u.a., ohne\n",
        "R\\]\n",
        "\n",
        "Oksanen, L. 2015. *Multivariate analysis of ecological communities in R:\n",
        "vegan tutorial*. URL:\n",
        "<http://cc.oulu.fi/~jarioksa/opetus/metodi/vegantutor.pdf>. \\[gute\n",
        "Einführung in das R-package *vegan* mit vielen Ordinationsmethoden\\]\n",
        "\n",
        "Wildi, O. 2017. *Data analysis in vegetation ecology*. 3rd ed. CABI,\n",
        "Wallingford, UK: 333 pp. \\[mit R\\]\n",
        "\n",
        "## Quellen des Beispiels\n",
        "\n",
        "Hüllbusch, E., Brandt, L.M., Ende, P. & Dengler, J. 2016. Little\n",
        "vegetation change during two decades in a dry grassland complex in the\n",
        "Biosphere Reserve Schorfheide-Chorin (NE Germany). *Tuexenia* 36:\n",
        "395−412.\n",
        "\n",
        "# Statistik 8: Clusteranalysen und Rückblick\n",
        "\n",
        "**In Statistik 8 lernen die Studierenden\n",
        "Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre\n",
        "Technik der deskriptiven Statistik multivariater Datensätze kennen. Es\n",
        "gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative\n",
        "Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer\n",
        "gehen wir auf die *k*-means Clusteranalyse (eine Partitionierung) und\n",
        "eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das\n",
        "gewählte Distanzmass und der Modus für die sukzessive Fusion von\n",
        "Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen\n",
        "ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren\n",
        "und mit anderen statistischen Prozeduren kombinieren kann.**\n",
        "\n",
        "**Im Abschluss von Statistik 8 werden wir dann die an den acht\n",
        "Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten\n",
        "und thematisieren, welches Verfahren wann gewählt werden sollte.\n",
        "Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom\n",
        "Einlesen der Daten bis zur Verschriftlichung der Ergebnisse,\n",
        "einschliesslich der verschiedenen zu treffenden Entscheidungen, zu\n",
        "thematisieren.**\n",
        "\n",
        "## Lernziele\n",
        "\n",
        "*Ihr...*\n",
        "\n",
        "- *habt eine prinzipielle Idee, wie **Cluster-Analysen**\n",
        "    funktionieren;*\n",
        "\n",
        "- *könnt **k-means clustering** auf Datensätze anwenden; und*\n",
        "\n",
        "- *kennt **unterschiedliche Methoden der agglomerativen\n",
        "    Clusteranalyse** sowie der Bewertung von ihren Ergebnissen und könnt\n",
        "    ihre jeweilige Eignung grob einschätzen.*\n",
        "\n",
        "## Clusteranalysen allgemein\n",
        "\n",
        "Wie Ordinationen (Statistik 6 und 7) gehören Clusteranalysen zu den\n",
        "multivariat-deskriptiven Methoden. Wozu macht man dann Clusteranalysen?\n",
        "\n",
        "- Clusteranalysen sind **komplementär zu Ordinationen**: Bei\n",
        "    Clusteranalysen liegt der Fokus auf den Unterschieden, während bei\n",
        "    der Ordination der Fokus auf dem allmählichen Wandel entlang von\n",
        "    Gradienten liegt. Insofern sind Ordinationen und Clusteranalysen\n",
        "    Methoden, die für die gleichen Datensätze und z. T. ähnliche\n",
        "    Fragestellungen angewendet werden können, aber mit Betonung\n",
        "    unterschiedlicher Aspekte. Oftmals werden in einer Studie sogar\n",
        "    beide Verfahren angewandt.\n",
        "\n",
        "- Prinzipiell geht es bei Clusteranalysen um das Herausarbeiten von\n",
        "    Gruppen von Objekten mit ähnlichen Eigenschaften, z. B.:\n",
        "\n",
        "    - um diese zu beschreiben,\n",
        "\n",
        "    - um diese auf Unterschiede zu testen oder\n",
        "\n",
        "    - um deren Verbreitung in Karten darstellen zu können.\n",
        "\n",
        "Es gibt drei grundlegende Typen von Clusteranalysen, jeweils mit\n",
        "mehreren Methoden:\n",
        "\n",
        "- **Partitionierung** (ohne Hierarchie)\n",
        "\n",
        "- **Hierarchische Clusteranalyse**\n",
        "\n",
        "    - **divisiv** (der Gesamtdatensatz wird sukzessive in immer\n",
        "        feinere Gruppen aufgeteilt)\n",
        "\n",
        "    - **agglomerativ** (beginnend mit den Einzelbeobachtungen werden\n",
        "        diese immer weiter zu Gruppen zusammengefasst)\n",
        "\n",
        "Im Kurs behandeln wir nur die Partitionierung und verschiedene\n",
        "agglomerative Clusterferfahren. Ein divisives Clusterverfahren wäre z.\n",
        "B. TWINSPAN (Hill 1979; Roleček et al. 2009), welches in der\n",
        "Vegetationsökologie viel verwendet wird, m. W. nicht in R implementiert\n",
        "ist, dafür unter anderem im Freeware-Programm JUICE (Tichý 2002).\n",
        "\n",
        "## k-means clustering\n",
        "\n",
        "Das *k-means clustering* ist die einfachste Clustermethode überhaupt.\n",
        "Ihre Kernaspekte lassen sich wie folgt beschreiben:\n",
        "\n",
        "- Partitionierung (ohne Hierarchie) in vom Benutzer vorgegebene *k*\n",
        "    Cluster.\n",
        "\n",
        "- Verfahren versucht die Summe der quadratische Abweichungen vom den\n",
        "    Clusterzentren (Zentroide) zu minimieren.\n",
        "\n",
        "- In der Tendenz entstehen ± sphärische Cluster ähnlicher Grösse\n",
        "    (sphärisch meint kugelförmig/isodiametrisch, aber eben nicht im\n",
        "    dreidimensionalen, sondern im vieldimensionalen Variablenraum).\n",
        "\n",
        "- Da das Ganze mit einem iterativen Optimierungsalgorithmus passiert,\n",
        "    der mit zufällig gewählten Startpunkten beginnt, unterscheiden sich\n",
        "    unterschiedliche Durchläufe im Ergebnis.\n",
        "\n",
        "Die Durchführung des *k-means clustering* eines multivariaten\n",
        "Datensatzes geschieht mit dem Befehl kmeans aus Base R, hier angewandt\n",
        "auf unseren Moordatensatz, den wir schon von den Ordinationen kennen:\n",
        "\n",
        "```{.r}\n",
        "kmeans.2 <- kmeans(sveg, 3)\n",
        "```\n",
        "\n",
        "Wie sehen unsere drei Cluster nun aus? Am besten plotten wir sie in das\n",
        "Ordinationsdiagramm, indem wir die Beobachtungen je nach\n",
        "Clusterzugehörigkeit einfärben:\n",
        "\n",
        "```{.r}\n",
        "plot(pca, type = \"n\")\n",
        "\n",
        "\n",
        "points(pca, display = \"sites\", pch=19, col=kmeans.2\\[\\[1\\]\\])\n",
        "```\n",
        "\n",
        "![](./myMediaFolder/media/image120.png){width=\"3.937007874015748in\"\n",
        "height=\"3.368367235345582in\"}\n",
        "\n",
        "Wie viele Cluster sollte man nun unterscheiden? Oftmals ergibt sich die\n",
        "Zahl (oder zumindest eine Grössenordnung) aus dem Zweck, für den man die\n",
        "Clusteranalyse macht. Es gibt auch unterschiedliche numerische\n",
        "Kriterien, um die „beste\" Partitionierung zu finden (allerdings liefern\n",
        "verschieden Gütemasse unterschiedliche Ergebnisse).\n",
        "\n",
        "Ein Gütemass ist **SSI = *Simple Structure Index***. Der SSI kombiniert\n",
        "drei Aspekte von Cluster-Güte: (a) maximale Differenz aller Variablen\n",
        "zwischen den Clustern, (b) Grössen der einzelnen Clustern und (c)\n",
        "Abweichung der Variablenwerte in den Clusterzentren vom Gesamtmittel.\n",
        "Der SSI reicht von 0 bis 1 und eine Partitionierung ist umso besser, je\n",
        "höher der Wert ist.\n",
        "\n",
        "Wenn wir mit einem kurzen R-Code (wird in der Demo gezeigt) für unseren\n",
        "Moordatensatz die Partitionen von *k* = 2 bis 10 ausrechnen und jeweils\n",
        "den SSI berechnen, ergibt sich das folgende Bild:\n",
        "\n",
        "![](./myMediaFolder/media/image121.emf.png){width=\"6.526797900262467in\"\n",
        "height=\"2.534342738407699in\"}\n",
        "\n",
        "Die farbige Visualisierung links zeigt, dass es eben keine hierarchische\n",
        "Clusteranalyse ist. Bei *k* \\> 2 bleibt die ursprüngliche Abgrenzung der\n",
        "zwei Hauptcluster nicht erhalten. Gemäss SSI wäre in diesem Fall die\n",
        "10-Cluster-Lösung die beste (es sei aber empfohlen, solchen numerischen\n",
        "„Empfehlungen\" nicht blindlings zu glauben).\n",
        "\n",
        "## Agglomerative Clusterverfahren\n",
        "\n",
        "### Einführung\n",
        "\n",
        "Bei agglomerativen Clusterverfahren folgt der Algorithmus immer dem\n",
        "folgenden Ablauf:\n",
        "\n",
        "- Sie fassen die **beiden ähnlichsten Beobachtungen als initiales\n",
        "    Cluster** zusammen.\n",
        "\n",
        "- Danach geht es mit dem **Zusammenfassen des nächstähnlichen Paares**\n",
        "    von Einzelbeobachtungen bzw. Clustern so lange weiter, bis alle\n",
        "    Cluster zu einem einzigen zusammengefasst sind.\n",
        "\n",
        "Es gibt deswegen so viele verschiedene agglomerative Clusterverfahren,\n",
        "da man zwei wesentliche Parameter im Prinzip frei kombinieren kann, das\n",
        "verwendete Distanzmass und den Modus für das Zusammenfügen von Clustern:\n",
        "\n",
        "An **Distanzmassen** sind die folgenden beiden die gängigsten:\n",
        "\n",
        "- **Euklidische (pythagoreische) Distanz**: Länge der Gerade, die die\n",
        "    beiden Punkte im multidimensionalen Hyperraum miteinander verbindet.\n",
        "\n",
        "- **Chord-Distanz**: euklidische Distanz, nachdem alle Variablen auf\n",
        "    Länge 1 standardisiert wurden.\n",
        "\n",
        "Die vier gängigsten **Modi für das Zusammenfassen von Clustern** sind:\n",
        "\n",
        "- ***Single linkage*** (*nearest neighbour*): Distanz zum nächsten\n",
        "    Element eines Clusters wird genommen.\n",
        "\n",
        "- ***Complete linkage*** (*furthest neighbour*): Distanz zum am\n",
        "    weitesten entfernten Element eines Clusters wird genommen.\n",
        "\n",
        "- ***Average linkage*** (4 verschiedene Methoden, darunter besonders\n",
        "    gängig **UPGMA = *unweighted pair-group method using arithmetic\n",
        "    averages***): Distanz zum Cluster\"zentrum\" wird genommen.\n",
        "\n",
        "- ***Ward's mimimum variance clustering***: Statt Distanzen zwischen\n",
        "    Clustermitgliedern zu minimieren, wird hier die Clustervariabilität\n",
        "    minimiert.\n",
        "\n",
        "Schauen wir uns an, welchen Effekt die vier Verfahren kombiniert mit der\n",
        "Chord-Distanz auf die Fischgemeinschaftsdaten des Doubs-Datensatzes\n",
        "haben:\n",
        "\n",
        "![](./myMediaFolder/media/image122.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"2.076661198600175in\"}![](./myMediaFolder/media/image123.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"2.1030369641294837in\"}![](./myMediaFolder/media/image124.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"2.1058475503062115in\"}![](./myMediaFolder/media/image125.emf.png){width=\"3.2283464566929134in\"\n",
        "height=\"2.1151246719160106in\"}\n",
        "\n",
        "Es zeigt sich, dass die Cluster doch sehr unterschiedlich aussehen\n",
        "können. Die terminalen Cluster sind oft identisch (ein Cluster aus den\n",
        "Probestellen 17 und 18 gibt es etwas bei allen vier Methoden), doch auf\n",
        "höherer Ebene gibt es gravierende Unterschiede. Diese äussern sich\n",
        "insbesondere in der Anfälligkeit gegenüber **Kettenbildung\n",
        "(*Chaining*)**, was meint, dass eine Aufnahme allen anderen\n",
        "gegenübergesellt wird und in diesem grossen Cluster im nächsten Schritt\n",
        "wieder eine einzige einzige Aufnahme dem Rest herausgegriffen usw.\n",
        "*Single linkage* ist methodenbedingt besonders anfällig für *Chaining*\n",
        "(siehe links oben). Da für die meisten Anwendungen solche Ein-\n",
        "Aufnahmen-Cluster unpraktisch sind, wird *single linkage* kaum noch\n",
        "verwendet. *Complete linkage* und UPGMA neigen weniger zu Chaining und\n",
        "die Ward-Methode am wenigsten.\n",
        "\n",
        "### Güte von Clusterungen\n",
        "\n",
        "Nun ist zwar Chaining unpraktisch, aber was, wenn es doch die realen\n",
        "Ähnlichkeitsbeziehungen am besten wiedergeben würde? Ein gutes Mass für\n",
        "die Güte eines Clusterergebnisses ist die **Cophenetische Korrelation**.\n",
        "Hier werden die Clusterpositionen in paarweise Distanzen zwischen\n",
        "Beobachtungen übersetzt und mit den ursprünglichen Distanzen verglichen\n",
        "(vergleichbar dem Stressplot im Falle einer NMDS-Ordination, vgl.\n",
        "Statistik 6). Schauen wir uns das Ergebnis für die vier Beispiele von\n",
        "oben an:\n",
        "\n",
        "![](./myMediaFolder/media/image126.jpeg){width=\"6.520833333333333in\"\n",
        "height=\"7.556330927384077in\"}\n",
        "\n",
        "Auch hier schneidet *single linkage* am schlechtesten ab. Wie meist,\n",
        "sind UPGMA und Ward am besten, wobei hier UPGMA sogar besser als Ward\n",
        "abschneidet.\n",
        "\n",
        "### Wie viele Cluster sollte man unterscheiden?\n",
        "\n",
        "Wie schon bei der *k*-means-Partitionierung stellt sich auch beim\n",
        "hierarchischen Clustering die Frage nach der optimalen Zahl von\n",
        "unterschiedenen Clustern. Vielfach ergibt sich die Antwort darauf\n",
        "zumindest grössenordnungsmässig aus der geplanten Verwendung der\n",
        "Cluster. Es gibt auch verschiedene mathematische Gütemasse, u. a.\n",
        "Silhouette, Matrix-Korrelation und Indikatorarten:\n",
        "\n",
        "```{.r}\n",
        "Sihouette:\n",
        "``` mittlere Distanz eines Objektes zu allen Objekten eines\n",
        "Clusters zur mittleren Distanz zu allen Objekten des nächstähnlichen\n",
        "Clusters. Die Werte reichen von --1 bis +1.\n",
        "\n",
        "![](./myMediaFolder/media/image127.emf.png){width=\"3.1496062992125986in\"\n",
        "height=\"3.417851049868766in\"}\n",
        "\n",
        "```{.r}\n",
        "Matrix-Korrelation:\n",
        "``` Vergleich der originalen Unähnlichkeitsmatrix\n",
        "mit der binären Matrix basierend auf der Gruppenzusammengehörigkeit im\n",
        "Dendrogramm.\n",
        "\n",
        "![](./myMediaFolder/media/image128.emf.png){width=\"3.1496062992125986in\"\n",
        "height=\"3.4399693788276466in\"}\n",
        "\n",
        "```{.r}\n",
        "Indikatorarten:\n",
        "``` Anzahl von Indikatorarten (links) bzw. Anteil von\n",
        "Clustern mit signifikanten Indikatorarten (rechts) (hier basierend auf\n",
        "dem IndVal-Konzept; siehe Borcard et al. 2018). Dieser Ansatz\n",
        "funktioniert natürlich nur, wenn es sich um Daten von\n",
        "Artengemeinschaften handelt.\n",
        "\n",
        "![](./myMediaFolder/media/image129.emf.png){width=\"6.299212598425197in\"\n",
        "height=\"3.340563210848644in\"}\n",
        "\n",
        "### Charakterisierung von Clustern\n",
        "\n",
        "Wie schon bei *k*-means können wir die Cluster dadurch charakterisieren,\n",
        "dass wir die Clusterzugehörigkeit in ein einfaches oder\n",
        "Biplot-Ordinationsdiagramm plotten. Weitere Möglichkeiten der\n",
        "Beschreibung/Charakterisierung von Clustern sind u. a. (jeweils\n",
        "visualisiert für die 4-Cluster-Lösung des Doubs-Datensatzes):\n",
        "\n",
        "```{.r}\n",
        "(1) Einfärbung im Dendrogramm\n",
        "``` (den R-Code dazu gibt es im\n",
        "Demoskript):\n",
        "\n",
        "![](./myMediaFolder/media/image130.emf.png){width=\"4.092148950131234in\"\n",
        "height=\"3.3016185476815396in\"}\n",
        "\n",
        "```{.r}\n",
        "(2) Geordnete Community-Tabelle\n",
        "``` (im Fall von von\n",
        "gemeinschaftsökologischen Daten), ggf. mit Hervorhebung der signifikant\n",
        "konzentrierten Arten:\n",
        "\n",
        "> **32222222222 111111 1111**\n",
        ">\n",
        "> **09876210543959876506473221341**\n",
        ">\n",
        "> **Icme 5432121\\...\\...\\...\\...\\...\\...\\....**\n",
        ">\n",
        "> **Abbr 54332431\\.....1\\...\\...\\...\\...\\...**\n",
        ">\n",
        "> **Blbj 54542432.1\\...1\\...\\...\\...\\...\\...**\n",
        ">\n",
        "> **Anan 54432222\\.....111\\...\\...\\...\\....**\n",
        ">\n",
        "> **Gyce 5555443212\\...11\\...\\...\\...\\.....**\n",
        ">\n",
        "> **Scer 522112221\\...21\\...\\...\\...\\...\\...**\n",
        ">\n",
        "> **Cyca 53421321\\.....1111\\...\\...\\...\\...**\n",
        ">\n",
        "> **Rham 55432333\\.....221\\...\\...\\...\\....**\n",
        ">\n",
        "> **Legi 35432322.1\\...1111\\...\\...\\...\\...**\n",
        ">\n",
        "> **Alal 55555555352..322\\...\\...\\...\\....**\n",
        ">\n",
        "> **Chna 12111322.1\\...211\\...\\...\\...\\....**\n",
        ">\n",
        "> **Titi 53453444\\...1321111.21\\...\\.....**\n",
        ">\n",
        "> **Ruru 55554555121455221..1\\...\\...\\...**\n",
        ">\n",
        "> **Albi 53111123\\.....2341\\...\\...\\...\\...**\n",
        ">\n",
        "> **Baba 35342544\\.....23322\\...\\...\\...1.**\n",
        ">\n",
        "> **Eslu 453423321\\...41111..12.1\\....1.**\n",
        ">\n",
        "> **Gogo 5544355421..242122111\\...\\...1.**\n",
        ">\n",
        "> **Pefl 54211432\\....41321..12\\...\\.....**\n",
        ">\n",
        "> **Pato 2211.222\\.....3344\\...\\...\\...\\...**\n",
        ">\n",
        "> **Sqce 3443242312152132232211..11.1.**\n",
        ">\n",
        "> **Lele 332213221\\...52235321.1\\...\\....**\n",
        ">\n",
        "> **Babl .1111112\\...32534554555534124.**\n",
        ">\n",
        "> **Teso .1\\...\\...\\.....11254\\...\\.....23.**\n",
        ">\n",
        "> **Phph .1\\....11\\...13334344454544455.**\n",
        ">\n",
        "> **Cogo \\...\\...\\...\\.....1123\\...\\...2123.**\n",
        ">\n",
        "> **Satr .1\\...\\...\\....2.123413455553553**\n",
        ">\n",
        "> **Thth .1\\...\\...\\...\\...11.2\\...\\...2134.**\n",
        ">\n",
        "> **sites species**\n",
        ">\n",
        "> **29 27**\n",
        "\n",
        "```{.r}\n",
        "(3)\n",
        "``` Vergleich der (Umwelt-)Variablen zwischen den Clustern mittels\n",
        "**ANOVA**.\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "- ***k-means clustering*** ist eine einfache nicht-hierarchische\n",
        "    Clustermethode, bei der der Benutzer vorgibt, wie viele Einheiten er\n",
        "    haben möchte.\n",
        "\n",
        "- **Agglomerative Clusterverfahren** fassen Einheiten sukzessive über\n",
        "    ihre Ähnlichkeitsbeziehungen zusammen. Am Ende kann man dann\n",
        "    subjektiv oder nach unterschiedlichen numerischen Kriterien\n",
        "    entscheiden, welche Clusterauflösung dem Bedarf am besten\n",
        "    entspricht.\n",
        "\n",
        "## Weiterführende Literatur\n",
        "\n",
        "**Borcard, D., Gillet, F. & Legendre, P. 2018. *Numerical ecology with\n",
        "R*. 2nd ed. Springer, Cham: 435 pp. \\[mit R\\]**\n",
        "\n",
        "Crawley, M.J. 2013. *The R book*. 2nd ed. John Wiley & Sons, Chichester,\n",
        "UK: 1051 pp. \\[mit R\\]\n",
        "\n",
        "Everitt, B. & Hothorn, T. 2011. *An introduction to applied multivariate\n",
        "analysis with R*. Springer, New York: 273 pp. \\[mit R\\]\n",
        "\n",
        "Hill, M.O. 1979. *TWINSPAN -- A FORTRAN program for arranging\n",
        "multivariate data in an ordered two-way table by classification of the\n",
        "individuals and attributes*. Cornell University, Ithaca, NY: 90 pp.\n",
        "\n",
        "Roleček, J., Tichý, L., Zelený, D. & Chytrý, M. 2009. Modified TWINSPAN\n",
        "classification in which the hierarchy represents cluster heterogeneity.\n",
        "*Journal of Vegetation Science* 20: 596--602.\n",
        "\n",
        "Tichý, L. 2002. JUICE, software for vegetation classification. *Journal\n",
        "of Vegetation Science* 13: 451--453.\n",
        "\n",
        "Wildi, O. 2017. *Data analysis in vegetation ecology*. 3rd ed. CABI,\n",
        "Wallingford, UK: 333 pp. \\[mit R\\]\n",
        "\n",
        "# Anhang: Übersicht über statistische Verfahren\n",
        "\n",
        "![](./myMediaFolder/media/image131.emf.png){width=\"9.912398293963255in\"\n",
        "height=\"6.603620953630796in\"}"
      ],
      "id": "6c2ac433"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}